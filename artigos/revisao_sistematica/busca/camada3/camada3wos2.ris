TY  - JOUR
AU  - Weichbroth, P
AU  - Jandy, K
AU  - Zurada, J
TI  - Toward Sustainable Development: Exploring the Value and Benefits of Digital Twins
T2  - TELECOM
AB  - The complexity and number of data streams generated by internal processes exceed the capabilities of most current simulation environments. Consequently, there is a need for the development of more advanced solutions that can handle any number of simultaneous simulations. One of the most promising ideas to address these and other challenges is the concept of a Digital Twin (DT), which refers to a digital representation or a virtual model designed to accurately reflect an intended or actual physical product, system, or process (i.e., a physical twin). As a Digital Twin spans the life-cycle of its physical twin, its development and application can bring considerable benefits to organizations seeking to improve existing processes as well as implement new ones. However, few studies have comprehensively examined the value and benefits of Digital Twins. To fill this gap, this study aims to provide a better understanding of this technology by reviewing the contemporary literature, with a particular focus on the documented case studies, as well as reported business and industrial deployments. The results obtained show that Digital Twins have proven beneficial for maintenance, cost reduction, optimization, simulation performance, monitoring, product life-cycle understanding, assessment validation, performance evaluation, product design, and safety and risk mitigation. In addition, when considering the human factor, DTs can facilitate education and training, team collaboration, and decision making. Undeniably, Digital Twins are a game changer for safer, faster, and more sustainable development.
SN  - 2673-4001
DA  - SEP
PY  - 2024
VL  - 5
IS  - 3
SP  - 774
EP  - 791
DO  - 10.3390/telecom5030039
AN  - WOS:001324070600001
ER  -

TY  - JOUR
AU  - Zhu, HT
AU  - Lin, BT
TI  - Digital twin-driven energy consumption management of integrated heat pipe cooling system for a data center
T2  - APPLIED ENERGY
AB  - The energy consumption management (ECM) for the integrated heat pipe cooling (IHPC) systems has become a significant cost-cutting strategy, given the growing demand for the decreased cooling and maintenance costs in data centers. However, the traditional ECM strategies lack an integration with the real-time information and the automatic feedback control, causing the risks of system operation difficult to diagnose and the potential for energy saving hard to exploit. In this respect, a digital twin approach was proposed to efficiently and automatically implement the ECM strategy for an IHPC system. First, a digital twin architecture was established to enable seamless integration and real-time interaction between the physical system and the digital twin. Secondly, the digital twin models of monitoring, simulation, energy evaluation and optimization were developed to drive the corresponding services. Finally, the approach was verified on an IHPC system operating in a real-life data center. It is found that the approach can automatically detect and justify the abnormal states of the IHPC system. Moreover, the approach can reduce the power consumption by 23.63% while meeting the production requirements. The mean relative errors of the supply air temperature and the cooling capacity between the digital twin simulated and the on-site records are 1.43% and 1.46%, respectively. In summary, the proposed approach provides a digital twin workflow that can significantly improve the efficiency of the ECM strategy deployed on an IHPC system.
SN  - 0306-2619
SN  - 1872-9118
DA  - NOV 1
PY  - 2024
VL  - 373
C7  - 123840
DO  - 10.1016/j.apenergy.2024.123840
C6  - JUL 2024
AN  - WOS:001339562200001
ER  -

TY  - JOUR
AU  - Kamali, M
AU  - Atazadeh, B
AU  - Rajabifard, A
AU  - Chen, YQ
TI  - Advancements in 3D digital model generation for digital twins in industrial environments: Knowledge gaps and future directions
T2  - ADVANCED ENGINEERING INFORMATICS
AB  - Digital twins are considered a transformative paradigm for industrial environments, providing a dynamic, digital, and intelligent representation of industrial assets. The necessity of digital twins in industrial settings is underscored by their ability to enhance asset monitoring, operational efficiency, and maintenance activities. The 3D digital model is fundamental for digital twins, serving not only as a digital representation of industrial environment but also facilitating the simulation of real-world scenarios. Although there have been extensive studies on the application of digital twins in industrial environments, the creation of 3D digital model for digital twins in existing industrial environments is still overlooked, primarily due to the complexity of these environments. This article aims to propose a workflow to create a 3D digital model for digital twins in existing industrial environments that includes four key components: 1) Data capturing, 2) 3D modeling, 3) Asset localization, and 4) Information integration. A significant body of literature on each component is surveyed to identify current knowledge gaps in harnessing 3D digital models for digital twins in industrial environments. In response to these gaps, this study proposes a series of future research directions, including automated data validation, real-time processing, semi-supervised or unsupervised learning-based 3D reconstruction methods, and 3D visualization approaches for industrial assets.
SN  - 1474-0346
SN  - 1873-5320
DA  - OCT
PY  - 2024
VL  - 62
C7  - 102929
DO  - 10.1016/j.aei.2024.102929
C6  - NOV 2024
AN  - CCC:001353342500001
ER  -

TY  - JOUR
AU  - Chiachio, M
AU  - Megia, M
AU  - Chiachio, J
AU  - Fernandez, J
AU  - Jalon, ML
TI  - Structural digital twin framework: Formulation and technology integration
T2  - AUTOMATION IN CONSTRUCTION
AB  - This work presents a digital twin framework for structural engineering. The digital twin is conceptualised and mathematically idealised within the context of structural integrity, and includes the main attributes to behave as a functional digital twin, namely simulation, learning, and management. The manuscript makes special emphasis on the autonomous interactions between the physical and digital counterparts along with on the workflow modelling of the digital twin, which are both missing aspects in the majority of use cases found in the literature, specially within the civil and structural engineering domain. The proposed framework is demonstrated in a proof of concept using a laboratory scale test structure monitored using internet-of-things-based sensors and actuators. The results reveal that the virtual counterpart can respond in real-time with self-adaptability in liaison to the performance of the physical counterpart. Moreover, the tests show that the digital twin is able to provide automated decision making for structural integrity.
SN  - 0926-5805
SN  - 1872-7891
DA  - AUG
PY  - 2022
VL  - 140
C7  - 104333
DO  - 10.1016/j.autcon.2022.104333
C6  - MAY 2022
AN  - WOS:000806365600007
ER  -

TY  - JOUR
AU  - Sheng, D
AU  - Lou, Y
AU  - Sun, FF
AU  - Xie, JP
AU  - Yu, Y
TI  - Reengineering and Its Reliability: An Analysis of Water Projects and Watershed Management under a Digital Twin Scheme in China
T2  - WATER
AB  - Water project and watershed management is currently being reengineered under digital twin schemes in China through pilot projects. An evaluation on pilot reengineering is important for its further implementation and improvement. This paper investigates national legislation and pilot projects' implementations of a digital twin watershed and digital twin water project from the perspectives of design, policy, technology, investment, personnel, cyberspace security, co-construction, and sharing through interviews and expert review, and it uses a Bayesian network to study their reliability. First, the design of the digital twin watershed and digital twin water project is reasonable with regard to system architecture and business management. Second, although there are some national legislations on cyberspace security and geospatial data, they are incomplete for policy making and are probably infeasible for some technology. Third, there are insufficient mechanisms to sustainably support investment, personnel, and cyberspace security. Forth, co-construction and sharing are required for both inside and outside water departments. Fifth, the Bayesian network is useful for investigating the reliability of weak nodes, and it is helpful for the design and further implementation of the digital twin watershed and digital twin water project, as will be demonstrated with an anonymous example. This study could provide useful insights into the further reengineering of water projects and watershed management under a digital twin scheme in the world.
SN  - 2073-4441
DA  - SEP
PY  - 2023
VL  - 15
IS  - 18
C7  - 3203
DO  - 10.3390/w15183203
AN  - WOS:001077816300001
ER  -

TY  - JOUR
AU  - Luo, RP
AU  - Sheng, BY
AU  - Lu, YK
AU  - Huang, YZ
AU  - Fu, GC
AU  - Yin, XY
TI  - Digital Twin Model Quality Optimization and Control Methods Based on Workflow Management
T2  - APPLIED SCIENCES-BASEL
AB  - Digital twin is an important emerging technology for digital transformation and intelligent upgrading. Digital twin models are the prerequisite for digital twin applications, and their quality directly affects the quality of digital twin services in monitoring, simulation, prediction, optimization, and other areas. However, researchers have paid insufficient attention to the quality control of digital twin models, thus hindering their effective application. To effectively control model construction and optimize model quality in the design process, this study developed digital twin model quality optimization and control methods based on workflow management. First, a workflow process model integrating digital twin model evaluation was constructed, which integrated the design process and model evaluation methods into workflow management. Then, digital twin model quality control and optimization in different stages were achieved at the macro and micro levels. Thus, the digital twin model quality was effectively controlled during the design process, and targeted design resources were selected to optimize model quality. Finally, the validity of the proposed method of model quality optimization and control was verified using the digital twin models of a practical teaching platform and a multifunctional lift-and-slide experimental line. All evaluation indexes of the model achieved good values, and the target quality optimization of the model could be performed during the design process. The results indicate that the proposed method can effectively control and optimize the model quality, which has excellent feasibility and enables the effective application of the digital twin.
SN  - 2076-3417
DA  - MAR
PY  - 2023
VL  - 13
IS  - 5
C7  - 2884
DO  - 10.3390/app13052884
AN  - WOS:000947687200001
ER  -

TY  - JOUR
AU  - HosseiniHaghighi, S
AU  - de Uribarri, PMA
AU  - Padsala, R
AU  - Eicker, U
TI  - Characterizing and structuring urban GIS data for housing stock energy modelling and retrofitting
T2  - ENERGY AND BUILDINGS
AB  - Addressing the gap between available urban building data, energy performance evaluation, and spatial distribution of end-users is a growing concern of municipalities to support energy planning and urban retrofitting towards low carbon emission. These days, many existing urban building energy models (UBEM) use the non-geo-specified 3D building stock geometries or the digital twins of cities as their primary input for energy simulation. The digital twins of cities or the built environment is an accurate scale virtual representation (geo-specified geometry and information) of city objects enabled to be paired with dynamic predictive models of their energy performance. The use of built environment digital twins for their energy assessment works well for those cities whose digital twin datasets are readily available for public or private use. In the cities without digital twin datasets, their energy assessment is limited by tools that use 2D building stock footprints as their primary input or apply a top-down urban energy simulation approach. This gap motivated the current research to identify an extended UBEM data preparation workflow focused on generating city digital twins using accessible GIS datasets as well as national and local data sources. The workflow is majorly developed to deal with GIS data processing utilizing multilevel spatial data integration and refinement to fill in the recognized inconsistencies of building databases. Then, procedural 3D city modelling and an automated transformation to a semantic, CityGML format are successfully provided to generate digital twin of the study area in an open data model. The archetype modelling method is also employed to create a set of customized housing energy profiles to support tailored bottom-up UBEM and retrofitting scenarios relying on the simulation platform Insel4Cities, under development at Concordia University. The workflow was practically carried out on a case study, Kelowna city in British Columbia (BC)/Canada, to validate the district heating load. The average simulated heating energy use intensity showed a deviation of less than 2.5% from the BC measured data. As an example of workflow usability, the developed model was also utilized to assess a retrofit scenario, combining a decentralized heat pump and photovoltaic system, to present the district's potential for net-zero action planning. (c) 2021 Elsevier B.V. All rights reserved.
SN  - 0378-7788
SN  - 1872-6178
DA  - FEB 1
PY  - 2022
VL  - 256
C7  - 111706
DO  - 10.1016/j.enbuild.2021.111706
C6  - DEC 2021
AN  - WOS:000754031200002
ER  -

TY  - CPAPER
AU  - Preuveneers, D
AU  - Joosen, W
AU  - Ilie-Zudor, E
A1  - IEEE
TI  - Robust digital twin compositions for Industry 4.0 smart manufacturing systems
T2  - 2018 IEEE 22ND INTERNATIONAL ENTERPRISE DISTRIBUTED OBJECT COMPUTING CONFERENCE WORKSHOPS (EDOCW 2018)
CP  - 22nd IEEE International Enterprise Distributed Object Computing Conference (IEEE EDOC)
AB  - Industry 4.0 is an emerging business paradigm that is reaping the benefits of enabling technologies driving intelligent systems and environments. By acquiring, processing and acting upon various kinds of relevant context information, smart automated manufacturing systems can make well-informed decisions to adapt and optimize their production processes at runtime. To manage this complexity, the manufacturing world is proposing the 'Digital Twin' model to represent physical products in the real space and their virtual counterparts in the virtual space, with data connections to tie the virtual and real products together for an augmented view of the manufacturing workflow. The benefits of such representations are simplified process simulations and efficiency optimizations, predictions, early warnings, etc. However, the robustness and fidelity of digital twins are a critical concern, especially when independently developed production systems and corresponding digital twins interfere with one another in a manufacturing workflow and jeopardize the proper behavior of production systems. We therefore evaluate the addition of safeguards to digital twins for smart cyber-physical production systems (CPPS) in an Industry 4.0 manufacturing workflow in the form of feature toggles that are managed at runtime by software circuit breakers. Our evaluation shows how these improvements can increase the robustness of interacting digital twins by avoiding local errors from cascading through the distributed production or manufacturing workflow.
SN  - 2325-6583
SN  - 978-1-5386-4141-5
PY  - 2018
SP  - 69
EP  - 78
DO  - 10.1109/EDOCW.2018.00021
AN  - WOS:000495093800011
ER  -

TY  - JOUR
AU  - Shen, ZW
AU  - Arraño-Vargas, F
AU  - Konstantinou, G
TI  - Virtual testbed for development and evaluation of power system digital twins and their applications
T2  - SUSTAINABLE ENERGY GRIDS & NETWORKS
AB  - Digital twins (DTs) show considerable promise for development of applications and solutions across a diverse range of power system services. Ensuring these applications function as intended without causing disruptions or unexpected issues requires comprehensive testing prior to practical deployment. However, current approaches to power system digital twin (PSDT) application development and testing experience significant obstacles, including limitations and risks associated with accessing real-world power systems, models and data. This article introduces a virtual testbed based on the concept of a virtual physical twin (VPT), that replaces the need for access to a physical twin (PT), as a platform designed to remove the heavy reliance on online data flows and provide a close to real-world environment. A testbed enables simultaneous development and testing of DTs and their related applications in a realistic, interactive, and safe setting without impact on the operation of the actual power system. A complete workflow for PSDT testbed implementations is proposed, with all necessary steps from concept to the eventual connection to the physical system. To validate the practical implementation steps, utility, and value of the testbed, a real -time transient stability analysis with two artificial intelligence algorithms is provided. The proposed testbed is expandable and adaptable, promoting the development of advanced and intelligent PSDTs. The key benefit of the testbed is that it enables the development of DT and its application closely in a realistic environment without affecting the operation of PT.
SN  - 2352-4677
DA  - JUN
PY  - 2024
VL  - 38
C7  - 101331
DO  - 10.1016/j.segan.2024.101331
C6  - FEB 2024
AN  - WOS:001196591400001
ER  -

TY  - JOUR
AU  - Getuli, V
AU  - D'Ascenzi, E
AU  - Fiesoli, I
TI  - INTEGRATION OF HUMAN WELL-BEING IN DIGITAL CONSTRUCTION PROCESSES AND DIGITAL TWINS: A SYSTEMATIC REVIEW OF STRESS DETECTION PARAMETERS AND TOOLS TO SUPPORT HUMAN-CENTRIC CONSTRUCTION PROCESSES
T2  - JOURNAL OF INFORMATION TECHNOLOGY IN CONSTRUCTION
AB  - The increasing digitalization of the construction industry, driven by Building Information Modeling (BIM) and the rise of digital twins, necessitates a holistic approach to worker well-being. Understanding how digital tools and processes, including BIM-based workflows and digital twin applications, impact the psychological and physiological states of construction workers is crucial for improving safety, productivity, and overall job satisfaction. This study integrates construction practices and neuroscience by systematically reviewing quantitative parameters and tools for assessing worker well-being within various digital construction workflows, with a specific focus on BIM and digital twin applications. We identify key stress detection parameters (e.g., EDA, HRV) and tools from medical research applicable to construction management for enhancing worker well-being and mitigating risks. A comprehensive literature review synthesizes findings from multiple disciplines, focusing on stress detection techniques and their application in optimizing digital construction processes, specifically within BIM-driven projects and the development and utilization of digital twins. Results highlight stress detection parameters and tools offering valuable insights into worker experience, emphasizing the need for both qualitative and quantitative measures in project management, particularly within the context of BIM and digital twin technologies. A holistic, interdisciplinary approach merging ergonomics, neuroscience, and construction methodologies is crucial for enhancing worker experience in increasingly digitalized construction environments. Integrating stress detection technologies into construction management processes, especially those leveraging BIM and digital twins, is essential for promoting worker well-being and safety, while acknowledging limitations in current systematic research. Future exploration includes developing human-centered digital tools within BIM and digital twin workflows and applying medical findings to improve construction workflows. This research aims to inspire construction professionals to prioritize worker well-being and adapt their methodologies to address the unique challenges of digital transformation in the industry, leveraging the potential of BIM and digital twins to create safer and more productive work environments.
SN  - 1874-4753
PY  - 2024
VL  - 29
SP  - 1257
EP  - 1274
DO  - 10.36680/j.itcon.2024.056
AN  - WOS:001400549600001
ER  -

TY  - CPAPER
AU  - Jäkel, JI
AU  - Kaus, M
AU  - Klemt-Albert, K
ED  - Strauss, A
ED  - Bergmeister, K
TI  - Sustainability assessment of bridge structures in the operation phase based on a digital twin
T2  - EUROPEAN ASSOCIATION ON QUALITY CONTROL OF BRIDGES AND STRUCTURES, EUROSTRUCT 2023, VOL 6, ISS 5
CP  - Annual Meeting of the European-Association-on-Quality-Control-of-Bridges-and-Structures (EUROSTRUCT)
AB  - There are about 65,000 bridge structures in the infrastructure network of the Federal Republic of Germany. Due to the long operating phase of bridges, they have an influence on sustainability. To evaluate the sustainability of bridges in the operational phase, digital building models can serve as a consistent and structured database to increase the efficiency and transparency.
   In this article, an innovative evaluation system for measuring and assessing the sustainability of a bridge structure in the operational phase based on a digital bridge twin is developed. First, the workflow is presented in a process model and the necessary requirements for the digital twin of the bridge structure are defined. In addition, the semi-automatic sustainability assessment system is described and the linkage via an interoperable interface to the digital twin is designed. The approach is tested and validated using a real demonstrator. The result of the paper shows an approach to use digital twins for a novel and semi-automatic sustainability assessment in the operation of bridge structures.
PY  - 2023
SP  - 667
EP  - 674
DO  - 10.1002/cepa.1988
AN  - WOS:001256546000151
ER  -

TY  - JOUR
AU  - Salem, T
AU  - Dragomir, M
AU  - Chatelet, E
TI  - Strategic Integration of Drone Technology and Digital Twins for Optimal Construction Project Management
T2  - APPLIED SCIENCES-BASEL
AB  - This research aims to develop an integrated approach to construction project management by integrating digital technology into monitoring and surveillance operations. Through the use of drones and image processing software, data can be updated regularly and accurately about the progress at the construction site, allowing managers and decision makers to have a clear view of the current situation and make effective decisions based on accurate. In addition, this approach contributes to improving communication and coordination among project team members, as data and images can be easily and effectively shared, reducing opportunities for error and enhancing effective interaction among different parties. Using digital twin technologies, planning and forecasting processes can also be improved, as comprehensive analysis of digital data provides a deeper understanding of project dynamics, identifies potential risks, and enables appropriate preventive measures to be taken. In conclusion, the integration of digital twins and the use of drones in construction projects represent a significant step towards achieving smarter and more efficient management, and successfully achieving the defined goals with greater effectiveness.
SN  - 2076-3417
SN  - 2076-3417
DA  - JUN
PY  - 2024
VL  - 14
IS  - 11
C7  - 4787
DO  - 10.3390/app14114787
AN  - CCC:001252171400001
ER  -

TY  - CPAPER
AU  - Li, MH
AU  - Wang, RH
AU  - Zhou, X
AU  - Zhu, ZM
AU  - Wen, YG
AU  - Tan, R
A1  - ACM
TI  - ChatTwin: Toward Automated Digital Twin Generation for Data Center via Large Language Models
T2  - PROCEEDINGS OF THE 10TH ACM INTERNATIONAL CONFERENCE ON SYSTEMS FOR ENERGY-EFFICIENT BUILDINGS, CITIES, AND TRANSPORTATION, BUILDSYS 2023
CP  - 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation (BuildSys)
AB  - Digital twin has been applied in various industrial fields to represent physical systems. However, the design of high-fidelity digital scenes is challenging in that it often requires intensive manual processes and domain expertise to edit the 3D models or description documents. To reduce human efforts, this paper proposes Chat-Twin, a conversational system that leverages the power of GPT-4 to automate the generation of scene description documents for digital twins. ChatTwin assists scene generation by i) segmenting userinput prompts, ii) generating scenes with segmented prompts, and iii) optimizing the generated content. Specifically, the Segment-and-Generate (SG) workflow decomposes the long-text generation into several subtasks and reduces the complexity of the original task. The evaluation through our data center digital twin system shows that ChatTwin outperforms other baselines in terms of generation accuracy and efficiency.
SN  - 979-8-4007-0230-3
PY  - 2023
SP  - 208
EP  - 211
DO  - 10.1145/3600100.3623719
AN  - WOS:001147988300022
ER  -

TY  - JOUR
AU  - Lakhan, A
AU  - Mohammed, MA
AU  - Zebar, DA
AU  - Abdulkareem, KH
AU  - Deveci, M
AU  - Marhoon, HA
AU  - Nedoma, J
AU  - Martinek, R
TI  - DT-LSMAS: Digital Twin-Assisted Large-Scale Multiagent System for Healthcare Workflows
T2  - IEEE SYSTEMS JOURNAL
AB  - Digital healthcare has garnered much attention from academia and industry for health and well-being. Many digital healthcare architectures based on large-scale edge and cloud multiagent systems (LSMASs) have recently been presented. The LSMAS allows agents from different institutions to work together to achieve healthcare processing goals for users. This article presents a digital twin large-scale multiagent strategy (DT-LSMAS) comprising mobile, edge, and cloud agents. The DT-LSMAS comprised different schemes for healthcare workflows, such as added healthcare workflows, application partitioning, and scheduling. We consider healthcare workflows with different biosensor data such as heartbeat, blood pressure, glucose monitoring, and other healthcare tasks. We partitioned workflows into mobile, edge, and cloud agents to meet the deadline, total time, and security of workflows in large-scale edge and cloud nodes. To handle the large-scale resource for real-time sensor data, we suggested digital twin-enabled edge nodes, where delay-sensitive workflow tasks are scheduled and executed under their quality of service requirements. Simulation results show that the DT-LSMAS outperformed in terms of total time by 50%, minimizing the risk of resource leakage and deadline missing during scheduling on heterogeneous nodes. In conclusion, the DT-LSMAS obtained optimal results for workflow applications.
SN  - 1932-8184
SN  - 1937-9234
SN  - 1937-9234
DA  - DEC
PY  - 2024
VL  - 18
IS  - 4
SP  - 1883
EP  - 1892
DO  - 10.1109/JSYST.2024.3424259
C6  - JUL 2024
AN  - CCC:001378277800012
ER  -

TY  - JOUR
AU  - Gonzalez-Caceres, A
AU  - Hunger, F
AU  - Forssén, J
AU  - Somanath, S
AU  - Mark, A
AU  - Naserentin, V
AU  - Bohlin, J
AU  - Logg, A
AU  - Waestberg, B
AU  - Komisarczyk, D
AU  - Edelvik, F
AU  - Hollberg, A
TI  - Towards digital twinning for multi-domain simulation workflows in urban design: a case study in Gothenburg
T2  - JOURNAL OF BUILDING PERFORMANCE SIMULATION
AB  - This work proposes an automated workflow using digital twinning for multi-domain environmental performance analysis of urban developments. Digital twins can potentially provide a common basis for multi-domain simulations and help overcome data availability and interoperability issues. The proposed workflow consists of five steps: (1) creating a procedural urban 3D model, (2) generating design alternatives parametrically, (3) exporting the context and each design alternative to each simulation tool, (4) running simulations for wind comfort, energy demand, and noise for each design alternative, and (5) combining and visualizing the simulation results using the digital twin. The workflow was applied to a neighbourhood in Sweden, the resultsreveal significant reduction in manual work when applying multiple simulation software for different domains. This is one step forward in streamlining the workflow for urban analysis, crucial for multi-domain optimization. In the future, further domains and simulation tools can be added to the workflow.
SN  - 1940-1493
SN  - 1940-1507
DA  - MAY 4
PY  - 2025
VL  - 18
IS  - 3
SP  - 311
EP  - 332
DO  - 10.1080/19401493.2024.2320112
C6  - FEB 2024
AN  - WOS:001175611600001
ER  -

TY  - JOUR
AU  - Ellul, C
AU  - Hamilton, N
AU  - Pieri, A
AU  - Floros, G
TI  - Exploring Data for Construction Digital Twins: Building Health and Safety and Progress Monitoring Twins Using the Unreal Gaming Engine
T2  - BUILDINGS
AB  - Although digital twins have been established in manufacturing for a long time, they are only more recently making their way into the urban environment and present a relatively new concept for the construction industry. The concept of a digital twin-a model of the physical environment that has a real-time two-way link between the physical and the digital, with the virtual model changing over time to reflect changes in the real world-lends itself well to the continually changing environment of a construction project. Predictive capabilities built into a twin also have great potential for construction planning-including in supply chain management and waste disposal as well as in the construction process itself. Underpinning this opportunity is location data, which model where something is happening and when and can be used to solve a wide range of problems. In particular, location (the power of where) can integrate diverse data sources and types into a single system, overcoming interoperability challenges that are known to be a barrier to twin implementation. This paper demonstrates the power of location-enabled digital twins in the context of a highway construction project, documenting and addressing data engineering tasks and functionality development to explore the potential of digital twins in the context of two case studies-health and safety and construction monitoring. We develop two demonstrators using data from an existing construction project (building on data and requirements from industry partner Skanska) to build twins that make use of the powers of 4D data presentation offered by the Unreal Gaming Engine and CesiumJS mapping, while software development expertise is sometimes available to construction firms, we specifically explore to what extent the no-code approach available within Unreal can be deployed in this context. Our findings provide evidence to construction companies as to the benefits of digital twins, as well as an understanding of the data engineering and technical skills required to achieve these benefits. The overall results demonstrate the potential for digital twins to unlock and democratise construction data, taking them beyond the niche use of experts and into the boardroom.
SN  - 2075-5309
SN  - 2075-5309
DA  - JUL
PY  - 2024
VL  - 14
IS  - 7
C7  - 2216
DO  - 10.3390/buildings14072216
AN  - CCC:001276547200001
ER  -

TY  - CPAPER
AU  - Lin, LY
AU  - Athe, P
AU  - Rouxeli, P
AU  - Dinh, N
AU  - Lane, J
A1  - Assoc Comp Machinery
TI  - DEVELOPMENT AND ASSESSMENT OF A NEARLY AUTONOMOUS MANAGEMENT AND CONTROL SYSTEM DURING A SINGLE LOSS OF FLOW ACCIDENT
T2  - PROCEEDINGS OF THE 2020 INTERNATIONAL CONFERENCE ON NUCLEAR ENGINEERING (ICONE2020), VOL 1
CP  - International Conference on Nuclear Engineering (ICONE)
AB  - In this work, a Nearly Autonomous Management and Control (NAMAC) system is designed to diagnose the reactor state and provide recommendations to the operator for maintaining the safety and performance of the reactor. A three layer-hierarchical workflow is suggested to guide the design and development of the NAMAC system. The three layers in this workflow corresponds to knowledge base, digital twin developmental layer (for different NAMAC functions), and NAMAC operational layer. Digital twin in NAMAC is described as knowledge acquisition system to support different autonomous control functions. Therefore, based on the knowledge base, a set of digital twin models is trained to determine the plant state, predict behavior of physical components or systems, and rank available control options. The trained digital twin models are assembled according to NAMAC operational workflow to support decision-making process in selecting the optimal control actions during an accident scenario.
   To demonstrate the capability of the NAMAC system, a case study is designed, where a baseline NAMAC is implemented for operating a simulator of the Experimental Breeder Reactor II (EBR-II) during a single loss of flow accident. Training database for development of digital twin models is obtained by sampling the control parameters in the GOTHIC data generation engine. After the training and testing, the digital twins are assembled into a NAMAC system according to the operational workflow. This NAMAC system is coupled with the GOTHIC plant simulator, and a confusion matrix is generated to illustrate the accuracy and robustness of implemented NAMAC system. It is found that within the training databases, NAMAC can make reasonable recommendations with zero confusion rate. However, when the scenario is beyond the training cases, the confusion rate increases, especially when the scenarios are more severe. Therefore, a discrepancy checker is added to detect unexpected reactor states and alert operators for safety-minded actions.
SN  - 978-0-7918-8376-1
PY  - 2020
C7  - V001T06A033
AN  - WOS:000850727000113
ER  -

TY  - CPAPER
AU  - West, TD
AU  - Blackburn, M
ED  - Dagli, CH
TI  - Is Digital Thread/Digital Twin Affordable? A Systemic Assessment of the Cost of DoD's Latest Manhattan Project
T2  - COMPLEX ADAPTIVE SYSTEMS CONFERENCE WITH THEME: ENGINEERING CYBER PHYSICAL SYSTEMS, CAS
CP  - Complex Adaptive Systems Conference on Engineering Cyber Physical Systems (CAS)
AB  - This paper analyzes challenges the U.S. Air Force will inevitably face as it implements Digital Thread/Digital Twin, a model-centric approach to weapon system development, to include developing the needed computational capability, software tools, and workforce to leverage that capability. A Cost Constructive Model (COCOMO) II based analysis is performed to roughly estimate the scope, cost, and time required to develop robust Digital Thread/Digital Twin models for the Next Generation Air Dominance (NGAD) aircraft. The authors determine that such a thoroughly unprecedented model development effort will require a national commitment on par with the Manhattan Project, which yielded the first atomic bomb. (c) 2017 The Authors. Published by Elsevier B.V.
SN  - 1877-0509
PY  - 2017
VL  - 114
SP  - 47
EP  - 56
DO  - 10.1016/j.procs.2017.09.003
AN  - WOS:000419234000006
ER  -

TY  - JOUR
AU  - Salem, T
AU  - Dragomir, M
AU  - Zinveli, A
AU  - Blagu, DA
TI  - EMPLOYING DIGITAL TWINS IN ENGINEERING PROJECT MANAGEMENT WITH APPLICATIONS IN CONSTRUCTIONS
T2  - ACTA TECHNICA NAPOCENSIS SERIES-APPLIED MATHEMATICS MECHANICS AND ENGINEERING
AB  - The study focuses on bridging the knowledge gap between modern technologies and construction project management, aiming for a clear vision of digitizing construction project management through the effective use of big data on digital twin platforms. By employing a suite of specialized digital tools, the proposal is to establish a practical system for managing construction projects, enhancing modeling, and data processing, and reducing associated risks. The investigation underscores the importance of a robust database developed through mature information systems and building an efficient model on digital twin platforms. The conclusions highlight the need for solid foundations for digital twins, including mature information systems for monitoring changes based on intelligent computational models. Efficient management of big data within BIM contributes to resolving technical and administrative issues in construction projects.
SN  - 1221-5872
SN  - 2393-2988
DA  - SEP
PY  - 2024
VL  - 67
IS  - 3
SP  - 543
EP  - 548
AN  - WOS:001425211200019
ER  -

TY  - JOUR
AU  - Serbouti, I
AU  - Chenal, J
AU  - Tazi, SA
AU  - Baik, A
AU  - Hakdaoui, M
TI  - Digital Transformation in African Heritage Preservation: A Digital Twin Framework for a Sustainable Bab Al-Mansour in Meknes City, Morocco
T2  - SMART CITIES
AB  - Highlights What are the main findings? - The study develops a Digital Twin Heritage Building (DTHB) framework based on digital transformation, integrating HBIM, IoT sensors, and predictive analytics to address structural and environmental challenges. - The DTHB framework enables real-time monitoring, scenario simulations, and predictive maintenance, offering a replicable model for African heritage sites. What is the implication of the main finding? - The research demonstrates how digital transformation addresses Africa's unique heritage preservation challenges through scalable and sustainable solutions. - By preserving cultural landmarks like Bab Al-Mansour, the study enhances sustainable development, cultural identity, and tourism across the African continent.Highlights What are the main findings? - The study develops a Digital Twin Heritage Building (DTHB) framework based on digital transformation, integrating HBIM, IoT sensors, and predictive analytics to address structural and environmental challenges. - The DTHB framework enables real-time monitoring, scenario simulations, and predictive maintenance, offering a replicable model for African heritage sites. What is the implication of the main finding? - The research demonstrates how digital transformation addresses Africa's unique heritage preservation challenges through scalable and sustainable solutions. - By preserving cultural landmarks like Bab Al-Mansour, the study enhances sustainable development, cultural identity, and tourism across the African continent.Abstract The advent of digital transformation has redefined the preservation of cultural heritage and historic sites through the integration of Digital Twin technology. Initially developed for industrial applications, Digital Twins are now increasingly employed in heritage conservation as dynamic, digital replicas of physical assets and environments. These systems enable detailed, interactive approaches to documentation, management, and preservation. This paper presents a detailed framework for implementing Digital Twin technology in the management of heritage buildings. By utilizing advanced methods for data collection, processing, and analysis, the framework creates a robust data hub for Digital Twin Heritage Buildings (DTHB). This architecture enhances real-time monitoring, improves accuracy, reduces operational costs, and enables predictive maintenance while minimizing invasive inspections. Focusing on Bab Al-Mansour Gate in Meknes, Morocco, a significant cultural landmark, this research outlines the workflow for developing a Bab Al-Mansour DTHB platform. The platform monitors structural health and detects damage over time, offering a dynamic tool for conservation planning. By integrating innovative technologies with data-driven solutions, this study provides a replicable model for preserving heritage sites, addressing critical gaps in real-time monitoring, resource optimization, and environmental risk mitigation.
SN  - 2624-6511
DA  - FEB
PY  - 2025
VL  - 8
IS  - 1
C7  - 29
DO  - 10.3390/smartcities8010029
AN  - WOS:001431790800001
ER  -

TY  - JOUR
AU  - Sacks, R
AU  - Brilakis, I
AU  - Pikas, E
AU  - Xie, HS
AU  - Girolami, M
TI  - Construction with digital twin information systems
T2  - DATA-CENTRIC ENGINEERING
AB  - The concept of a "digital twin" as a model for data-driven management and control of physical systems has emerged over the past decade in the domains of manufacturing, production, and operations. In the context of buildings and civil infrastructure, the notion of a digital twin remains ill-defined, with little or no consensus among researchers and practitioners of the ways in which digital twin processes and data-centric technologies can support design and construction. This paper builds on existing concepts of Building Information Modeling (BIM), lean project production systems, automated data acquisition from construction sites and supply chains, and artificial intelligence to formulate a mode of construction that applies digital twin information systems to achieve closed loop control systems. It contributes a set of four core information and control concepts for digital twin construction (DTC), which define the dimensions of the conceptual space for the information used in DTC workflows. Working from the core concepts, we propose a DTC information system workflow-including information stores, information processing functions, and monitoring technologies-according to three concentric control workflow cycles. DTC should be viewed as a comprehensive mode of construction that prioritizes closing the control loops rather than an extension of BIM tools integrated with sensing and monitoring technologies.
SN  - 2632-6736
PY  - 2020
VL  - 1
C7  - e14
DO  - 10.1017/dce.2020.16
AN  - WOS:000851324300014
ER  -

TY  - JOUR
AU  - Zacherl, L
AU  - Baumann, T
TI  - Digital Twin of an Open Cooling Tower: Experimental Studies and Numerical Validation
T2  - ACS ENGINEERING AU
AB  - Increasing dynamics in surface water bodies and all-time low groundwater levels, both a consequence of global warming, put high stress on the water supply chain and require a re-evaluation of all water uses. Industry uses a significant amount of water for cooling, often in open cooling towers. We developed a digital twin for an evaporative open cooling tower, focusing on the hydrochemistry to optimize water consumption and use of inhibitor chemicals to prevent scaling. The model is based on the USGS hydrochemical standard model PhreeqC, which is controlled by Python scripts. The digital twin implements evaporation in the cooling tower, recharge of water with added inhibitors, and desalination to avoid corrosion. In contrast to previous operation strategies, which rely on a thickening ratio that can be measured using the electrical conductivity, the model allows prediction of the behavior of the cooling tower based on the saturation index for the mineral precipitates. Additionally, the digital twin offers the option of controlling the cooling tower. We present a workflow to adapt the digital twin to the actual cooling tower and to parametrize the chemicals used for the prevention of mineral scaling. The optimization objectives were to reduce the consumption of inhibitors while maintaining stable hydrochemical conditions and a benign corrosion behavior. Additionally, the digital twin should reveal possibilities for demand-driven load balancing. After site-specific adaptation of flow and evaporation rates, volumes, temperatures, and equilibrium constants for the inhibitors, the model was able to forecast the hydrochemical conditions in the cooling tower. The parameter and sensitivity analyses revealed that the total volume of water in the system and the thickening ratio have a large effect on water consumption. While slightly increased concentrations of the inhibitor would allow for significantly higher thickening ratios and slightly lower water consumption, the corrosion stability of the materials in the cooling system puts limits on this approach. Evaporation remains the main factor in water consumption. For the reference site, the digital twin revealed that the implemented operation scheme was already close to optimal conditions, considering water consumption and the use of inhibitors.
SN  - 2694-2488
DA  - AUG 20
PY  - 2025
VL  - 5
IS  - 4
SP  - 347
EP  - 358
DO  - 10.1021/acsengineeringau.4c00052
C6  - JUN 2025
AN  - WOS:001518451900001
ER  -

TY  - JOUR
AU  - Jyeniskhan, N
AU  - Keutayeva, A
AU  - Kazbek, G
AU  - Ali, MH
AU  - Shehab, E
TI  - Integrating Machine Learning Model and Digital Twin System for Additive Manufacturing
T2  - IEEE ACCESS
AB  - Additive manufacturing is a promising manufacturing process with diverse applications, but ensuring the quality and reliability of the manufactured products are key challenges. The digital twin has emerged as a technology solution to address these challenge, allowing real-time monitoring and control of the manufacturing process. This paper proposes a digital twin system framework for additive manufacturing that integrates machine learning models, employing Unity, OctoPrint, and Raspberry Pi for real-time control and monitoring. Particularly, the system utilizes machine learning models for defect detection, achieving an Average Precision (AP) score of 92%, with specific performance metrics of 91% for defected objects and 94% for non-defected objects, demonstrating high efficiency. The Unity client user interface is also developed for control and visualization, facilitating easy additive manufacturing process monitoring. This research article presents a detailed description of the proposed digital twin framework and its workflow for implementation, the machine learning models, and the Unity client user interface. It also demonstrates the effectiveness of the integrated system through case studies and experimental results. The main findings show that the proposed digital twin system met its functional requirements and effectively detects defects and provides real-time control and monitoring of the additive manufacturing process. This paper contributes to the growing field of digital twin technology and additive manufacturing, providing a promising solution for enhancing the quality and reliability in the field of additive manufacturing.
SN  - 2169-3536
PY  - 2023
VL  - 11
SP  - 71113
EP  - 71126
DO  - 10.1109/ACCESS.2023.3294486
AN  - WOS:001033508600001
ER  -

TY  - CPAPER
AU  - Fox-Ivey, R
AU  - Laurent, J
AU  - Petitclerc, B
ED  - Ozer, H
ED  - Rushing, JF
ED  - Leng, Z
TI  - Using 3D Pavement Surveys to Create a Digital Twin of Your Runway or Highway
T2  - AIRFIELD AND HIGHWAY PAVEMENTS 2021: AIRFIELD PAVEMENT TECHNOLOGY
CP  - International Airfield and Highway Pavements Conference of the Transportation-and-Development-Institute (T and DI) of the American-Society-of-Civil-Engineers (ASCE)
AB  - Highway departments and airports perform regular pavement condition inspections in order to record pavement distress data, which are important inputs into pavement management systems (PMS). Increasingly pavement condition data are captured using high-speed 3D lasers and high-accuracy blended inertial positioning systems. This enables fast, accurate, and objective pavement assessment. Recent advances in 3D scanning and GNSS positioning data integration now permit these routine pavement inspections to deliver much more value; effectively creating a "3D Digital Twin" of roads and runway surfaces. These 3D Digital Twins can provide a much richer data set to the asset manager while reducing closure time, data collection costs, and staff exposure. 3D Digital Twins can be directly imported into road design software applications and used to generate volumetric estimates, preliminary, and final designs. Outputs of 3D design software can then be sent to laser tracking total stations in order to control 3D pavers and milling machines such that better-designed surfaces are now possible. This paper and presentation present details of the capture technology, key workflow steps, and finally point density and accuracy compared to traditional.
SN  - 978-0-7844-8352-7
PY  - 2021
SP  - 180
EP  - 192
AN  - WOS:000692653900016
ER  -

TY  - JOUR
AU  - Stavropoulos, P
AU  - Papacharalampopoulos, A
AU  - Sabatakakis, K
AU  - Mourtzis, D
TI  - Metamodelling of Manufacturing Processes and Automation Workflows towards Designing and Operating Digital Twins
T2  - APPLIED SCIENCES-BASEL
AB  - The automation of workflows for the optimization of manufacturing processes through digital twins seems to be achievable nowadays. The enabling technologies of Industry 4.0 have matured, while the plethora of available sensors and data processing methods can be used to address functionalities related to manufacturing processes, such as process monitoring and control, quality assessment and process modelling. However, technologies succeeding Computer-Integrated Manufacturing and several promising techniques, such as metamodelling languages, have not been exploited enough. To this end, a framework is presented, utilizing an automation workflow knowledge database, a classification of technologies and a metamodelling language. This approach will be highly useful for creating digital twins for both the design and operation of manufacturing processes, while keeping humans in the loop. Two process control paradigms are used to illustrate the applicability of such an approach, under the framework of certifiable human-in-the-loop process optimization.
SN  - 2076-3417
DA  - FEB
PY  - 2023
VL  - 13
IS  - 3
C7  - 1945
DO  - 10.3390/app13031945
AN  - WOS:000929239500001
ER  -

TY  - JOUR
AU  - Oluwasegun, A
AU  - Jung, JC
TI  - Original The application of machine learning for the prognostics and health management of control element drive system
T2  - NUCLEAR ENGINEERING AND TECHNOLOGY
AB  - Digital twin technology can provide significant value for the prognostics and health management (PHM) of critical plant components by improving insight into system design and operating conditions. Digital twinning of systems can be utilized for anomaly detection, diagnosis and the estimation of the system's remaining useful life in order to optimize operations and maintenance processes in a nuclear plant. In this regard, a conceptual framework for the application of digital twin technology for the prognosis of Control Element Drive Mechanism (CEDM), and a data-driven approach to anomaly detection using coil current profile are presented in this study. Health management of plant components can capitalize on the data and signals that are already recorded as part of the monitored parameters of the plant's instrumentation and control systems. This work is focused on the development of machine learning algorithm and workflow for the analysis of the CEDM using the recorded coil current data. The workflow involves features extraction from the coil-current profile and consequently performing both clustering and classification algorithms. This approach provides an opportunity for health monitoring in support of condition-based predictive maintenance optimization and in the development of the CEDM digital twin model for improved plant safety and availability. (C) 2020 Korean Nuclear Society, Published by Elsevier Korea LLC. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
SN  - 1738-5733
DA  - OCT
PY  - 2020
VL  - 52
IS  - 10
SP  - 2262
EP  - 2273
DO  - 10.1016/j.net.2020.03.028
AN  - WOS:000560710100011
ER  -

TY  - JOUR
AU  - Zheng, Y
AU  - Al Barazi, A
AU  - Seppänen, O
AU  - Abou-Ibrahim, H
AU  - Görsch, C
TI  - Semantic digital twin framework for monitoring construction workflows
T2  - AUTOMATION IN CONSTRUCTION
AB  - As construction workflows become increasingly dynamic, there is a growing need for Digital Twins (DTs) to support integrated, real-time workflow monitoring. However, establishing DTs in construction remains challenging due to fragmented data sources and the lack of systematic semantic integration methods. This paper investigates how semantic web ontologies can be systematically applied to establish a semantic DT for monitoring construction workflows. Accordingly, a DT framework (DiCon-DT) is proposed, utilizing an ontology network to model and integrate diverse data into a semantic DT data lake, and further enabling simulation and contextual interpretation. Validated through a furniture installation case study, the framework successfully enabled semantic data integration and supported predictive and cognitive tasks for construction monitoring. Future research should focus on extending the ontology network, automating semantic data mapping, and validating the framework at larger complex project scales.
SN  - 0926-5805
SN  - 1872-7891
DA  - AUG
PY  - 2025
VL  - 176
C7  - 106301
DO  - 10.1016/j.autcon.2025.106301
AN  - WOS:001501442900001
ER  -

TY  - JOUR
AU  - Salem, T
AU  - Dragomir, M
TI  - Options for and Challenges of Employing Digital Twins in Construction Management
T2  - APPLIED SCIENCES-BASEL
AB  - The notions of smart construction and smart or digital cities include many modern concepts that are advocated today, especially in countries with advanced economies, and depend on using information technology and the Internet of Things as a basis to automate processes and activate digital systems to manage activities and services related to the operation of buildings and urban structures. In light of the spread of digital technology and modern managerial approaches, the concept of a digital twin is being used on a large scale with the current trend and direction to digitalize activities providing many economic, social and technical advantages. A digital twin is a system in which a virtual representation of a real entity or physical system is used continuously by being fed with data and deriving outputs in the form of decisions and actions that are generated through the processes of machine learning, simulation, development and lifecycle management. This study aims to review the literature on construction project management through the lens of digital twins and ways to use them in the field to improve operational results. The authors propose a framework for analyzing and supervising the development of digital twins that uses three main stages: the commonly encountered Building Information Modeling (BIM); the existing monitoring and actuation digital twins; and an envisioned third stage that makes use of artificial intelligence, complex visualization instruments and advanced controls with the capability to exact change within a construction project on the building site.
SN  - 2076-3417
SN  - 2076-3417
DA  - MAR
PY  - 2022
VL  - 12
IS  - 6
C7  - 2928
DO  - 10.3390/app12062928
AN  - INSPEC:23331371
ER  -

TY  - JOUR
AU  - Torres, J
AU  - San-Mateos, R
AU  - Lasarte, N
AU  - Mediavilla, A
AU  - Sagarna, M
AU  - León, I
TI  - Building Digital Twins to Overcome Digitalization Barriers for Automating Construction Site Management
T2  - BUILDINGS
AB  - Construction sites are highly unpredictable environments involving a wide variety of stakeholders with complex information exchanges, which lead to the well-known inefficiencies and unproductivity of the construction sector. The adoption of Building Digital Twins (BDT) in the construction site is a promising solution to this issue, by automating data acquisition and knowledge extraction processes and providing what-if scenario simulation capabilities. Furthermore, the current research sets the principles to define, replicate, and scale-up the architecture of a Building Digital Twin Platform (BDTP), conceived as a scalar ecosystem, which allows to seamlessly manage on-site construction processes, integrating cross-cutting domains for the construction site optimization (Progress monitoring, Quality control, Operational Health and Safety, Equipment control, and Production planning). The starting point of the research is a comprehensive diagnosis of on-site process inefficiencies and the barriers to its digitalization leading to the user requirements, which have been underpinned by questionnaires and interviews addressed within an open innovation user-centered approach around Living Labs. The research has been conceived following the Design Science Research (DSR) methodology and based on the Plan-Do-Check-Act (PDCA) analysis for the continuous improvement of the construction process. By means of the adoption of the standard Business Process Model and Notation (BPMN), based on the BDTP architecture, the research has resulted in BPMN workflows stemmed from the Digital Twin (DT) where the DT itself is an actor in a service-oriented data-exchange workflow. Moreover, the use of a BDTP can pave the way for the transition from user-driven construction management to hybrid management, coexisting with both human and digital actors and merging expert knowledge with artificial intelligence techniques.
SN  - 2075-5309
DA  - JUL
PY  - 2024
VL  - 14
IS  - 7
C7  - 2238
DO  - 10.3390/buildings14072238
AN  - WOS:001276599200001
ER  -

TY  - JOUR
AU  - Osadcha, I
AU  - Klumbyte, E
AU  - Jurelionis, A
AU  - Spudys, P
AU  - Hartmann, T
AU  - Saket, S
AU  - Harasymczuk, D
AU  - Fokaides, P
TI  - Towards interoperable building energy performance simulation: A digital Twin perspective
T2  - JOURNAL OF BUILDING ENGINEERING
AB  - Seamless integration between Building Information Modeling (BIM) and Building Energy Performance Simulation (BEPS) plays an important role in the sustainable design and operation of buildings. This study explores the main causes of interoperability issues in the BIM-to-BEPS workflow, highlighting the limitations of the standardization framework and file-based data exchange methods. Analysis of domain-specific ISO standards showed that most focus on organizational processes and overlook semantic interoperability, which is essential for automated data exchange. An experimental evaluation of two BIM-to-BEPS workflows using Autodesk Revit, EnergyPlus, and TRNSYS further confirmed that even when open data standards such as IFC are used, translation errors in both geometry and semantics are common and require significant manual correction. The paper argues that reliance on file-based exchange creates barriers to interoperability and proposes a shift toward semantic web technologies, linked data, and API integration to better support the widespread adoption of Digital Twins.
SN  - 2352-7102
DA  - SEP 15
PY  - 2025
VL  - 110
C7  - 113059
DO  - 10.1016/j.jobe.2025.113059
AN  - WOS:001507494700001
ER  -

TY  - JOUR
AU  - Talmaki, SA
AU  - Kamat, VR
TI  - Sensor Acquisition and Allocation for Real-Time Monitoring of Articulated Construction Equipment in Digital Twins
T2  - SENSORS
AB  - The visibility available to an equipment operator on a dynamic construction site can often be blocked by various obstacles such as materials, temporary or permanent facilities, other equipment, and workers. Equipment monitoring in real-time digital twins can thus play a crucial role in accident prevention. This paper develops a scalable technical approach and presents a prototype application framework for transmitting real world sensor data to update 3D equipment models inside a graphical digital twin for concurrent visualization of a monitored construction operation. The developed framework and workflow can be extended to visualize any construction operation, as it occurs, inside a dynamic 3D world simply by outfitting the real equipment with appropriate sensors and connecting them to their virtual counterparts. The implemented proof-of-concept interface is described in the context of a real-time 3D digital twin for assisting excavator operators prevent unintended strikes with underground utilities. Experiments to validate the proposed technical approach by simulating the real-time motion of a backhoe loader's articulated arm using orientation sensors installed on its boom, stick, and bucket are described. The experimental results characterize the scope and potential reasons for spatio-temporal discrepancies that can occur between a monitored real operation and its replicated digital twin. The effect of an operator warning mechanism based on preset safety thresholds is also investigated and described.
SN  - 1424-8220
SN  - 1424-8220
DA  - OCT
PY  - 2022
VL  - 22
IS  - 19
C7  - 7635
DO  - 10.3390/s22197635
AN  - MEDLINE:36236733
ER  -

TY  - JOUR
AU  - Dubs, L
AU  - Charitatos, V
AU  - Buoso, S
AU  - Wegener, S
AU  - Winklhofer, S
AU  - Alkadhi, H
AU  - Kurtcuoglu, V
TI  - Assessment of extracranial carotid artery disease using digital twins - A pilot study
T2  - NEUROIMAGE-CLINICAL
AB  - To improve risk stratification in extracranial internal carotid artery disease (CAD), patients who would benefit maximally from revascularization must be identified. In cardiology, the fractional flow reserve (FFR) has become a reference standard for evaluating the functional severity of coronary artery stenosis, and noninvasive surro-gates thereof relying on computational fluid dynamics (CFD) have been developed. Here, we present a CFD-based workflow using digital twins of patients' carotid bifurcations derived from computed tomography angiography for the noninvasive functional assessment of CAD. We reconstructed patient-specific digital twins of 37 carotid bifurcations. We implemented a CFD model using common carotid artery peak systolic velocity (PSV) acquired with Doppler ultrasound (DUS) as inlet boundary condition and a two-element Windkessel model as oulet boundary condition. The agreement between CFD and DUS on the PSV in the internal carotid artery (ICA) was then compared. The relative error for the agreement between DUS and CFD was 9% & PLUSMN; 20% and the intraclass correlation coefficient was 0.88. Furthermore, hyperemic simulations in a physiological range were feasible and unmasked markedly different pressure drops along two ICA stenoses with similar degree of narrowing under comparable ICA blood flow. Hereby, we lay the foundation for prospective studies on noninvasive CFD-based derivation of metrics similar to the FFR for the assessment of CAD.
SN  - 2213-1582
PY  - 2023
VL  - 38
C7  - 103435
DO  - 10.1016/j.nicl.2023.103435
C6  - MAY 2023
AN  - WOS:001060774100001
ER  -

TY  - JOUR
AU  - Fedeli, A
AU  - Di Salle, A
AU  - Micucci, D
AU  - Rebelo, L
AU  - Rossi, MT
AU  - Mariani, L
AU  - Iovino, L
TI  - How low-code platforms support digital twins of processes
T2  - SOFTWARE AND SYSTEMS MODELING
AB  - Digital Twin of Processes, also defined as process digital twins (PDTs), are emerging as a feasible solution for modeling, monitoring, and optimizing business processes by providing real-time, data-driven insights into operational workflows. However, designing, developing, and maintaining PDTs can be complex and resource-intensive, often requiring highly specialized expertise in software engineering and domain-specific processes. This paper proposes insights and guidelines into using low-code development platforms (LCDPs) to simplify and expedite the modeling and deployment of PDTs, leveraging intuitive, visual development environments and pre-built components. We identified 11 core characteristics that define PDTs and assessed the potential of LCDPs to support their design, development, and execution. The applicability of this framework is demonstrated through three case studies of littering management, order management, and guest invitations, where we evaluate how well LCDPs address the key requirements of PDT implementation. Our results indicate that while LCDPs offer significant advantages in terms of ease of adoption and cost efficiency, several challenges remain, particularly around scalability and process performance. At the same time, we propose lessons learned from these experiences that could help address these challenges in future implementations.
SN  - 1619-1366
SN  - 1619-1374
DA  - 2025 JUL 16
PY  - 2025
DO  - 10.1007/s10270-025-01310-4
C6  - JUL 2025
AN  - WOS:001530185600001
ER  -

TY  - JOUR
AU  - Guo, MS
AU  - Lv, X
AU  - Wang, D
AU  - Chen, H
AU  - Wei, F
TI  - Innovative integration of computer vision, IoT, and digital twin in food quality and safety assessment
T2  - TRENDS IN FOOD SCIENCE & TECHNOLOGY
AB  - Background: Ensuring food quality and safety is a key priority for public health and economic stability. Traditional methods of food quality assessment, while effective, are often labor-intensive, destructive or lack traceability and transparency. Recent advances in deep learning and computer vision introduce digitally intelligent, cost-effective and automated solutions. Scope and approach: This review presents a typical workflow of deep learning and computer vision, from data acquisition and data preprocessing to model selection, training and evaluation for validation, and summarizes the applications of deep learning and computer vision in different areas of food, such as image classification, object detection, image segmentation, and image generation, as well as model optimization strategies for different tasks. The applications of Internet of Things (IoT), digital twin, computer vision, and deep learning technologies in the food industry are highlighted. In addition, this review also discusses transfer learning and model compression methods, and reviews the applications of lightweight models and embedded systems in the food industry. Key findings and conclusions: The innovative integration of technologies such as computer vision, deep learning, IoT, and digital twin has enhanced food traceability and transparency, and promoted sustainable development. The advancement of cloud computing and big data technologies has promoted the deep integration of these technologies, enabling real-time, accurate and dynamic decision-making in food production. Looking forward to the future, the focus of future research should be placed on improving the availability and quality of labeled datasets, enhancing the interpretability and robustness of model.
SN  - 0924-2244
SN  - 1879-3053
DA  - SEP
PY  - 2025
VL  - 163
C7  - 105176
DO  - 10.1016/j.tifs.2025.105176
AN  - CCC:001548109200001
ER  -

TY  - CPAPER
AU  - Gamage, TPB
AU  - Elsayed, A
AU  - Lin, CC
AU  - Wu, A
AU  - Feng, Y
AU  - Yu, JW
AU  - Gao, LK
AU  - Wijenayaka, S
AU  - Nash, MP
AU  - Doyle, AJ
AU  - Nickerson, DP
A1  - IEEE
TI  - Vision for the 12 LABOURS Digital Twin Platform
T2  - 2023 45TH ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE & BIOLOGY SOCIETY, EMBC
CP  - 45th Annual International Conference of the IEEE-Engineering-in-Medicine-and-Biology-Society (EMBC)
AB  - Clinical translation of personalised computational physiology workflows and digital twins can revolutionise healthcare by providing a better understanding of an individual's physiological processes and any changes that could lead to serious health consequences. However, the lack of common infrastructure for developing these workflows and digital twins has hampered the realisation of this vision. The Auckland Bioengineering Institute's 12 LABOURS project aims to address these challenges by developing a Digital Twin Platform to enable researchers to develop and personalise computational physiology models to an individual's health data in clinical workflows. This will allow clinical trials to be more efficiently conducted to demonstrate the efficacy of these personalised clinical workflows. We present a demonstration of the platform's capabilities using publicly available data and an existing automated computational physiology workflow developed to assist clinicians with diagnosing and treating breast cancer. We also demonstrate how the platform facilitates the discovery and exploration of data and the presentation of workflow results as part of clinical reports through a web portal. Future developments will involve integrating the platform with health systems and remote-monitoring devices such as wearables and implantables to support home-based healthcare. Integrating outputs from multiple workflows that are applied to the same individual's health data will also enable the generation of their personalised digital twin.
SN  - 1557-170X
SN  - 1558-4615
SN  - 979-8-3503-2447-1
PY  - 2023
DO  - 10.1109/EMBC40787.2023.10341138
AN  - WOS:001133788305021
ER  -

TY  - JOUR
AU  - Tripathi, I
AU  - Froese, TM
AU  - Mallory-Hill, S
TI  - Applicability of BIM-IoT-GIS integrated digital twins for post occupancy evaluations
T2  - FRONTIERS IN BUILT ENVIRONMENT
AB  - Post Occupancy Evaluations (POE) provided a systematic methodology for determining the performance gap between expected and actual performance. Monitoring quality of the indoor environment is essential for understanding building performance in relation to occupant health, wellbeing, and comfort. Because of the global COVID-19 pandemic, researchers faced numerous issues accessing the building for collecting data and making spot measurements of the indoor environment. Technologies such as Building Information Modeling (BIM), Internet of Things (IoT), and Geographical Information Systems (GIS) have the potential to address existing challenges for data collection, analysis, and visualization in post occupancy evaluations. This study aims to explore the applications of a BIM-IoT-GIS-integrated digital twin for post occupancy evaluations. First, high-level use case scenarios are developed to derive system requirements for a digital twin platform. Second, four tests are conducted that provide a step-by-step procedure for BIM-IoT-GIS integration. Third, the integration is validated by geo-reference checks, data transfer checks, and visual checks. Based on the tests, a streamlined workflow is recommended for similar/future projects. The results demonstrate that Revit-ArcGIS Pro integration meets the system requirements for post occupancy evaluations. Moreover, as shown in the graphical abstract (Figure), the spatial-temporal capabilities of ArcGIS Pro enable continuous monitoring and visualization of building performance in 4D. In conclusion, BIM-IoT-GIS integration can provide a solid foundation for developing a centralized digital twin for post occupancy evaluations and enables researcher to collect and analyze the data without being physically present in the building.
SN  - 2297-3362
DA  - JUN 9
PY  - 2023
VL  - 9
C7  - 1103743
DO  - 10.3389/fbuil.2023.1103743
AN  - WOS:001019940100001
ER  -

TY  - JOUR
AU  - Green, AC
AU  - Lewis, E
AU  - Tong, X
AU  - Wardle, R
TI  - A framework for incorporating rainfall data into a flooding digital twin
T2  - JOURNAL OF HYDROLOGY
AB  - The use of digital twins in operational flood management has garnered attention for its potential to enhance real-time flood risk monitoring and the deployment of timely interventions. With increasing access to realtime rainfall data from national meteorological services, low-cost sensors networks, citizen science data and dense observation networks in 'urban observatories', there is immense potential for developing detailed digital twins for flooding. However, the useability of these real-time rainfall data sources, including the reliability and quality of rainfall data for real-time digital twin applications has not yet been analysed. This study investigates the suitability of current real-time rainfall data sources for digital twin applications and identifies barriers to operationalising a flooding digital twin. Using the PYRAMID (near) real-time dynamic flood modelling platform and Newcastle upon Tyne (U.K.) as a demonstrator - due to its high data availability and diversity of rainfall data sources - we evaluate the reliability of rain gauge and radar rainfall data from the Environment Agency, U.K. Meteorological Office and additional sources, including citizen science and urban observatory data. The reliability of real-time data is shown to be a major barrier to digital twin deployment due to variability in data quality and intermittent data streams, even with national rainfall data provided by government organisations. To address these challenges, we propose a blending algorithm that adapts to changing data availability in real-time, implemented within the PYRAMID workflow. Flood depths are shown to be sensitive to data blends, with peak rainfall rates varying by a factor of 10, leading to flood depths differing by up to 15%. This paper highlights the critical need for improved rainfall data reliability to enable the operational use of digital twins for flood management.
SN  - 0022-1694
SN  - 1879-2707
DA  - AUG
PY  - 2025
VL  - 656
C7  - 132893
DO  - 10.1016/j.jhydrol.2025.132893
C6  - AUG 2025
AN  - WOS:001446903300001
ER  -

TY  - JOUR
AU  - Zhang, H
AU  - Yan, JW
AU  - Yang, J
AU  - Meng, W
AU  - Chen, SJ
TI  - Two-stage point cloud registration using multi-scale edge convolution for digital twin-based bridge construction progress monitoring
T2  - AUTOMATION IN CONSTRUCTION
AB  - Progress monitoring is an important part of bridge engineering project management. However, traditional bridge construction progress monitoring methods mainly rely on manual records and on-site inspections, which are cumbersome, inefficient and inaccurate. A bridge construction progress monitoring framework based on digital twins is proposed. By comparing the planned progress in the virtual world with the actual progress in the physical world, qualitative and quantitative evaluation of the overall construction progress and the construction progress of local components is achieved. In addition, a two-stage point cloud registration network based on multi-scale edge convolution is proposed to achieve the matching and comparison between the as-designed BIM model and the point cloud model. Finally, the effectiveness of the proposed method is evaluated on synthetic datasets and real bridge data. The experimental results show that compared with other methods, the proposed method effectively improves the accuracy of bridge construction progress monitoring.
SN  - 0926-5805
SN  - 1872-7891
DA  - OCT
PY  - 2025
VL  - 178
C7  - 106415
DO  - 10.1016/j.autcon.2025.106415
AN  - WOS:001548920200002
ER  -

TY  - CPAPER
AU  - Teizer, J
AU  - Johansen, KW
AU  - Schultz, C
ED  - Jazizadeh, F
ED  - Shealy, T
ED  - Garvin, MJ
TI  - The Concept of Digital Twin for Construction Safety
T2  - CONSTRUCTION RESEARCH CONGRESS 2022: COMPUTER APPLICATIONS, AUTOMATION, AND DATA ANALYTICS
CP  - Construction Research Congress (CRC) on Project Management and Delivery, Contracts, and Design and Materials
AB  - "Digital twins" as models for information-driven management and control of physical systems have emerged over the past years in multiple industrial sectors and recently also in construction. However, in the domain of construction safety, a digital twin remains undefined, with little or no consensus among researchers and practitioners of two essential aspects: (1) the connection between the physical reality of a construction site (the "physical" twin) and the corresponding computer model (the "digital" twin), and (2) the most effective selection and exploitation of reallife data for supporting safe design, planning, and execution of construction. This paper outlines the concept for a Digital Twin for Construction Safety (DTCS), defining three essential steps in the digital twin workflow: (1) safe design and planning for hazard prevention, (2) risk monitoring and control for proactive prediction and warning, and (3) continuous performance improvement for personalized- or project-based learning. DTCS should be viewed as a system-based approach enhancing the overall safety performance rather than exclusively integrating sensing information or safety knowledge in Building Information Modeling (BIM) for safety purposes. The result is an outline of our vision of the DTCS and a description of its components. Additionally, we point toward future research on the topic.
SN  - 978-0-7844-8396-1
PY  - 2022
SP  - 1156
EP  - 1165
AN  - WOS:000776568400121
ER  -

TY  - JOUR
AU  - Inamdar, A
AU  - van Driel, WD
AU  - Zhang, GQ
TI  - Digital Twin Technology-A Review and Its Application Model for Prognostics and Health Management of Microelectronics
T2  - ELECTRONICS
AB  - Digital Twins (DT) play a key role in Industry 4.0 applications, and the technology is in the process of being mature. Since its conceptualisation, it has been heavily contextualised and often misinterpreted as being merely a virtual model. Thus, it is crucial to define it clearly and have a deeper understanding of its architecture, workflow, and implementation scales. This paper reviews the notion of a Digital Twin represented in the literature and analyses different kinds of descriptions, including several definitions and architectural models. A new fit-for-all definition is proposed which describes the underlying technology without being context-specific and also overcomes the pitfalls of the existing generalised definitions. In addition, the existing three-dimensional and five-dimensional models of the DT architecture and their characteristic features are analysed. A new simplified two-branched model of DT is introduced, which retains a clear separation between the real and virtual spaces and outlines the latter based on the two key modelling approaches. This model is then extended for condition monitoring of electronic components and systems, and a hybrid approach to Prognostics and Health Management (PHM) is further elaborated on. The proposed framework, enabled by the two-branched Digital Twin model, combines the physics-of-degradation and data-driven approaches and empowers the next generation of reliability assessment methods. Finally, the benefits, challenges, and outlook of the proposed approach are also discussed.
SN  - 2079-9292
DA  - AUG
PY  - 2024
VL  - 13
IS  - 16
C7  - 3255
DO  - 10.3390/electronics13163255
AN  - WOS:001305199800001
ER  -

TY  - JOUR
AU  - Shi, JB
AU  - Dourthe, L
AU  - Li, DN
AU  - Deng, L
AU  - Louback, L
AU  - Song, F
AU  - Abolins, N
AU  - Verano, F
AU  - Zhang, PS
AU  - Groover, J
AU  - Falla, DG
AU  - Li, K
TI  - Real- Time Underreamer Vibration Predicting, Monitoring, and Decision- Making Using Hybrid Modeling and a Process Digital Twin
T2  - SPE DRILLING & COMPLETION
AB  - In hole enlargement while drilling (HEWD) operations, underreamers are used extensively to enlarge the pilot hole. Reamer wipeout failure can cause additional bottomhole assembly (BHA) trips, which can cost operators millions of dollars. Excessive reamer shock and vibration are leading causes of reamer wipeout; therefore, careful monitoring of reamer vibration is important in mitigating such a risk. Currently, downhole vibration sensors and drilling dynamics simulations (DDSs) are used to comprehend and reduce downhole vibration, but vibration sensors cannot be placed exactly at the reamer to monitor the vibrations in real time. DDSs are difficult to calibrate and are computationally expensive for use in real time; therefore, the real -time reamer vibration status is typically unknown during drilling operations. A process digital twin using a hybrid modeling approach is proposed and tested to address the vibration issue. Large amounts of field data are used in advanced DDSs to calibrate the HEWD runs. For each HEWD section, calibrated DDSs are performed to comprehend the downhole vibration at the reamer and downhole vibration sensors. A surrogate regression model between reamer vibration and sensor vibration is built using machine learning. This surrogate model is implemented in a drilling monitoring software platform as a process digital twin. During drilling, the surrogate model uses downhole measurement while drilling (MWD) data as inputs to predict reamer vibration. Wipeout risk levels are calculated and sent to the operators for real -time decision-making to reduce the possibility of reamer wipeout. Large volumes of reamer field data, including field recorded vibration and reamer dull conditions were used to validate the digital twin workflow. Then, the process digital twin was implemented and tested in two reamer runs in the Gulf of Mexico. A down -hole high-frequency sensor was placed 8 ft above the reamer cutting structure in one field run, and the recorded sensor vibration data and corresponding reamer dull conditions showed a very good match with the real -time digital twin predictions in a low-vibration scenario. Cases in high vibration are needed to fully validate the feasibility and accuracy of the digital twin. State - of-the -art downhole sensors, DDS packages, large amounts of field data, and a hybrid approach are the solutions to building, calibrating, and field testing the reamer digital twin to ensure its effectiveness and accuracy. Such a hybrid modeling approach can not only be applied to reamers but also to other critical BHA components.
SN  - 1064-6671
SN  - 1930-0204
SN  - 1930-0204
DA  - JUN
PY  - 2023
VL  - 38
IS  - 2
SP  - 201
EP  - 219
DO  - 10.2118/208795-pa
AN  - CCC:001040342700004
ER  -

TY  - JOUR
AU  - Han, MQ
AU  - Zhang, JH
AU  - Huang, YJ
AU  - Xu, JW
AU  - Chen, X
AU  - Chen, BM
TI  - Enhancing worker monitoring and management on large-scale construction sites with UAVs and digital twin modeling
T2  - AUTOMATION IN CONSTRUCTION
AB  - Monitoring large-scale work sites is challenging, particularly in vast outdoor areas. Unmanned aerial vehicles (UAVs) provide an effective solution for site monitoring and worker management. This paper introduces a UAV-based framework integrated with digital twin (DT) modeling to enhance real-time data management and worker authorization verification. The pretrained YOLO-LCA model improved detection accuracy from 31.5% to 96.4%. The framework combines multi-object tracking with 3D site reconstruction, enabling precise global registration and situational awareness. Cross-referencing UAV detections with GPS-enabled worker IDs ensures that only authorized personnel are present, effectively identifying unapproved workers. The proposed framework has undergone large-scale validation across multiple construction projects in Hong Kong, demonstrating significant potential for modernizing work site management. By integrating UAVs and DT technology, this framework supports efficient monitoring, operational safety, and informed decision-making, providing a scalable approach to addressing the demands of large-scale construction site management.
SN  - 0926-5805
SN  - 1872-7891
DA  - JUN
PY  - 2025
VL  - 174
C7  - 106108
DO  - 10.1016/j.autcon.2025.106108
C6  - MAR 2025
AN  - WOS:001446356300001
ER  -

TY  - JOUR
AU  - Jin, SS
AU  - Yu, FY
AU  - Wang, BY
AU  - Zhang, M
AU  - Wang, YL
TI  - Research on a Real-Time Control System for Discrete Factories Based on Digital Twin Technology
T2  - APPLIED SCIENCES-BASEL
AB  - Gear factories are most typical discrete manufacturing factories. Many gear factories are striving to explore how to achieve intelligent manufacturing in order to improve efficiency and reduce costs. Digital twin technology is currently one of the most reliable ways to achieve intelligent manufacturing. This article aims to establish a real-time control system in order to promote intelligent manufacturing for discrete manufacturing factories. Firstly, a model for a digital twin gear factory is put forward based on the characteristics of gear factories, and the composition of a real-time control system for gear factories is clarified. Then, a human-computer interaction architecture for the real-time control system is proposed. The real-time control system consists of three parts as follows: a monitoring module, a virtualizing module, and a controlling module. At work, it appears as a kind of human-machine interaction form with the three following interfaces: a monitoring window, a virtualizing window, and a controlling window. Finally, a gear factory, which is specialized in manufacturing the intermediate shaft dual gear of a new energy vehicle gearbox, develops a set of software for the real-time control system. The prototype software is obtained through some development activities such as 3D MAX and WebGL virtualization modeling and OPC UA and REST communication design.
SN  - 2076-3417
SN  - 2076-3417
DA  - MAY
PY  - 2024
VL  - 14
IS  - 10
C7  - 4076
DO  - 10.3390/app14104076
AN  - INSPEC:25303129
ER  -

TY  - JOUR
AU  - Daraba, D
AU  - Pop, F
AU  - Daraba, C
TI  - Digital Twin Used in Real-Time Monitoring of Operations Performed on CNC Technological Equipment
T2  - APPLIED SCIENCES-BASEL
AB  - Featured Application Digital Twin used in real-time monitoring of operations performed on CNC technological equipment that will retrieve the relevant parameters of machine working.Abstract This article presents the development and implementation of a real-time monitoring solution designed for CNC machines, specifically applied to 150 industrial printing machines, leveraging Digital Twin (DT) technology. The system integrates an SQL database with Android and .NET interfaces, ensuring seamless data synchronization across all machines and optimizing production processes. The real-time monitoring enables immediate reflection of operational changes, enhancing predictive maintenance and reducing machine downtime. A notable feature of the system is its 1 s average data synchronization rate per machine, managing 150 resources distributed over a 10,000 mp area. This fast synchronization improves workflow coordination, reducing production time by approximately 10%, and minimizing operator delays caused by material issues, machine malfunctions, or product defects. The integration of advanced analytics further supports real-time decision-making, predictive maintenance, and performance optimization, aligning the solution with the objectives of Industry 4.0 and Industry 5.0 initiatives. This version reflects the specific results of the research, including the 1 s synchronization rate, the 10% reduction in production time, and the scalability of the system for 150 resources.
SN  - 2076-3417
DA  - NOV
PY  - 2024
VL  - 14
IS  - 22
C7  - 10088
DO  - 10.3390/app142210088
AN  - WOS:001366917200001
ER  -

TY  - JOUR
AU  - Al-Najjar, A
AU  - Rao, NSV
TI  - Virtual Infrastructure Twin for Computing-Instrument Ecosystems: Software and Measurements
T2  - IEEE ACCESS
AB  - Advanced science workflows are orchestrated over complex ecosystems consisting of computing platforms and physical instruments connected over wide-area networks. The availability of physical facilities of these ecosystems may be limited during typically long software development periods, due to the expense, disruption risk and limited readiness during the design and construction stages. We present a Virtual Infrastructure Twin (VIT) of such an ecosystem to support the software development and testing for multi-site computing-instrument ecosystems. A VIT is implemented using virtual hosts connected over emulated local-area networks at sites, which are in turn connected over an emulated wide-area network. We describe the design and implementation details of a VIT including: (i) host and network emulations and the incorporation of instrument simulators, (ii) services for remote instrument steering, and (iii) performance analytics based on network throughput and disk input/output measurements. We illustrate a use case by emulating an ecosystem of four geographically distributed laboratory sites with services for accessing and steering instruments and estimating throughput profiles using physical testbed measurements.
SN  - 2169-3536
PY  - 2023
VL  - 11
SP  - 20254
EP  - 20266
DO  - 10.1109/ACCESS.2023.3246954
AN  - WOS:000946236900001
ER  -

TY  - JOUR
AU  - Khalaj, O
AU  - Hassas, P
AU  - Masek, B
AU  - Stadler, C
AU  - Svoboda, J
TI  - Optimization of cooling rate of Q-P treated 42SiCr steel using AI digital twinning
T2  - HELIYON
AB  - In the continuously advancing field of mechanical engineering, digitalization is bringing a major transformation, specifically with the concept of digital twins. Digital twins are dynamic digital models of real-world systems and processes, crucial for Industry 4.0 and the emerging Industry 5.0, which are changing how humans and machines work together in manufacturing. This paper explores the combination of physics-based and data-driven modeling using advanced Artificial Intelligence (AI) and Machine Learning (ML) techniques. This approach provides a comprehensive understanding of mechanical systems, improving materials design and manufacturing processes. The focus is on the advanced 42SiCr alloy, where AI-driven digital twinning is used to optimize cooling rates during Quenching and Partitioning (Q-P) treatments. This leads to significant improvements in the mechanical properties of 42SiCr steel. Given its complex properties influenced by various factors, this alloy is perfect for digital twinning. The Q-P heat treatment process not only restores the material's deformability but also gives it advanced high-strength steel (AHSS) properties. The findings show how AI and ML can effectively guide the development of high-strength steels and enhance their treatment processes. Additionally, integrating digital twins with new technologies like the Metaverse offers exciting possibilities for simulated production, remote monitoring, and collaborative design. By establishing a clear workflow from physical to digital twins and presenting empirical results, this paper connects theoretical modeling with practical applications, paving the way for smarter manufacturing solutions in mechanical engineering. Furthermore, this paper analyzes how digital twins can be integrated into advanced technologies like the Metaverse, opening up new possibilities for simulated production, remote monitoring, design collaboration, training simulations, analytics, and complete supply chain visibility. This integration is a crucial step toward realizing the full potential of digitalization in mechanical engineering.
SN  - 2405-8440
DA  - JUN 15
PY  - 2024
VL  - 10
IS  - 11
C7  - e32101
DO  - 10.1016/j.heliyon.2024.e32101
C6  - JUN 2024
AN  - WOS:001286620000001
ER  -

TY  - JOUR
AU  - Zech, P
AU  - Jäger, A
AU  - Schneiderbauer, L
AU  - Exenberger, H
AU  - Fröch, G
AU  - Flora, M
TI  - Agile Construction Digital Twin Engineering
T2  - BUILDINGS
AB  - Digital twins have attracted a lot of attention recently. However, the current manifestations are merely digital shadows, lacking means for bidirectional data exchange, which makes their use for assisting the construction of buildings much more difficult. We argue that this is due to the lack of a systematic process for developing a digital twin during a building's life cycle. We argue to look for a solution by combining agile engineering with IT change management to establish an agile, change-driven process for engineering digital twins. Such a process, of course, deserves a qualitative assessment of the engineering process and the resulting digital twin. In the future, it should be possible to obtain a digital twin from a BIM-based design process by applying IT change management in an agile manner. This should happen under maximum automation and life cycle orientation. Our proposal is motivated by several years of interdisciplinary collaboration between civil engineering and computer science and evaluated using the Technology Acceptance Model. While the TAM is not specifically designed for digital twin methodologies, its application here aims to assess perceived usefulness and ease of use of DT methodologies from the user's perspective, without addressing scalability concerns. This aims to provide actionable insights to guide the refinement of the process model, aligning it with user requirements and achieving its intended outcomes. Our evaluation confirms the proposed process's perceived usefulness and ease of use, with robust correlations indicating strong acceptance potential among stakeholders. These results highlight the feasibility of the proposed approach and its alignment with expectations in real-world applications.
SN  - 2075-5309
DA  - FEB
PY  - 2025
VL  - 15
IS  - 3
C7  - 386
DO  - 10.3390/buildings15030386
AN  - WOS:001419231900001
ER  -

TY  - JOUR
AU  - Yildiz, O
TI  - Digital Twin-Based Ray Tracing Analysis for Antenna Orientation Optimization in Wireless Networks
T2  - ELECTRONICS
AB  - Efficient antenna orientation of transmitters is essential for improving wireless signal quality and coverage, especially in large-scale and complex 6G networks. Identifying the best antenna angles is difficult due to the nonlinear interaction among orientation, signal propagation, and interference. This paper introduces a digital twin-based evaluation approach utilizing ray tracing simulations to assess the influence of antenna orientation on critical performance metrics: path gain, received signal strength (RSS), and signal-to-interference-plus-noise ratio (SINR). A thorough array of orientation scenarios was simulated to produce a dataset reflecting varied coverage conditions. The dataset was utilized to investigate antenna configurations that produced the optimal and suboptimal performance for each parameter. Additionally, three machine learning models-k-nearest neighbors (KNN), multi-layer perceptron (MLP), and XGBoost-were developed to forecast ideal configurations. XGBoost had superior prediction accuracy compared to the other models, as evidenced by regression outcomes and cumulative distribution function (CDF) analyses. The proposed workflow demonstrates that learning-based predictors can uncover orientation refinements that conventional grid sweeps overlook, enabling agile, interference-aware optimization. Key contributions include an end-to-end digital twin methodology for rapid what-if analysis and a systematic comparison of lightweight machine learning predictors for antenna orientation. This comprehensive method provides a pragmatic and scalable solution for the data-driven optimization of wireless systems in real-world settings.
SN  - 2079-9292
DA  - JUL 29
PY  - 2025
VL  - 14
IS  - 15
C7  - 3023
DO  - 10.3390/electronics14153023
AN  - WOS:001548735800001
ER  -

TY  - CPAPER
AU  - Hugues, J
AU  - Hristosov, A
AU  - Hudak, JJ
AU  - Yankel, J
A1  - ACM
TI  - TwinOps - DevOps meets Model-Based Engineering and Digital Twins for the engineering of CPS
T2  - 23RD ACM/IEEE INTERNATIONAL CONFERENCE ON MODEL DRIVEN ENGINEERING LANGUAGES AND SYSTEMS, MODELS 2020 COMPANION
CP  - 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems (MODELS)
AB  - The engineering of Cyber-Physical Systems (CPS) requires a large set of expertise to capture the system requirements and to derive a correct solution. Model-based Engineering and DevOps aim to efficiently deliver software with increased quality. Model-based Engineering relies on models as first-class artifacts to analyze, simulate, and ultimately generate parts of a system. DevOps focuses on software engineering activities, from early development to integration, and then improvement through the monitoring of the system at run-time. We claim these can be efficiently combined to improve the engineering process of CPS.
   In this paper, we present TwinOps, a process that unifies Model-based Engineering, Digital Twins, and DevOps practice in a uniform workflow. TwinOps illustrates howto leverage several best practices in MBE and DevOps for the engineering Cyber-Physical systems. We illustrate our contribution using a Digital Twins case study to illustrate TwinOps benefits, combining AADL and Modelica models, and an IoT platform.
SN  - 978-1-4503-8135-2
PY  - 2020
DO  - 10.1145/3417990.3421446
AN  - WOS:001334498000079
ER  -

TY  - JOUR
AU  - Zhang, H
AU  - Yan, JW
AU  - Yang, J
AU  - Meng, W
AU  - Chen, SJ
TI  - Two-stage point cloud registration using multi-scale edge convolution for digital twin-based bridge construction progress monitoring
T2  - AUTOMATION IN CONSTRUCTION
AB  - Progress monitoring is an important part of bridge engineering project management. However, traditional bridge construction progress monitoring methods mainly rely on manual records and on-site inspections, which are cumbersome, inefficient and inaccurate. A bridge construction progress monitoring framework based on digital twins is proposed. By comparing the planned progress in the virtual world with the actual progress in the physical world, qualitative and quantitative evaluation of the overall construction progress and the construction progress of local components is achieved. In addition, a two-stage point cloud registration network based on multi-scale edge convolution is proposed to achieve the matching and comparison between the as-designed BIM model and the point cloud model. Finally, the effectiveness of the proposed method is evaluated on synthetic datasets and real bridge data. The experimental results show that compared with other methods, the proposed method effectively improves the accuracy of bridge construction progress monitoring.
SN  - 0926-5805
SN  - 1872-7891
DA  - OCT
PY  - 2025
VL  - 178
C7  - 106415
DO  - 10.1016/j.autcon.2025.106415
AN  - WOS:001554443300001
ER  -

TY  - JOUR
AU  - Su, WM
AU  - Huang, Y
AU  - Zhang, B
AU  - Chen, JQ
AU  - Sheng, F
TI  - Development of a digital twin-driven bridge construction safety monitoring system
T2  - PROCEEDINGS OF THE INSTITUTION OF CIVIL ENGINEERS-ENGINEERING SUSTAINABILITY
AB  - The construction phases of bridges involve a wide range of elements and tasks, generating a vast amount of data in terms of type and volume. Traditional manual and document-based methods are no longer sufficient for effective and accurate project management. Consequently, more comprehensive and systematic management approaches are required. This study proposes a three-dimensional digital twin information display platform that links with the actual state of the structure, facilitating bridge construction monitoring and safety warning through the integration of digital twin technology with multi-source monitoring information. A dynamic multi-resolution digital twin model for bridge construction has been established with a bridge digital model and structural finite element model. Then a structural evaluation and warning method focusing on key stress and deformation indicators has been comprehensively formulated. A comprehensive digital twin information monitoring and management system platform has been developed for bridge construction, complete with a feedback mechanism for warning information. The experimental case application in the Zhanlin Yellow River Bridge construction process demonstrates that this method enables flexible management of the construction process and dynamic evaluation of safety status. These advancements contribute to ensuring bridge construction safety and provide administrators with a digital, real-time online management platform.
SN  - 0032-4728
SN  - 1478-4629
SN  - 1477-4747
SN  - 1751-7680
DA  - 2025 APR 22
PY  - 2025
DO  - 10.1080/00324728.2025.2531823
DO  - 10.1680/jensu.24.00136
C6  - APR 2025
AN  - WOS:001537848300001
ER  -

TY  - JOUR
AU  - de Parga, SA
AU  - Bravo, JR
AU  - Sibuet, N
AU  - Hernandez, JA
AU  - Rossi, R
AU  - Boschert, S
AU  - Quintana-Ortí, ES
AU  - Tomás, AE
AU  - Tatu, CC
AU  - Vázquez-Novoa, F
AU  - Ejarque, J
AU  - Badia, RM
TI  - Parallel reduced-order modeling for digital twins using high-performance computing workflows
T2  - COMPUTERS & STRUCTURES
AB  - The integration of reduced-order models with high-performance computing is critical for developing digital twins, particularly for real-time monitoring and predictive maintenance of industrial systems. This paper presents a comprehensive, high-performance computing-enabled workflow for developing and deploying projection-based reduced-order models for large-scale mechanical simulations. We use PyCOMPSs' parallel framework to efficiently execute reduced-order model training simulations, employing parallel singular value decomposition algorithms such as randomized singular value decomposition, Lanczos singular value decomposition, and full singular value decomposition based on tall-skinny QR. Moreover, we introduce a partitioned version of the hyperreduction scheme known as the Empirical Cubature Method to further enhance computational efficiency in projection-based reduced-order models for mechanical systems. Despite the widespread use of high-performance computing for projection-based reduced-order models, there is a significant lack of publications detailing comprehensive workflows for building and deploying end-to-end projection-based reduced-order models in high-performance computing environments. Our workflow is validated through a case study focusing on the thermal dynamics of a motor, a multiphysics problem involving convective heat transfer and mechanical components. The projection-based reduced-order model is designed to deliver a real-time prognosis tool that could enable rapid and safe motor restarts post-emergency shutdowns under different operating conditions, demonstrating its potential impact on the practice of simulations in engineering mechanics. To facilitate deployment, we use the High-Performance Computing Workflow as a Service strategy and Functional Mock-Up Units to ensure compatibility and ease of integration across high-performance computing, edge, and cloud environments. The outcomes illustrate the efficacy of combining projection-based reduced-order models and high-performance computing, establishing a precedent for scalable, real-time digital twin applications in computational mechanics across multiple industries.
SN  - 0045-7949
SN  - 1879-2243
DA  - SEP
PY  - 2025
VL  - 316
C7  - 107867
DO  - 10.1016/j.compstruc.2025.107867
AN  - WOS:001521409600002
ER  -

TY  - CPAPER
AU  - Jiao, J
AU  - Yuan, G
AU  - Liu, XJ
AU  - Tian, GD
AU  - Pham, DT
ED  - Fera, M
ED  - Caterino, M
ED  - Macchiaroli, R
ED  - Pham, DT
TI  - Digital Twin-driven Dynamic Scheduling Cloud Platform for Disassembly Workshop
T2  - ADVANCES IN REMANUFACTURING, IWAR 2023
CP  - 7th International Workshop on Autonomous Remanufacturing (IWAR)
AB  - As an emerging technology that integrates multi-physics, multi-scale and multi-discipline properties, digital twinning can enable the interaction between the physical and information worlds. Disassembly has been widely used as an important method for manufacturing and green manufacturing. However, the uncertainty of the disassembly shop and the isolation of the disassembly information seriously affect the effective operation of the disassembly shop. In this paper, we propose a disassembly scheduling cloud platform based on digital twinning techniques. Compared with intelligent algorithms, it can effectively address the uncertainty factor in the disassembly shop. First, we build a workflow model graph for the disassembly scheduling cloud platform. Second, the whole process monitoring system of the disassembly scheduling workshop is constructed to monitor the disassembly dynamics in real time. Third, we build a digital twinning-based dynamic perturbation architecture for the disassembly scheduling cloud platform and use big data analysis techniques to predict and diagnose dynamic perturbations from multiple sources. Finally, the effectiveness of the proposed framework is validated by enterprise instances, on which the disassembly scheduling cloud platform is applied.
SN  - 2195-4356
SN  - 2195-4364
SN  - 978-3-031-52648-0
SN  - 978-3-031-52649-7
PY  - 2024
SP  - 265
EP  - 279
DO  - 10.1007/978-3-031-52649-7_21
AN  - WOS:001274189000021
ER  -

TY  - JOUR
AU  - Ramnarine, ID
AU  - Sherif, TA
AU  - Alorabi, AH
AU  - Helmy, H
AU  - Yoshida, T
AU  - Murayama, A
AU  - Yang, PPJ
TI  - Urban revitalization pathways toward zero carbon emissions through systems architecting of urban digital twins
T2  - ENVIRONMENT AND PLANNING B-URBAN ANALYTICS AND CITY SCIENCE
AB  - Recognizing the critical role of cities in mitigating greenhouse gas emissions, many cities are adopting carbon neutrality goals as part of their climate action strategies. The efficacy of these initiatives, however, has been undermined by complexity of systemic problems, ineffectiveness in planning implementation, and lack of stakeholder engagement. Urban and community-level carbon reduction should transcend urban design and systems optimization to incorporate multi-faceted dimensions in urban contexts. To address these challenges, this paper proposes a framework of urban digital twins that includes digital representation, performance modeling, design interventions and interactive platform for decisions over temporal processes. The CANVAS, or Carbon Neutrality Architecting New Visions for Architectural Systems, is a systems architecting approach to modeling the process of urban revitalization for achieving carbon neutrality by 2050. The developed workflow integrates multidisciplinary approaches for carbon mapping, gap identification, alternative generation, Urban Building Energy Modeling (UBEM) simulation, evaluation, and decision-making to demonstrates applicability of the proposed framework through a case study of the Nihonbashi district in Tokyo. The approach revealed that Energy Use Intensity (EUI) can be decreased by 99 kWh/m2/y through reconstruction and operational improvements. Emerging photovoltaic technologies can further cut EUI by an average of 42.5 kWh/m2/y, although results vary significantly in respect to building characteristics, particularly geometry and floor area. The incremental, cyclical systems architecting approach revealed that a 97% reduction in carbon emissions could be achieved by the seventh cycle through stakeholder-centric system interventions. This paper contributes to the development of urban digital twin methodologies by integrating systems architecting concepts with UBEM as transformative tools for carbon neutral urban design and development.
SN  - 2399-8083
SN  - 2399-8091
DA  - 2025 FEB 4
PY  - 2025
DO  - 10.1177/23998083251318142
C6  - FEB 2025
AN  - WOS:001413412400001
ER  -

TY  - JOUR
AU  - Kaewunruen, S
AU  - AbdelHadi, M
AU  - Kongpuang, M
AU  - Pansuk, W
AU  - Remennikov, AM
TI  - Digital Twins for Managing Railway Bridge Maintenance, Resilience, and Climate Change Adaptation
T2  - SENSORS
AB  - Innovative digital twins (DTs) that allow engineers to visualise, share information, and monitor the condition during operation is necessary to optimise railway construction and maintenance. Building Information Modelling (BIM) is an approach for creating and managing an inventive 3D model simulating digital information that is useful to project management, monitoring and operation of a specific asset during the whole life cycle assessment (LCA). BIM application can help to provide an efficient cost management and time schedule and reduce the project delivery time throughout the whole life cycle of the project. In this study, an innovative DT has been developed using BIM integration through a life cycle analysis. Minnamurra Railway Bridge (MRB), Australia, has been chosen as a real-world use case to demonstrate the extended application of BIM (i.e., the DT) to enhance the operation, maintenance and asset management to improve the sustainability and resilience of the railway bridge. Moreover, the DT has been exploited to determine GHG emissions and cost consumption through the integration of BIM. This study demonstrates the feasibility of DT technology for railway maintenance and resilience optimisation. It also generates a virtual collaboration for co-simulations and co-creation of values across stakeholders participating in construction, operation and maintenance, and enhancing a reduction in costs and GHG emission.
SN  - 1424-8220
DA  - JAN
PY  - 2023
VL  - 23
IS  - 1
C7  - 252
DO  - 10.3390/s23010252
AN  - WOS:000909670200001
ER  -

TY  - CPAPER
AU  - Sirigu, G
AU  - Carminati, B
AU  - Ferrari, E
ED  - Ferrara, AL
ED  - Krishnan, R
TI  - Human Digital Twins: Efficient Privacy-Preserving Access Control Through Views Pre-materialisation
T2  - DATA AND APPLICATIONS SECURITY AND PRIVACY XXXVIII, DBSEC 2024
CP  - 38th Annual IFIP WG 11.3 Conference on Data and Applications Security and Privacy (DBSec)
AB  - Digital Twins (DTs) are virtual copies of physical entities, processes, or systems used for various tasks, such as controlling, monitoring, and analysing the status of the real entity. The DT sector is expected to surpass six billion U.S. dollars by 2025, with the Human Digital Twin (HDT) being a prime example. HDTs are being used in various applications, such as personalised medicine, healthcare, and education. However, the materialisation of HDTs can be costly and lead to delays in HDT-based services. To overcome this, we propose a strategy, HDT-ViewMat, to identify the portions of an HDT that should be pre-materialised, considering the trade-off between potential delays and resource waste. The proposed strategy analyses the process/workflow that requires HDT data to estimate the probability of its tasks being executed. Furthermore, due to the sensitivity of the data maintained by the HDTs, access to them must be limited to guarantee the users' privacy. This strategy also considers the compliance of privacy policies with users' preferences. HDT-ViewMat assesses the user's chance of executing a task in the workflow based on the probability of the task's invocation and the probability of the user accepting the policies of the corresponding service provider.
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-031-65171-7
SN  - 978-3-031-65172-4
PY  - 2024
VL  - 14901
SP  - 24
EP  - 43
DO  - 10.1007/978-3-031-65172-4_2
AN  - WOS:001313706100002
ER  -

TY  - JOUR
AU  - Luger, M
AU  - Seidel, A
AU  - Paehler, U
AU  - Schroeck, S
AU  - Hofmann, P
AU  - Koelbl, S
AU  - Drechsler, K
TI  - An Ontology-Augmented Digital Twin for Fiber-Reinforced Polymer Structures at the Example of Wind Turbine Rotor Blades
T2  - ADVANCED ENGINEERING MATERIALS
AB  - A methodology for establishing a structural digital twin is proposed to facilitate the lifetime prediction of fiber-reinforced polymer (FRP) structures, in this case, a wind turbine rotor blade. The digital twin incorporates production peculiarities and imperfections occurring during the manufacturing process of the FRP component. The methodology involves the computation of process-defined effective elastic properties and residual stresses through numerical simulation of the resin cure cycle. The results are then transferred to a structural finite-element model. By applying local wind conditions to this model, a comprehensive state of stress is obtained. This serves as a basis for a practical evaluation of material fatigue within the composite, leading to the prediction of the component's lifetime. The entire workflow is implemented in a Jupyter-based application that uses an ontology with an appertaining knowledge graph to facilitate the transfer of intermediate results between the observation scales and process steps of the digital twin. In line with the principles of open science, the methodology utilizes open-source software.
SN  - 1438-1656
SN  - 1527-2648
DA  - APR
PY  - 2025
VL  - 27
IS  - 8
DO  - 10.1002/adem.202401437
C6  - JAN 2025
AN  - WOS:001396696400001
ER  -

TY  - JOUR
AU  - Lünnemann, P
AU  - Lindow, K
AU  - Gosslau, L
TI  - Implementing digital twins in existing infrastructures
T2  - FORSCHUNG IM INGENIEURWESEN-ENGINEERING RESEARCH
AB  - Digital twins can offer various added values for companies. As part of a three-year research project, we are investigating the methodological approach, for building digital twins in existing infrastructures. In particular, the functional requirements of future users will be addressed, as this is less focused in existing approaches. Within the framework of this publication, we discuss the applied methodology as well as the created models and concepts. Initial insights were gained in the simultaneous development of digital twins in parallel projects with use cases for electric motors, production process monitoring and maintenance of gas turbine components.In detail, it becomes clear that software development methods (e.g. use cases, user stories, scenario development) are a good way to describe the expected added value functions. It is essential to involve the future users in the development as early as possible. Transferring the necessary functions identified in this way into a functional architecture shows that this architecture is mostly independent of the use case. Likewise, the IT systems used here hardly vary at all. Overall, it shows that a methodical approach can be followed in the development and the implementation can have a high degree of similarity, even in very different use cases, while the exact design, depending on these use cases, is very diverse.
SN  - 0015-7899
SN  - 1434-0860
DA  - MAR
PY  - 2023
VL  - 87
IS  - 1
SP  - 421
EP  - 429
DO  - 10.1007/s10010-023-00639-w
C6  - MAR 2023
AN  - WOS:000956256500017
ER  -

TY  - JOUR
AU  - Magdy, R
AU  - Hamdy, KA
AU  - Essawy, YAS
TI  - Real-Time Progress Monitoring of Bricklaying
T2  - BUILDINGS
AB  - The construction industry is one of the largest contributors to the world economy. However, the level of automation and digitalization in the construction industry is still at its infancy in comparison with other industries due to the complex nature and the large size of construction projects. Meanwhile, construction projects are prone to cost overruns and schedule delays due to the adoption of traditional progress monitoring techniques to retrieve progress on-site, having indoor activities participating with an accountable ratio of these works. Improvements in deep learning and Computer Vision (CV) algorithms provide promising results in detecting objects in real time. Also, researchers have investigated the probability of using CV as a tool to create a Digital Twin (DT) for construction sites. This paper proposes a model utilizing the state-of-the-art YOLOv8 algorithm to monitor the progress of bricklaying activities, automatically extracting and analyzing real-time data from construction sites. The detected data is then integrated into a 3D Building Information Model (BIM), which serves as a DT, allowing project managers to visualize, track, and compare the actual progress of bricklaying with the planned schedule. By incorporating this technology, the model aims to enhance accuracy in progress monitoring, reduce human error, and enable real-time updates to project timelines, contributing to more efficient project management and timely completion.
SN  - 2075-5309
DA  - JUL 13
PY  - 2025
VL  - 15
IS  - 14
C7  - 2456
DO  - 10.3390/buildings15142456
AN  - WOS:001535574800001
ER  -

TY  - CPAPER
AU  - Nguyen, MT
AU  - Lam, AN
AU  - Nguyen, P
AU  - Truong, HL
ED  - Shahriar, H
ED  - Ohsaki, H
ED  - Sharmin, M
ED  - Towey, D
ED  - Majumder, AKMJA
ED  - Hori, Y
ED  - Yang, JJ
ED  - Takemoto, M
ED  - Sakib, N
ED  - Banno, R
ED  - Ahamed, SI
TI  - Security Orchestration with Explainability for Digital Twins-based Smart Systems
T2  - 2024 IEEE 48TH ANNUAL COMPUTERS, SOFTWARE, AND APPLICATIONS CONFERENCE, COMPSAC 2024
CP  - 48th Annual IEEE International Computers, Software, and Applications Conference (COMPSAC) - Digital Development for a Better Future
AB  - The Digital Twin (DT) paradigm has been largely adopted for many smart systems in various domains. Due to the heterogeneous and distributed nature of the physical twins, these systems increasingly incorporate disparate security tools, especially those based on service-based AI/ML capabilities. That presents numerous challenges in achieving a comprehensive understanding of security analytics and explainability in security operations carried out by ML-based security services, which require continuous monitoring and optimization to remain effective. This paper aims to support security service integration and automated analyses with enhanced explainability in DTs. We introduce a novel framework that unifies runtime contexts to facilitate security services unification and operation interpretation in security orchestration. We define a workflow and provide necessary services for generating security reports across physical and logical layers. Leveraging a centralized knowledge service, we let security analysts incorporate domain knowledge in automating incident reasoning and security enforcement at the logical layer. We demonstrate our explainability framework on a DT of an Industry 4.0 toy factory with two ML-based security services detecting network anomalies. Our experiments show a significant reduction in manual effort for orchestrating security incident analysis and mitigation.
SN  - 2836-3787
SN  - 979-8-3503-7697-5
SN  - 979-8-3503-7696-8
PY  - 2024
SP  - 1194
EP  - 1203
DO  - 10.1109/COMPSAC61105.2024.00159
AN  - WOS:001308581200150
ER  -

TY  - JOUR
AU  - Zhou, L
TI  - Trustworthy digital twinning data platform for power infrastructure construction projects using blockchain and semantic web
T2  - FRONTIERS IN BUILT ENVIRONMENT
AB  - Power infrastructure projects are characterized by complex supply chain structures and numerous stakeholders, presenting significant challenges in maintaining data integrity and ensuring seamless integration of project information. Previous Digital Twins (DTs) and Building Information Modeling (BIM) collaboration methods lack robust mechanisms for data traceability and immutable storage, leading to potential risks such as data loss or tampering. Furthermore, existing project information exchange and data management methods do not adequately integrate diverse data types, such as project documentation, onsite environment monitoring IoT sensor readings and CAD/BIM-based design information. This research introduces a novel DT data platform prototype, utilizing Blockchain and Semantic Web technologies, to establish a trustworthy DT data environment for power infrastructure projects. This system collects heterogeneous data, including manual inputs and IoT-generated data, and processes them into RDF format on dedicated devices. The integrated data is then stored on a Permissioned Blockchain, ensuring traceability and immutability. The framework incorporates Distributed File Systems to enhance storage efficiency and features a semantic gateway that transforms heterogeneous data into RDF graphs, fostering interoperability and the potential for automated data linkage. The efficacy of this prototype was demonstrated through a case study, testing data consistency and showcasing prototype queries enhanced by Semantic Web, thus substantiating the platform's capacity to support multidisciplinary project management.
SN  - 2297-3362
DA  - NOV 14
PY  - 2024
VL  - 10
C7  - 1440513
DO  - 10.3389/fbuil.2024.1440513
AN  - WOS:001364837800001
ER  -

TY  - JOUR
AU  - Blume, C
AU  - Blume, S
AU  - Thiede, S
AU  - Herrmann, C
TI  - Data-Driven Digital Twins for Technical Building Services Operation in Factories: A Cooling Tower Case Study
T2  - JOURNAL OF MANUFACTURING AND MATERIALS PROCESSING
AB  - Cyber-physical production systems (CPPS) and digital twins (DT) with a data-driven core enable retrospective analyses of acquired data to achieve a pervasive system understanding and can further support prospective operational management in production systems. Cost pressure and environmental compliances sensitize facility operators for energy and resource efficiency within the whole life cycle while achieving reliability requirements. In manufacturing systems, technical building services (TBS) such as cooling towers (CT) are drivers of resource demands while they fulfil a vital mission to keep the production running. Data-driven approaches, such as data mining (DM), help to support operators in their daily business. Within this paper the development of a data-driven DT for TBS operation is presented and applied on an industrial CT case study located in Germany. It aims to improve system understanding and performance prediction as essentials for a successful operational management. The approach comprises seven consecutive steps in a broadly applicable workflow based on the CRISP-DM paradigm. Step by step, the workflow is explained including a tailored data pre-processing, transformation and aggregation as well as feature selection procedure. The graphical presentation of interim results in portfolio diagrams, heat maps and Sankey diagrams amongst others to enhance the intuitive understanding of the procedure. The comparative evaluation of selected DM algorithms confirms a high prediction accuracy for cooling capacity (R-2 = 0.96) by using polynomial regression and electric power demand (R-2 = 0.99) by linear regression. The results are evaluated graphically and the transfer into industrial practice is discussed conclusively.
SN  - 2504-4494
DA  - DEC
PY  - 2020
VL  - 4
IS  - 4
C7  - 97
DO  - 10.3390/jmmp4040097
AN  - WOS:000602946600001
ER  -

TY  - JOUR
AU  - Marra, A
AU  - Gerbino, S
AU  - Greco, A
AU  - Fabbrocino, G
TI  - Combining Integrated Informative System and Historical Digital Twin for Maintenance and Preservation of Artistic Assets
T2  - SENSORS
AB  - The protection of artistic and cultural heritage is a major challenge due to its peculiarities and its exposure to significant natural hazards. Several methodologies exist to assess the condition of artistic heritage and to protect it from exceptional actions. Moreover, novel digital technologies offer many solutions able to deliver a digital replica of artifacts of interest, so that a reduction in the uncertainties in the analysis models can be achieved. A rational approach to the preservation and protection of artistic heritage is based on traditional approaches supported and integrated by novel technologies, so that qualitative and quantitative indicators of the current condition of artistic heritage can be defined and validated in an interdisciplinary framework. The present paper reports the results of an approach to the maintenance and preservation of art objects housed in a museum complex based on a comprehensive digital path towards a Historical Digital Twin (HDT). A workflow aimed at estimating the stress regime and the dynamic properties of two sculptures, based on the detailed three-dimensional model resulting from a laser scanner survey, is illustrated and discussed. The results highlight the great advantages resulting from the integration of traditional and novel procedures in the field of conservation of artistic assets.
SN  - 1424-8220
SN  - 1424-8220
DA  - SEP
PY  - 2021
VL  - 21
IS  - 17
C7  - 5956
DO  - 10.3390/s21175956
AN  - INSPEC:23139157
ER  -

TY  - CPAPER
AU  - Abane, A
AU  - Battou, A
AU  - Amlou, A
AU  - Zhang, T
A1  - IEEE
TI  - A Data Collection Platform for Network Management
T2  - 2023 20TH ACS/IEEE INTERNATIONAL CONFERENCE ON COMPUTER SYSTEMS AND APPLICATIONS, AICCSA
CP  - 20th ACS/IEEE International Conference on Computer Systems and Applications (AICCSA)
AB  - Network management relies on extensive monitoring of network state to analyse network behavior, design optimizations, plan upgrades, and conduct troubleshooting. Network monitoring collects various data from network devices through different protocols and interfaces such as NETCONF and Syslog, and from monitoring tools such as Zeek and Osquery. To unify and automate the monitoring workflow across the network, this paper identifies and discusses the data collection requirements for network management, reviews different monitoring approaches, and proposes an efficient data collection platform that addresses the requirements through an extensible and lightweight protocol. The platform design is demonstrated through an adaptive collection of data for network management based on digital twin technology.
SN  - 2161-5322
SN  - 979-8-3503-1943-9
PY  - 2023
DO  - 10.1109/AICCSA59173.2023.10479236
AN  - WOS:001222477900006
ER  -

TY  - JOUR
AU  - Cho, DS
AU  - Cho, JM
AU  - Kim, WT
TI  - A Generative Digital Twin for Continually Enhancing the Intended Functional Safety of Cyber–Physical Systems
T2  - IEEE TRANSACTIONS ON RELIABILITY
AB  - Cyber-physical systems (CPSs) used in safety-critical applications must safely operate even in unpredictable situations. The functional safety of their physical components, which have relatively low complexity, has been significantly assured through the safety standards. However, it is still difficult to ensure the intended functional safety (IFS) of autonomous control components driven by artificial intelligence models, as only a limited number of scenarios can be realized by engineers. This issue may cause the fatal injuries and the material damages under the unconsidered scenarios. To address this issue, we propose a generative digital twin (gDT) that continually enhances the IFS of the CPSs using a deep reinforcement learning algorithm and a retraining scheme. The workflow of the gDT involves 1) generating unknown hazardous scenarios that engineers cannot consider during the development phase and 2) feeding the generated scenarios back to the autonomous control components. Experimental results show that the proposed digital twin can discover the unknown hazardous scenarios against prototypes of the CPSs and can enhance the infraction avoiding rate to 90% against the discovered ones while maintaining the ability to operate in the existing known safe ones.
SN  - 0018-9529
SN  - 1558-1721
DA  - JUN
PY  - 2025
VL  - 74
IS  - 2
SP  - 2561
EP  - 2575
DO  - 10.1109/TR.2024.3434606
C6  - OCT 2024
AN  - WOS:001336006400001
ER  -

TY  - CPAPER
AU  - Gitahi, J
AU  - Kolbe, TH
ED  - Kolbe, TH
ED  - Donaubauer, A
ED  - Beil, C
TI  - Requirements for Web-Based 4D Visualisation of Integrated 3D City Models and Sensor Data in Urban Digital Twins
T2  - RECENT ADVANCES IN 3D GEOINFORMATION SCIENCE, 3D GEOINFO 2023
CP  - 18th International 3D GeoInfo Conference
AB  - Urban Digital Twins (UDTs) have emerged as essential tools for managing city operations, forming the basis of smart city solutions. They offer a digital representation of the physical urban environment, which supports various city applications such as monitoring mobility, air quality, and modelling simulations. To accurately represent the physical world, UDTs need to be updated continuously to reflect the changes in the urban environment on time. The Internet of Things (IoT) enables real-time data collection to capture these changes. Combined with 3D city models, IoT allows the interactive visualisation of patterns and trends in UDTs. In this study, we conduct investigations on the requirements for the web visualisation of semantic 3D city models enriched with time-dependent properties from IoT and simulation data. We explore the 3D models and IoT data integration requirements, 4D web visualisation design considerations, and the technical implementation requirements for rendering dynamic properties for UDTs applications. The paper also presents a workflow and a web viewer prototype for the 4D visualisation of integrated 3D models and dynamic data.
SN  - 1863-2246
SN  - 1863-2351
SN  - 978-3-031-43701-4
SN  - 978-3-031-43699-4
SN  - 978-3-031-43698-7
PY  - 2024
SP  - 707
EP  - 725
DO  - 10.1007/978-3-031-43699-4_43
AN  - WOS:001264764900043
ER  -

TY  - JOUR
AU  - Steindl, G
AU  - Kastner, W
TI  - Semantic Microservice Framework for Digital Twins
T2  - APPLIED SCIENCES-BASEL
AB  - Digital Twins (DT) in industrial cyber-physical systems are the key enabling technology for Industry 4.0. Services are an essential part of almost every DT concept, but their interaction is usually implementation-specific since no common guidelines are available. This work identifies some fundamental requirements for a DT service framework based on applications identified in corresponding literature. Based on these requirements, a service framework architecture is proposed. The architecture utilizes Semantic Web technology and a workflow engine for service orchestration to support the fulfilment of the identified requirements. As a case study for sensor data evaluation of an industrial process, a proof-of-concept implementation is presented, showing the feasibility and suitability of the proposed DT service framework architecture.
SN  - 2076-3417
DA  - JUN
PY  - 2021
VL  - 11
IS  - 12
C7  - 5633
DO  - 10.3390/app11125633
AN  - WOS:000665992600001
ER  -

TY  - JOUR
AU  - Gillette, K
AU  - Gsell, MAF
AU  - Prassl, AJ
AU  - Karabelas, E
AU  - Reiter, U
AU  - Reiter, G
AU  - Grandits, T
AU  - Payer, C
AU  - Stern, D
AU  - Urschler, M
AU  - Bayer, JD
AU  - Augustin, CM
AU  - Neic, A
AU  - Pock, T
AU  - Vigmond, EJ
AU  - Plank, G
TI  - A Framework for the generation of digital twins of cardiac electrophysiology from clinical 12-leads ECGs
T2  - MEDICAL IMAGE ANALYSIS
AB  - Cardiac digital twins (Cardiac Digital Twin (CDT)s) of human electrophysiology (Electrophysiology (EP)) are digital replicas of patient hearts derived from clinical data that match like-for-like all available clinical observations. Due to their inherent predictive potential, CDTs show high promise as a complementary modality aiding in clinical decision making and also in the cost-effective, saf e and ethical testing of novel EP device therapies. However, current workflows for both the anatomical and functional twinning phases within CDT generation, referring to the inference of model anatomy and parameters from clinical data, are not sufficiently efficient, robust and accurate for advanced clinical and industrial applications.
   Our study addresses three primary limitations impeding the routine generation of high-fidelity CDTs by introducing; a comprehensive parameter vector encapsulating all factors relating to the ventricular EP; an abstract reference frame within the model allowing the unattended manipulation of model parameter fields; a novel fast-forward electrocardiogram (Electrocardiogram (ECG)) model for efficient and biophysically-detailed simulation required for parameter inference. A novel workflow for the generation of CDTs is then introduced as an initial proof of concept.
   Anatomical twinning was performed within a reasonable time compatible with clinical workflows ( < 4h) for 12 subjects from clinically-attained magnetic resonance images. After assessment of the underlying fast forward ECG model against a gold standard bidomain ECG model, functional twinning of optimal parameters according to a clinically-attained 12 lead ECG was then performed using a forward Saltelli sampling approach for a single subject. The achieved results in terms of efficiency and fidelity demonstrate that our workflow is well-suited and viable for generating biophysically-detailed CDTs at scale.
   (c) 2021 The Author(s). Published by Elsevier B.V.
   This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ )
SN  - 1361-8415
SN  - 1361-8423
DA  - JUL
PY  - 2021
VL  - 71
C7  - 102080
DO  - 10.1016/j.media.2021.102080
C6  - MAY 2021
AN  - CCC:000663614500005
ER  -

TY  - CPAPER
AU  - Nasti, A
AU  - Voutchkov, I
AU  - Keane, A
A1  - ASME
TI  - GENERATIVE DEEP LEARNING ON IMAGES OF THERMO-MECHANICAL SIMULATION RESULTS
T2  - PROCEEDINGS OF ASME TURBO EXPO 2024: TURBOMACHINERY TECHNICAL CONFERENCE AND EXPOSITION, GT2024, VOL 10A
CP  - 69th ASME Turbomachinery Technical Conference and Exposition (ASME Turbo Expo) (GT)
AB  - This paper presents a novel methodology that combines engineering simulation and machine learning for the thermo-mechanical design of a secondary air system double-sided seal in an aero engine turbine subassembly. Secondary air system seals are crucial in aero engine design as they have a direct impact on specific fuel consumption.
   The study uses an automated analysis workflow to generate a large dataset of images embedding key design and performance attributes for the seals, such as running clearances at key operating conditions. These images are used to train a conditional Generative Adversarial Network (cGAN), which can then be used for design exploration or optimisation. The paper introduces a unique approach to encoding and decoding these images, enabling automatic quality monitoring of the generated images and training processes, as well as extraction of targeted results.
   The predictability of the Deep Learning models is assessed, demonstrating how this methodology can generate designs in targeted categories and can support decision making both in the preliminary design phase, to enable classification of 'good' and 'bad' designs, and in the detailed design phase, to support optimisation and robust design. Beyond design, these methods can also be used to support the implementation of the Digital Twin. Keywords: Deep Learning, cGAN, seal design, thermo-mechanical simulation, Generative AI, secondary air system, multi-disciplinary design optimisation, design space explorations, digital twin
SN  - 978-0-7918-8802-5
PY  - 2024
AN  - WOS:001303798800060
ER  -

TY  - JOUR
AU  - Jahangir, MF
AU  - Schultz, CPL
AU  - Kamari, A
TI  - A REVIEW OF DRIVERS AND BARRIERS OF DIGITAL TWIN ADOPTION IN BUILDING PROJECT DEVELOPMENT PROCESSES
T2  - JOURNAL OF INFORMATION TECHNOLOGY IN CONSTRUCTION
AB  - Over the past few years, the AECO Industry has undergone a shift toward digital transformation, with a growing trend towards adopting innovative technologies such as Digital Twin (DT). DT offers a wide range of applications throughout the building development process. However, some specific factors impede its widespread adoption in the building industry. This study aims to systematically review the available literature on the building project development process from the perspective of DT, with a particular focus on predictive simulations, i.e., co-sims. The review provides a comprehensive overview of drivers and barriers to DT adoption through an analysis of 147 studies between 2013 and 2023. The research identifies seven external and 41 internal drivers, including efficient project management and monitoring, predictive maintenance, and the collection and visualization of real -time data, all of which contribute to improved decision-making processes and reduced operational expenses. Further, the study identifies nine external and 31 internal barriers that impede the adoption of DT in the building development process. These barriers encompass challenges such as a high initial investment cost, a scarcity of a skilled workforce, difficulties in data interoperability, and resistance to change within the organization. A key outcome of the literature review is having identified the opportunity to exploit technologies developed in the automotive sector that enable a seamless integration of specialized simulator models in building development processes, resulting in collaborative simulations. Thus, we propose the concept of a Building Simulation Identity Card (BSIC) to be pursued in future research that would enable stakeholders to address the challenges of collaboration, cooperation, coordination, and communication by creating a common vocabulary to effectively facilitate the adoption of DT in the building's development process.
SN  - 1874-4753
PY  - 2024
VL  - 29
SP  - 141
EP  - 178
DO  - 10.36680/j.itcon.2024.008
AN  - WOS:001192657800001
ER  -

TY  - CPAPER
AU  - Jonek, M
AU  - Niermann, D
AU  - Petzoldt, C
AU  - Manns, M
AU  - Freitag, M
ED  - Mpofu, K
ED  - Sacks, N
ED  - Damm, O
TI  - Automated Analysis of Assembly Processes in Human-Robot Collaboration: Research Approaches and Challenges
T2  - 56TH CIRP CONFERENCE ON MANUFACTURING SYSTEMS, CIRP CMS 2023
CP  - 56th Conference on Manufacturing Systems-CMS
AB  - Due to the increasing trends of individualization and variant diversity, cobotic production systems are becoming more important. However, cobots are hardly used in small and medium-sized enterprises (SME) because of a lack in expertise, experience or resources. In order to be able to assess whether the use of a cobot is reasonable, there are already many methods in research with different focuses such as ergonomics, productivity or economic efficiency. However, many methods are not targeted for use by SME, thus still require the involvement of experts, and often only consider economic efficiency without consideration of human-centered aspects. Furthermore, these are mostly not integrated into commonly used process planning tools.
   This work provides an overview of methods for automated analysis of assembly processes in HRC. We present a method for HRC process analysis with an individual weighting of fatigue, time, costs and safety that can be set depending on an application-specific focus defined by the user. It allows non-experts to perform a process analysis based on the individual weighting and a digital twin of the workstation. An evaluation method developed for this purpose calculates the benefit of a cobot in the analyzed process depending on the preset focus. To keep the effort low, the method includes a workflow to easily create a digital twin of the workplace with sufficient accuracy.
   In addition to the pre-estimation of the human-cobot process, the method allows an easy planning of collaborative production processes, which is why we suggest a conceptual integration into a process planning and control framework. The proposed approach is demonstrated in an use case by analyzing the potential for introduction of collaborative robots for the production of currently manually assembled solar inverters. (c) 2023 The Authors. Published by Elsevier B.V.
SN  - 2212-8271
PY  - 2023
VL  - 120
SP  - 1203
EP  - 1208
DO  - 10.1016/j.procir.2023.09.149
DO  - 10.1016/j.procir.2023.09.149
AN  - WOS:001483980700202
ER  -

TY  - JOUR
AU  - Schmidt, A
AU  - Helgers, H
AU  - Lohmann, LJ
AU  - Vetter, F
AU  - Juckers, A
AU  - Mouellef, M
AU  - Zobel-Roos, S
AU  - Strube, J
TI  - Process analytical technology as key-enabler for digital twins in continuous biomanufacturing
T2  - JOURNAL OF CHEMICAL TECHNOLOGY AND BIOTECHNOLOGY
AB  - Over the last few years rapid progress has been made in adopting well-known process modeling techniques from chemicals to biologics manufacturing. The main challenge has been analytical methods as engineers need quantitative data for their workflow. Industrialization 4.0, Internet of Things, artificial intelligence and machine learning activities up to big data analysis have taken their share in solving fundamental problems like component- or at least group-specific evaluation of spectroscopic data. Besides, concerning inline analytics methods included in process analytical technology concepts the key technology has been the generation of decisive validated digital twins based on process models. This review aims to summarize the methodology to achieve a holistic understanding of process models, control and optimization by means of digital twins using the example of recent work published in this field. (c) 2021 The Authors. Journal of Chemical Technology and Biotechnology published by John Wiley & Sons Ltd on behalf of Society of Chemical Industry (SCI).
SN  - 0268-2575
SN  - 1097-4660
DA  - SEP
PY  - 2022
VL  - 97
IS  - 9
SP  - 2336
EP  - 2346
DO  - 10.1002/jctb.7008
C6  - FEB 2022
AN  - WOS:000751503800001
ER  -

TY  - JOUR
AU  - De Rubeis, T
AU  - Ciccozzi, A
AU  - Ragnoli, M
AU  - Stornelli, V
AU  - Brusaporci, S
AU  - Tata, A
AU  - Ambrosini, D
TI  - A Workflow for a Building Information Modeling-Based Thermo-Hygrometric Digital Twin: An Experimentation in an Existing Building
T2  - SUSTAINABILITY
AB  - Building Information Modeling (BIM)-based digital twin (DT) could play a fundamental role in overcoming the limitations of traditional monitoring methods by driving the digitalization of the construction sector. While existing studies on the topic have provided valuable insights, significant knowledge gaps remain, which continue to hinder the large-scale adoption of this approach. Moreover, to date, there is no standardized procedure available, able to guide the step-by-step creation of a DT. Another significant challenge concerns the choice of technologies able to integrate perfectly with each other throughout the process. This paper outlines a comprehensive workflow for creating a digital twin (DT) of an existing building and proposes various solutions to improve the integration of different technologies involved. These enhancements aim to address the limitations of current monitoring methods and leverage the advantages of BIM and DT for accessing and managing monitoring data, ultimately facilitating the implementation of energy-efficient interventions. This work examines the concept of "Living Lab" in an office building also used as an academic laboratory. The created DT allowed for real-time remote monitoring of four rooms, each with a different functional and occupational characteristic, useful also for future predictive analyses.
SN  - 2071-1050
SN  - 2071-1050
DA  - DEC
PY  - 2024
VL  - 16
IS  - 23
C7  - 10281
DO  - 10.3390/su162310281
AN  - CCC:001377768500001
ER  -

TY  - JOUR
AU  - Li, YJ
AU  - Liu, W
AU  - Zhang, Y
AU  - Zhang, WL
AU  - Gao, CY
AU  - Chen, QH
AU  - Ji, YQ
TI  - Interactive Real-Time Monitoring and Information Traceability for Complex Aircraft Assembly Field Based on Digital Twin
T2  - IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS
AB  - High-quality aircraft assembly is critical in aircraft manufacturing. To meet increasing quality demands, the aircraft assembly process needs to be monitored in real time. In recent years, digital twins have attracted increasing interest in different applications. However, due to the heterogeneous data and multiple protocols in the assembly field, the large amount of data needs to satisfy high throughput, low latency, and low storage cost requirements without interrupting the assembly process, which introduces challenges to real-time monitoring systems. In this article, a real-time monitoring system based on the digital twin that provides interactive services for users based on multisource, multidimensional data, and sophisticated analyses is designed. Moreover, we introduce a time series database (TSDB) for dynamic data storage and SQLServer for static data storage. Time series data can be regarded as a standard type of organized data that are convenient for hierarchical representations and constructing multidimensional data cubes for information traceability. The storage cost is significantly reduced because the compression rate reaches 3.8%. Furthermore, the powerful and comprehensive analysis engine of the TSDB ensures that the monitoring system has real-time responses of less than 4 ms. In addition, a flexible and scalable system that collects multidimensional data without affecting the existing workflow is proposed. We perform various experiments to verify the feasibility and effectiveness of assembling large-scale aircraft components. This study provides a solid data foundation for accelerating the pace of intelligent aircraft manufacturing.
SN  - 1551-3203
SN  - 1941-0050
DA  - SEP
PY  - 2023
VL  - 19
IS  - 9
SP  - 9745
EP  - 9756
DO  - 10.1109/TII.2023.3234618
AN  - WOS:001037910900045
ER  -

TY  - JOUR
AU  - Liu, ZS
AU  - Zhang, ZH
AU  - Yuan, C
TI  - An Intelligent Evaluation Method for Service Safety of Cable Net Structures under Multiple Factors
T2  - SUSTAINABILITY
AB  - Various uncertainties often influence the serviceability of cable net structures, which can impact their structural safety performance. The accurate identification of sensitive factors during the structure's service life and the determination of its serviceability state is crucial for achieving intelligent serviceability safety. In this paper, based on the digital twin model, a multi-factor-based assessment method for the serviceability safety of cable net structures was proposed. Firstly, the assessment method for serviceability safety under multi-factor influence was described in detail, outlining the specific workflow. Secondly, key indicators that affect the structural safety state are selected, and their range of variation is determined. Subsequently, a comprehensive dataset of sample data, considering long-term multi-factor influence, was constructed, through combined simulation using MATLAB 2021 and ANSYS 15.0. Finally, a support-vector-regression-based structural safety assessment model was established and validated.
SN  - 2071-1050
DA  - NOV
PY  - 2023
VL  - 15
IS  - 21
C7  - 15633
DO  - 10.3390/su152115633
AN  - WOS:001099631300001
ER  -

TY  - JOUR
AU  - Chalipour, A
AU  - Noorzai, E
AU  - Golabchi, M
AU  - Nourzad, SHH
TI  - Uncovering key success factors for enhanced office-site communication through digital twins
T2  - TQM JOURNAL
AB  - PurposeThis study aims to explore the transformative potential of digital twins (DTs) in the construction industry by addressing challenges and bridging the technology adoption gap. Focusing on construction offices, on-site environments and their interface, the research identifies key success factors for DT implementation. Emphasizing practical applications, the study foresees a substantial impact on project management, highlighting the pivotal role of DTs in reshaping the construction sector.Design/methodology/approachWe reviewed 30 variables within the DT literature, classifying investigations into three main domains: on-site environments, office environments and site-office interface in construction projects. We employed a questionnaire to examine variables, with collected data analyzed using SmartPLS version 4 software. Confirmatory analysis and structural equation modeling rigorously assessed the model's reliability, validity and predictive utility.FindingsThe study identifies 11 critical factors crucial for successfully implementing DTs in construction projects, emphasizing contributions from office-related (ORF), site-related (SRF) and office-site-related factors (OSRF). Notably, impactful contributions from ORF affirm its pivotal role. The research sheds light on specific aspects within construction offices, on-site environments and their interface, providing valuable insights for industry stakeholders, policymakers and researchers.Originality/valueDespite discussions on DT's potential, a research gap exists in addressing essential factors for its effective application in monitoring and optimizing construction projects throughout their lifecycle. This study fills this gap by identifying and categorizing these factors, enhancing the knowledge base for successful DT assimilation and emphasizing the need for practitioners to understand key factors crucial for its integration into construction processes.
SN  - 1754-2731
SN  - 1754-274X
DA  - 2025 MAR 3
PY  - 2025
DO  - 10.1108/TQM-04-2024-0146
C6  - MAR 2025
AN  - WOS:001433390900001
ER  -

TY  - JOUR
AU  - Aubert, K
AU  - Germaneau, A
AU  - Rochette, M
AU  - Ye, WF
AU  - Severyns, M
AU  - Billot, M
AU  - Rigoard, P
AU  - Vendeuvre, T
TI  - Development of Digital Twins to Optimize Trauma Surgery and Postoperative Management. A Case Study Focusing on Tibial Plateau Fracture
T2  - FRONTIERS IN BIOENGINEERING AND BIOTECHNOLOGY
AB  - Background and context: Surgical procedures are evolving toward less invasive and more tailored approaches to consider the specific pathology, morphology, and life habits of a patient. However, these new surgical methods require thorough preoperative planning and an advanced understanding of biomechanical behaviors. In this sense, patient-specific modeling is developing in the form of digital twins to help personalized clinical decision-making.
   Purpose: This study presents a patient-specific finite element model approach, focusing on tibial plateau fractures, to enhance biomechanical knowledge to optimize surgical trauma procedures and improve decision-making in postoperative management.
   Study design: This is a level 5 study.
   Methods: We used a postoperative 3D X-ray image of a patient who suffered from depression and separation of the lateral tibial plateau. The surgeon stabilized the fracture with polymethyl methacrylate cement injection and bi-cortical screw osteosynthesis. A digital twin of the patient's fracture was created by segmentation. From the digital twin, four stabilization methods were modeled including two screw lengths, whether or not, to inject PMMA cement. The four stabilization methods were associated with three bone healing conditions resulting in twelve scenarios. Mechanical strength, stress distribution, interfragmentary strains, and fragment kinematics were assessed by applying the maximum load during gait. Repeated fracture risks were evaluated regarding to the volume of bone with stress above the local yield strength and regarding to the interfragmentary strains.
   Results: Stress distribution analysis highlighted the mechanical contribution of cement injection and the favorable mechanical response of uni-cortical screw compared to bi-cortical screw. Evaluation of repeated fracture risks for this clinical case showed fracture instability for two of the twelve simulated scenarios.
   Conclusion: This study presents a patient-specific finite element modeling workflow to assess the biomechanical behaviors associated with different stabilization methods of tibial plateau fractures. Strength and interfragmentary strains were evaluated to quantify the mechanical effects of surgical procedures. We evaluate repeated fracture risks and provide data for postoperative management.
SN  - 2296-4185
DA  - OCT 7
PY  - 2021
VL  - 9
C7  - 722275
DO  - 10.3389/fbioe.2021.722275
AN  - WOS:000709949800001
ER  -

TY  - JOUR
AU  - Wang, LG
AU  - Ge, RH
AU  - Chen, XZ
TI  - On the determination of particle impact breakage in selection function
T2  - PARTICUOLOGY
AB  - This paper presents a thorough study of particle impact breakage in selection function with a unified breakage criterion. The impact mode and breakage pattern for particulate materials are classified based on a significant review of well-established impact testers. It was found that the lack of a unified breakage criterion to determine the breakage probability disables a direct comparison of particle breakage propensity from different impact loading testers. The literature breakage models to describe the breakage probability are reviewed where the advantage and drawback of these models are scrutinized. The sourced literature breakage models are compared with the zeolite breakage datasets in a unified breakage criterion to evaluate the model performance. A novel computational modelling workflow for a milling process is proposed to provide a guidance in implementing the digital twin in milling process prediction. The breakage probability models, i.e. the selection functions are comprehensively assessed in population balance model to examine the model serviceability. The model simplicity and fidelity in the model assessment are specifically discussed and the value of digital twin in substantially reducing the experimental trials is highlighted. (c) 2021 Chinese Society of Particuology and Institute of Process Engineering, Chinese Academy of Sciences. Published by Elsevier B.V. All rights reserved.
SN  - 1674-2001
SN  - 2210-4291
DA  - JUN
PY  - 2022
VL  - 65
SP  - 117
EP  - 132
DO  - 10.1016/j.partic.2021.08.003
C6  - NOV 2021
AN  - WOS:000720823500002
ER  -

TY  - JOUR
AU  - Théodon, L
AU  - Coufort-Saudejaud, C
AU  - Debayle, J
TI  - Workflow based on GANs and CNNs towards a digital twin for the 3D morphological characterization of latex aggregates
T2  - POWDER TECHNOLOGY
AB  - This paper presents a workflow for estimating the 3D morphological characteristics of latex aggregates from 2D in-situ images using deep learning and stochastic geometry models. The method includes automatic image segmentation using a Convolutional Neural Network (CNN), 3D object generation using a Generative Adversarial Network (GAN), and estimation of 3D characteristics. Validation with synthetic datasets shows effective size, shape, and texture characterization, with the Mean Absolute Percentage Error (MAPE) for morphological characteristics of generated objects being around 5% at most. Application to real in-situ images demonstrates feasibility and consistency with experimental observations, successfully generating a digital twin of the latex aggregate population. The method's flexibility and efficiency make it suitable for real-time industrial applications, offering potential for process monitoring and quality control. Future work will focus on enhancing model performance and adapting to different particle types for broader applicability in various industrial settings.
SN  - 0032-5910
SN  - 1873-328X
DA  - NOV
PY  - 2025
VL  - 465
C7  - 121286
DO  - 10.1016/j.powtec.2025.121286
AN  - WOS:001528134100006
ER  -

TY  - JOUR
AU  - Lin, LY
AU  - Athe, P
AU  - Rouxelin, P
AU  - Avramova, M
AU  - Gupta, A
AU  - Youngblood, R
AU  - Lane, J
AU  - Dinh, N
TI  - Development and assessment of a nearly autonomous management and control system for advanced reactors
T2  - ANNALS OF NUCLEAR ENERGY
AB  - This paper develops a Nearly Autonomous Management and Control (NAMAC) system for advanced reactors. The development process of NAMAC is characterized by a three layer-layer architecture: knowledge base, the Digital Twin (DT) developmental layer, and the NAMAC operational layer. The DT is described as a knowledge acquisition system from the knowledge base for intended uses in the NAMAC system. A set of DTs with different functions is developed with acceptable performance and assembled according to the NAMAC operational workflow to furnish recommendations to operators. To demonstrate the capability of the NAMAC system, a case study is designed, where a baseline NAMAC is implemented for operating a simulator of the Experimental Breeder Reactor II during a single loss of flow accident. When NAMAC is operated in the training domain, it can provide reasonable recommendations that prevent the peak fuel centerline temperature from exceeding a safety criterion. (C) 2020 Elsevier Ltd. All rights reserved.
SN  - 0306-4549
DA  - JAN
PY  - 2021
VL  - 150
C7  - 107861
DO  - 10.1016/j.anucene.2020.107861
AN  - WOS:000590396000013
ER  -

TY  - JOUR
AU  - Balcewicz, M
AU  - Siegert, M
AU  - Gurris, M
AU  - Ruf, M
AU  - Krach, D
AU  - Steeb, H
AU  - Saenger, EH
TI  - Digital Rock Physics: A Geological Driven Workflow for the Segmentation of Anisotropic Ruhr Sandstone
T2  - FRONTIERS IN EARTH SCIENCE
AB  - Over the last 3 decades, Digital Rock Physics (DRP) has become a complementary part of the characterization of reservoir rocks due to the non-destructive testing character of this technique. The use of high-resolution X-ray Computed Tomography (XRCT) has become widely accepted to create a digital twin of the material under investigation. Compared to other imaging techniques, XRCT technology allows a location-dependent resolution of the individual material particles in volume. However, there are still challenges in assigning physical properties to a particular voxel within the digital twin, due to standard histogram analysis or sub-resolution features in the rock. For this reason, high-resolution image-based data from XRCT, transmitted-light microscope, Scanning Electron Microscope (SEM) as well as geological input properties like geological diagenesis, mineralogical composition, sample's microfabrics, and estimated sample's porosity are combined to obtain an optimal spatial segmented image of the studied Ruhr sandstone. Based on a homogeneity test, which corresponds to the evaluation of the gray-scale image histogram, the preferred scan sample sizes in terms of permeability, thermal, and effective elastic rock properties are determined. In addition, these numerically derived property predictions are compared with laboratory measurements to obtain possible upper limits for sample size, segmentation accuracy, and a geometrically calibrated digital twin of the Ruhr sandstone. The comparison corresponding gray-scale image histograms as a function of sample sizes with the corresponding advanced numerical simulations provides a unique workflow for reservoir characterization of the Ruhr sandstone.
SN  - 2296-6463
DA  - JUN 29
PY  - 2021
VL  - 9
C7  - 673753
DO  - 10.3389/feart.2021.673753
AN  - WOS:000672426300001
ER  -

TY  - JOUR
AU  - Gunckel, PV
AU  - Lobos, G
AU  - Rodríguez, FK
AU  - Bustos, RM
AU  - Godoy, D
TI  - Methodology proposal for the development of failure prediction models applied to conveyor belts of mining material using machine learning
T2  - RELIABILITY ENGINEERING & SYSTEM SAFETY
AB  - The widespread adoption of areas such as Machine Learning, the establishment of Industry 4.0, and the various techniques and information available to companies today foster the need to incorporate advanced control and monitoring tools, such as predictive failure systems, into asset management. While there are various documented cases of trained ML models yielding good results, there is still a lack of clarity on how to address all the stages that an analysis like this requires in a general manner, considering that it must be valid across different areas and different data characteristics. This article presents and describes a workflow that encompasses this methodological proposal for the development of failure forecasting systems, which was then applied to the case of a mining conveyor belt in Chile. The study and its application case result in a successful integration between data from a Distributed Control System (DCS), a Digital Twin, and an operational logbook, as well as precision and recall values exceeding 0.83 in the best cases of the various trained algorithms with data transformed into new variables and the application of principal component analysis (PCA). This is done both for failure prediction in general and for fault type-oriented forecasting Based on this, the paper presents a transferable methodological proposal that is adaptable to various data sources without relying on specific assets or physical process information. Its main strength lies in reducing dependence on maintenance data for anomaly detection. However, this approach lacks validation and raises clarity issues, diverging from the Functional and Informational Requirements outlined by other authors. Despite these challenges, the model shows acceptable results, and the potential to integrate operational data allows for further development. Future iterations may focus on improving calculation times and addressing the challenge of identifying the origins or causes of predicted events.
SN  - 0951-8320
SN  - 1879-0836
DA  - APR
PY  - 2025
VL  - 256
C7  - 110709
DO  - 10.1016/j.ress.2024.110709
C6  - DEC 2024
AN  - WOS:001381388100001
ER  -

TY  - JOUR
AU  - Kliestik, T
AU  - Dragomir, R
AU  - Baluta, AV
AU  - Grecu, I
AU  - Durana, P
AU  - Karabolevski, OL
AU  - Kral, P
AU  - Balica, R
AU  - Suler, P
AU  - Busu, OV
AU  - Bugaj, M
AU  - Voinea, DV
AU  - Vrbka, J
AU  - Cocosatu, M
AU  - Pera, A
AU  - Gajdosikova, D
AU  - Grupac, M
TI  - Enterprise generative artificial intelligence technologies, Internet of Things and blockchain-based fintech management, and digital twin industrial metaverse in the cognitive algorithmic economy
T2  - OECONOMIA COPERNICANA
AB  - Research background: Enterprise generative AI system-based worker behavior tracking and monitoring, socially responsible organizational practices, employee performance management satisfaction, and human resource management procedures, relationships, and outcomes develop on hiring and objective performance assessment algorithms in terms of human resource management activities, functions, processes, practices, policies, and productivity. Deep reinforcement and machine learning techniques, operational and analytical generative AI and cloud capabilities, and real-time anomalous behavior recognition systems further fintech development for credit and lending services, payment analytics processes, and risk assessment, monitoring, and mitigation. Generative AI tools can bolster predictive analytics by collaborative and interconnected sensor and machine data for tailored, seamless, and finetuned product, operational process, and organizational workflow development, efficiency, and innovation, driving agile transformative changes in digital twin industrial metaverse. Purpose of the article: We show that enterprise generative AI-driven schedule prediction tools, job search and algorithmic hiring systems, and synthetic training data can improve team selection, job performance and firing decisions, hiring decision processes, and workforce productivity in terms of prediction and decision-making by use of algorithmic management, system performance, and production process tracking tools. Blockchain-based fintech operations can shape cloud-based financial and digital banking services, quote-to-cash process automation, cash-settled crypto futures, digital loan decisioning, asset tokenization simulated transactions, transaction switching and routing operations, tailored peer-to-peer lending, and proactive credit line management. Collaborative unstructured enterprise data processing, infrastructure, and governance can develop on AI decision and behavior automation technology, retrieval augmented generation and development management systems, and real-time data descriptive and predictive analytics, driving productivity surges and competitive advantage in digital twin industrial metaverse. Methods: Reference and review management tools, together with evidence synthesis screening software, harnessed were Abstrackr, AMSTAR, ASReview Lab, CASP, Catchii, Citation- chaser, DistillerSR, JBI SUMARI, Litstream, PICO Portal, and Rayyan. Findings & value added: The current state of the art is improved for theory on organizational issues and for policy making as deep learning-based generative AI tools and workplace monitoring systems can augment performance and productivity, gauge employee effectiveness, build resilient, satisfied, and engaged workforce, assess human capital, skill, and career development, drive employee and productivity expectations in relation to flexibility and stability, and shape turnover, retention, and loyalty. Cloud and account servicing technologies can be deployed in generative AI fintechs for embedded cryptocurrency trading, transaction moni toring and processing, digital asset transfers, payment screening, corporate and retail banking operations, and fraud prevention.
   Generative AI technologies can reshape jobs and reimagine meaningful work, involving creativity and innovation and adaptable and resilient sustained performance, providing valuable constructive feedback, optimizing workplace flexibility and psychological safety, and measuring and supporting autonomy and flexibility-based efficien- cy, performance, and productivity, while configuring demanding, engaging, and rewarding experiences by cloud and edge computing devices in digital twin industrial metaverse.
SN  - 2083-1277
SN  - 2353-1827
DA  - DEC
PY  - 2024
VL  - 15
IS  - 4
SP  - 1183
EP  - 1221
DO  - 10.24136/oc.3109
AN  - WOS:001399801800002
ER  -

TY  - JOUR
AU  - Döbrich, O
AU  - Brauner, C
TI  - Machine vision system for digital twin modeling of composite structures
T2  - FRONTIERS IN MATERIALS
AB  - Although the structural design of composite structures has already been carried out on a virtual level, composite mechanical properties remain sensitive to fiber orientation and therefore to the quality and reliability of the production process. Considering both manual single-unit manufacturing and advanced mass-unit fabrication, requirements on the production quality may differ, but certainty on the achieved result is crucial. A digital twin model, deterministically derived from produced parts, can be transferred into a virtual simulation environment to check for potential deviations of fiber alignment, resulting from variations in source material or composite production. Transferring that deterministic information into a virtual simulation environment allows for an estimation of the part's structural potential despite any possible deviations by carrying out numerical simulation predictions on that model. This step of quality assessment can help reduce scrap parts by relying on simulation data that may demonstrate the feasibility of parts despite the containment of deviations with an otherwise uncertain impact. Therefore, further steps toward digitalization of the composite production process chain, especially on the characterization of the production quality, are aspired. In this contribution, a vision system based on a Microsoft Azure Kinect RGB-D camera is introduced which is used to digitalize the composite preform configuration from machine vision data by evaluating the achieved local fiber orientation as result of the complex preform draping process by digital image processing. A digital workflow is introduced that enables to feed the captured real-world data back into a digital environment where numerical simulations with the "as-built" fiber orientation can be carried out. The obtained results are used for assessing production quality and composite performance in the presence of possible deviations. The system, which consists of a camera array of consumer grade, can acquire real-world data and then transfer the data into a virtual environment.
SN  - 2296-8016
DA  - MAR 30
PY  - 2023
VL  - 10
C7  - 1154655
DO  - 10.3389/fmats.2023.1154655
AN  - WOS:000970453700001
ER  -

TY  - JOUR
AU  - Omer, M
AU  - Margetts, L
AU  - Mosleh, MH
AU  - Hewitt, S
AU  - Parwaiz, M
TI  - Use of gaming technology to bring bridge inspection to the office
T2  - STRUCTURE AND INFRASTRUCTURE ENGINEERING
AB  - This paper proposes a novel method for bridge inspection that essentially digitises bridges using Light Detection and Ranging (LIDAR) so that they can be later inspected in a virtual reality (VR) environment. The work uses conventional terrestrial LIDAR together with affordable VR hardware and freely available software development kits originally intended for authoring computer games. The resulting VR app is evaluated for a case study involving a typical masonry bridge, comparing the proposed technique with traditional inspection methods. The new approach promises to be highly effective in terms of interpretation of results, accessibility to critical areas and safety of inspectors. The work represents an important step towards the creation of digital twins of important assets in the built environment. Recent bridge collapse incidents have affected local economies, traffic congestion, and in some extreme cases led to a loss of life. The work is timely as law making agencies are paying greater attention to structural rehabilitation. This paper will be of particular interest to bridge engineers, construction professionals and law makers and could lead to future revisions of bridge inspection processes and standards.
SN  - 1573-2479
SN  - 1744-8980
DA  - OCT 3
PY  - 2019
VL  - 15
IS  - 10
SP  - 1292
EP  - 1307
DO  - 10.1080/15732479.2019.1615962
AN  - WOS:000479039400002
ER  -

TY  - JOUR
AU  - Liu, ZY
AU  - Hu, L
AU  - Hu, WF
AU  - Tan, JR
TI  - Petri Nets-Based Modeling Solution for Cyber-Physical Product Control Considering Scheduling, Deployment, and Data-Driven Monitoring
T2  - IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS
AB  - For a complex electromechanical product that is a cyber-physical system (CPS), its dynamic behaviors are embodied in the closed-loop control between the logic process in its cyber component and actual actuators/sensors in its physical component, and thus, a well-defined model of the control is important to create a digital twin that acts as much like the real machine as possible. This article proposes a Petri nets (PNs)-based modeling solution that employs hybrid PNs (HPNs) for physics and system of sequential systems with shared resources ((SR)-R-4) nets for logic in building a hierarchical control model. We also present PNs technologies for implementing a smooth transition and bidirectional mapping from the virtual prototype to the real machine. These technologies involve a PNs integration of a reinforcement learning (RL) method for generating a workflow scheduling agent in design, an extension of PNs definitions that is compatible with the microcontroller for easy deployment in manufacturing, and an architecture of PNs execution recording for data-driven monitoring in service. A software kit is provided for the solution that includes an integrated development environment of PNs, tools for quickly building a virtual prototype, and a monitor server for remote data-driven monitoring. This solution is successfully applied in the development of a typical cyber-physical product case, namely, the chemiluminescence immunoassay (CLIA) analyzer.
SN  - 2168-2216
SN  - 2168-2232
DA  - FEB
PY  - 2023
VL  - 53
IS  - 2
SP  - 990
EP  - 1002
DO  - 10.1109/TSMC.2022.3170489
C6  - JUL 2022
AN  - WOS:000833055900001
ER  -

TY  - CPAPER
AU  - Tsinarakis, GJ
AU  - Spanoudakis, PS
AU  - Arabatzis, G
AU  - Tsourveloudis, NSC
AU  - Doitsidis, L
A1  - IEEE
TI  - Implementation of a Petri-net based Digital Twin for the development procedure of an Electric Vehicle
T2  - 2020 28TH MEDITERRANEAN CONFERENCE ON CONTROL AND AUTOMATION (MED)
CP  - 28th Mediterranean Conference on Control and Automation (MED)
AB  - In the current work the development procedure (design, manufacture and assembly) of an electric vehicle is considered.. Uncertainties make difficult to follow initial time plan, so monitoring of the development procedure is necessary. To handle delays a method using Petri nets to model the tasks of the development procedure and their dependencies is introduced. The model is not used offline as a passive element but is connected and interacts with the physical system (development procedure). Based on the information exchange between physical and digital system, alternative ways to overcome delays are studied and the optimal solution is calculated, tested and applied. Results are provided according to different scenarios, in order to show the efficiency and applicability of the proposed method.
SN  - 2325-369X
SN  - 978-1-7281-5742-9
PY  - 2020
SP  - 862
EP  - 867
DO  - 10.1109/med48518.2020.9182784
AN  - WOS:000612207700140
ER  -

TY  - JOUR
AU  - Sachdeva, R
AU  - Armstrong, AK
AU  - Arnaout, R
AU  - Grosse-Wortmann, L
AU  - Han, BK
AU  - Mertens, L
AU  - Moore, RA
AU  - Olivieri, LJ
AU  - Parthiban, A
AU  - Powell, AJ
TI  - Novel Techniques in Imaging Congenital Heart Disease JACC Scientific Statement
T2  - JOURNAL OF THE AMERICAN COLLEGE OF CARDIOLOGY
AB  - Recent years have witnessed exponential growth in cardiac imaging technologies, allowing better visualization of complex cardiac anatomy and improved assessment of physiology. These advances have become increasingly important as more complex surgical and catheter -based procedures are evolving to address the needs of a growing congenital heart disease population. This state-of-the-art review presents advances in echocardiography, cardiac magnetic resonance, cardiac computed tomography, invasive angiography, 3 -dimensional modeling, and digital twin technology. The paper also highlights the integration of artificial intelligence with imaging technology. While some techniques are in their infancy and need further refinement, others have found their way into clinical workflow at well-resourced centers. Studies to evaluate the clinical value and cost-effectiveness of these techniques are needed. For techniques that enhance the value of care for congenital heart disease patients, resources will need to be allocated for education and training to promote widespread implementation. (J Am Coll Cardiol 2024;83:63-81) (c) 2024 by the American College of Cardiology Foundation.
SN  - 0735-1097
SN  - 1558-3597
DA  - JAN
PY  - 2024
VL  - 83
IS  - 1
SP  - 63
EP  - 81
DO  - 10.1016/j.jacc.2023.10.025
C6  - JAN 2024
AN  - BIOABS:BACD202400201414
ER  -

TY  - JOUR
AU  - Antonino, PO
AU  - Capilla, R
AU  - Kazman, R
AU  - Kuhn, T
AU  - Schnicke, F
AU  - Treichel, T
AU  - Bachorek, A
AU  - Müller-Zhang, Z
AU  - Salamanca, V
TI  - Continuous engineering for Industry 4.0 architectures and systems
T2  - SOFTWARE-PRACTICE & EXPERIENCE
AB  - Traditionally, the quality of a software or system architecture has been evaluated in the early stages of the development process using architecture quality evaluation methods. Emergent approaches like Industry 4.0 require continuous monitoring of both run-time and development-time quality properties, in contrast to traditional systems where quality is evaluated at specific milestones using techniques such as project reviews. Considering the dynamics and minimum down-time imposed by the industrial production domain, it must also be ensured that Industry 4.0 system evaluations are continuously performed with high confidence and with as much automation as possible, using simulations, for instance. In this regard, there is a need to develop new methods for continuously monitoring and evaluating the quality properties of software-based systems for Industry 4.0, which must be supported by automated quality evaluation techniques. In this research we analyze traditional architecture evaluation methods and Industry 4.0 scenarios, and propose an approach based on Digital Twins and simulations to continuously evaluate runtime quality aspects of the architecture and systems of industrial production plants. The evaluation is based on the instantiation of our approach for a concrete demand of an automation plant in the automotive domain.
SN  - 0038-0644
SN  - 1097-024X
DA  - OCT
PY  - 2022
VL  - 52
IS  - 10
SP  - 2241
EP  - 2262
DO  - 10.1002/spe.3124
C6  - JUL 2022
AN  - WOS:000831086400001
ER  -

TY  - CPAPER
AU  - Barhebwa-Mushamuka, F
AU  - Wagner, S
TI  - Multi-Partners Digital Project Twin: A Tool for Project Monitoring
T2  - IFAC PAPERSONLINE
CP  - 10th IFAC Triennial Conference on Manufacturing Modelling, Management and Control (MIM)
AB  - Monitoring a complex project with multiple partners is a challenging but essential task to maintain project performance despite changes. The tools usually used in project management are static tools that do not allow to see all the project dynamics. This paper proposes a digital twin for multi-partner projects whose simulation model mimics the dynamic execution of the project. It allows the project manager to manage the start and end of tasks, deliverables, and milestones and predict the consumption of the project efforts during the project horizon. Copyright (C) 2022 The Authors.
SN  - 2405-8963
DA  - OCT
PY  - 2022
VL  - 55
IS  - 10
SP  - 383
EP  - 388
DO  - 10.1016/j.ifacol.2022.09.423
C6  - OCT 2022
AN  - WOS:000881681700065
ER  -

TY  - JOUR
AU  - Loverdos, D
AU  - Sarhosis, V
TI  - Geometrical digital twins of masonry structures for documentation and structural assessment using machine learning
T2  - ENGINEERING STRUCTURES
AB  - The generation of numerical models for masonry structures is a timely and costly procedure since it requires the discretization of a large quantity of smaller particles. Similarly, traditional visual inspection involves the cautious consideration of each element on a masonry construction. In both cases, each brick element needs to be considered individually. The work presented in this document aims to alleviate the issues arising from doc-umenting individual masonry units and cracks on a structure using computer vision and convolutional neural networks (CNN). In particular, for the first time a dynamic workflow has been developed in which masonry units and cracks in masonry structures are automatically detected and used for the development of a complete geo-metric digital twin. The outcome is a collection of space coordinates and geometrical objects that represent the masonry fabric entity and allow the comprehension of the object for documentation and structural assessment. This interoperability between architectural, structural, and structural analysis models paves the way to use engineering to create a smarter, safer, and more sustainable future for our existing infrastructures.
SN  - 0141-0296
SN  - 1873-7323
DA  - JAN 15
PY  - 2023
VL  - 275
C7  - 115256
DO  - 10.1016/j.engstruct.2022.115256
C6  - NOV 2022
AN  - WOS:000892087600001
ER  -

TY  - JOUR
AU  - Hemdan, EE
AU  - Al-Atroush, ME
TI  - A Review Study of Intelligent Road Crack Detection: Algorithms and Systems
T2  - INTERNATIONAL JOURNAL OF PAVEMENT RESEARCH AND TECHNOLOGY
AB  - Crack and defect detection in civil engineering structures, including pavements, buildings, bridges, and roads, is vital for ensuring structural integrity, public safety, and long-term asset sustainability. While traditional detection methods have been widely used, they are often inefficient, inaccurate, and lack scalability, especially in real-time monitoring scenarios. The upward traffic volumes and growing complexity of structural systems aggravate these challenges. The key objective of this study is to discuss and review the currently utilized technologies as on crack detection and highlight the gap and future direction. Therefore, this review systematically analyzes modern developments in automated crack and defect detection. Besides, a review study was exploited to provide and discuss state-of-the-art Deep Learning (DL) approaches, as well as exploring the integration with advanced technologies as Internet of Things (IoT) and Digital Twins to facilitate real-time monitoring and predictive cack detection for smart road management in the context of smart cities. The study explores the limitations of traditional approaches and estimates how emerging knowledge can address problems for instance detection accuracy, workflow automation, and complex data management using Artificial Intelligence (AI) approaches as DL. The outcomes display the transformative potential of DL-based techniques when combined with IoT and Digital Twins, presenting smart crack detection, real-time data processing, and predictive potentials for smart road monitoring in the future. Also, this work explores key challenges, and future research scope, including enlightening interoperability, and enhancing real-time decision-making of crack detection for intelligent road management. This work presents a critical review for researchers and practitioners, suggesting a roadmap for leveraging advanced DL approaches for detecting pavement cracks methods to promise smart and resilient road infrastructure.
SN  - 1996-6814
SN  - 1997-1400
DA  - 2025 MAY 7
PY  - 2025
DO  - 10.1007/s42947-025-00556-x
C6  - MAY 2025
AN  - WOS:001483439600001
ER  -

TY  - JOUR
AU  - Coito, T
AU  - Martins, MSE
AU  - Firme, B
AU  - Figueiredo, J
AU  - Vieira, SM
AU  - Sousa, JMC
TI  - Assessing the impact of automation in pharmaceutical quality control labs using a digital twin
T2  - JOURNAL OF MANUFACTURING SYSTEMS
AB  - Nowadays, pharmaceutical Quality Control (QC) laboratories have complex workflows where analysts test different samples simultaneously. Tests ensure the physical properties of drugs are expected and within guidelines. Each test follows an analytical procedure containing tasks. Cyber-Physical Production System (CPPS) improves tasks/operations; however, accurate cost analysis with reasoned data is challenging. Theoretical estimation of impacts requires a high level of abstraction and fails to capture the proper behavior of the workflow. This paper proposes a method for evaluating the introduction of automation in a pharmaceutical QC laboratory. In the proposed methodology, this paper developed a simulation model of the analytical workflow of the tests. The impact assessment compares the current As-Is and future To-Be workflows, reworking the affected tasks. The model of the new resource is a hybrid parallel process with an initial buffer. The paper analyses several scenarios on parameters such as throughput, resource occupation, and annual man-hours gained. The simulation model was validated against actual historical data and compared to theoretical projections on the impact of automation. From our results, we found the available equipment has a high impact. Using production data, we project an increase in the analyst availability of 4,7% and equipment availability of 1,2%.
SN  - 1878-6642
SN  - 0278-6125
SN  - 1878-6642
DA  - JAN
PY  - 2022
VL  - 62
SP  - 270
EP  - 285
DO  - 10.1016/j.jmsy.2021.11.014
AN  - INSPEC:21894959
ER  -

TY  - JOUR
AU  - Cui, HC
AU  - Wu, JW
TI  - How Architectural Heritage Is Moving to Smart: A Systematic Review of HBIM
T2  - BUILDINGS
AB  - Heritage Building Information Modeling (HBIM) has emerged as a key tool in advancing heritage conservation and sustainable management. Preceding reviews had typically concentrated on specific technical aspects but did not provide sufficient bibliometric analysis. This study aims to integrate existing HBIM research to identify key research patterns, emerging trends, and forecast future directions. A total of 1516 documents were initially retrieved from the Web of Science Core Collection using targeted search terms. Following a relevance screening, 1175 documents were related to the topic. CiteSpace 6.4.R1, VOSviewer 1.6.20, and Bibliometrix 4.1, three bibliometric tools, were employed to conduct both quantitative and qualitative assessments. The results show three historical phases of HBIM, identify core journals, influential authors, and leading regions, and extract six major keyword clusters: risk assessment, data acquisition, semantic annotation, digital twins, and energy and equipment management. Nine co-citation clusters further outline the foundational literature in the field. The results highlight growing scholarly interest in workflow integration and digital twin applications. Future projections emphasize the transformative potential of artificial intelligence in HBIM, while also recognizing critical implementation barriers, particularly in developing countries and resource-constrained contexts. This study provides a comprehensive and systematic framework for HBIM research, offering valuable insights for scholars, practitioners, and policymakers involved in heritage preservation and digital management.
SN  - 2075-5309
DA  - JUL 28
PY  - 2025
VL  - 15
IS  - 15
C7  - 2664
DO  - 10.3390/buildings15152664
AN  - WOS:001549399700001
ER  -

TY  - JOUR
AU  - Pizzagalli, SL
AU  - Kuts, V
AU  - Otto, T
TI  - User-centred design in industrial collaborative automated systems
T2  - PROCEEDINGS OF THE ESTONIAN ACADEMY OF SCIENCES
AB  - Autonomous systems and collaborative robotics are part of the pillar technologies of the Industry 4.0 (I4.0) paradigm. These include advanced simulations, Digital Twins (DTs) and novel Human Machine Interfaces (HMIs). The increasing development of these technologies together with the higher requirements for customized production processes demands a closer collaboration between operators and automated systems. This leads to a redefinition of how human operators manage and interact with machines and how they are supported in this by adaptable interfaces, simulations and real-time data collection and analysis. New Human Robot Collaboration (HRC) paradigms are paramount in a scenario where the boundaries between human and machine performed tasks are flexible and increasingly dematerialized. The redefinition of standards, design methods, programming interfaces and assessment techniques is central to facilitate these technological and production changes. The augmentation of human capabilities in the workplace insists on a definition of a framework of requirements that would integrate human, organizational and production needs in the same scenario and workflow. This research proposes a User-Centred Design (UCD) approach which is crucial in addressing the open challenges of HRC systems. Our work regards the DT as well as Augmented and Virtual Reality (AR/VR) technologies as central in this process by considering them key tools for the design, control, and assessment of modern collaborative industrial scenarios.
SN  - 1736-6046
SN  - 1736-7530
PY  - 2021
VL  - 70
IS  - 4
SP  - 436
EP  - 443
DO  - 10.3176/proc.2021.4.10
AN  - WOS:000750607700010
ER  -

TY  - JOUR
AU  - Berti, F
AU  - Bridio, S
AU  - Luraghi, G
AU  - Pant, S
AU  - Allegretti, D
AU  - Pennati, G
AU  - Petrini, L
TI  - Reliable Numerical Models of Nickel-Titanium Stents: How to Deduce the Specific Material Properties from Testing Real Devices
T2  - ANNALS OF BIOMEDICAL ENGINEERING
AB  - The current interest of those dealing with medical research is the preparation of digital twins. In this frame, the first step to accomplish is the preparation of reliable numerical models. This is a challenging task since it is not common to know the exact device geometry and material properties unless in studies performed in collaboration with the manufacturer. The particular case of modeling Ni-Ti stents can be highlighted as a worst-case scenario due to both the complex geometrical features and non-linear material response. Indeed, if the limitations in the description of the geometry can be overcome, many difficulties still exist in the assessment of the material, which can vary according to the manufacturing process and requires many parameters for its description. The purpose of this work is to propose a coupled experimental and computational workflow to identify the set of material properties in the case of commercially-resembling Ni-Ti stents. This has been achieved from non-destructive tensile tests on the devices compared with results from Finite Element Analysis (FEA). A surrogate modeling approach is proposed for the identification of the material parameters, based on a minimization problem on the database of responses of Ni-Ti materials obtained with FEA with a series of different parameters. The reliability of the final result was validated through the comparison with the output of additional experiments.
SN  - 0090-6964
SN  - 1573-9686
DA  - APR
PY  - 2022
VL  - 50
IS  - 4
SP  - 467
EP  - 481
DO  - 10.1007/s10439-022-02932-1
C6  - FEB 2022
AN  - WOS:000761831900001
ER  -

TY  - JOUR
AU  - Pregnolato, M
AU  - Gunner, S
AU  - Voyagaki, E
AU  - De Risi, R
AU  - Carhart, N
AU  - Gavriel, G
AU  - Tully, P
AU  - Tryfonas, T
AU  - Macdonald, J
AU  - Taylor, C
TI  - Towards Civil Engineering 4.0: Concept, workflow and application of Digital Twins for existing infrastructure
T2  - AUTOMATION IN CONSTRUCTION
AB  - Digital Twins (DTs) are forecasted to be used in two-thirds of large industrial companies in the next decade. In the Architecture, Engineering and Construction (AEC) sector, their actual application is still largely at the prototype stage. Industry and academia are currently reconciling many competing definitions and unclear processes for developing DTs. There is a compelling need to establish DTs as practice in AEC by developing common procedures and standards tailored to the sector's procedures and use cases. This paper proposes a step-by-step workflow process for developing a DT for an existing asset in the built environment, providing a proof-ofconcept case study based on the Clifton Suspension Bridge in Bristol (UK). To achieve its aim, this paper (i) reviews the state-of-the-art of DTs in Civil Engineering, (ii) proposes a working DT-based workflow framework for the built environment applicable to existing assets, (iii) applies the framework and develops of the physicalvirtual architecture to a case study of bridge management, and finally (iv) discusses insights from the application. The main novelty lies in the development of a versatile methodological framework that can be applied to the broad context of civil infrastructure. This paper's importance resides in the knowledge challenge, value proposition and operation dictated by developing a DT workflow for the built environment, which ultimately represents a relevant use case for the digital transformation of national infrastructure.
SN  - 0926-5805
SN  - 1872-7891
DA  - SEP
PY  - 2022
VL  - 141
C7  - 104421
DO  - 10.1016/j.autcon.2022.104421
C6  - JUL 2022
AN  - WOS:000861483900002
ER  -

TY  - JOUR
AU  - Boan, T
AU  - Jiajun, L
AU  - Bosché, F
TI  - AUTONOMOUS MIXED REALITY FRAMEWORK FOR REAL-TIME CONSTRUCTION INSPECTION
T2  - JOURNAL OF INFORMATION TECHNOLOGY IN CONSTRUCTION
AB  - The increasing complexity in construction projects necessitates advancements in the precision and efficiency of inspection processes. In response to this challenge, the present study explores the feasibility of framework for autonomous inspection using Mixed Reality (MR), Building Information Modelling (BIM) and Artificial Intelligence (AI).The proposed framework encompasses techniques for: object detection in images taken through an MR headset; matching to the object instance in the digital twin; and visualisation of detection results in the MR headset to enable real-time human-in-the-loop decision making, thereby optimising the inspection workflow. The framework's efficacy is evaluated with two datasets representing diverse construction settings, including residential and office environments, focusing on the checking of the presence of ubiquitous elements like electrical sockets and switches. These tests illustrate the practical applicability and limitations of the proposed method.
SN  - 1874-4753
PY  - 2025
VL  - 30
SP  - 852
EP  - 874
DO  - 10.36680/j.itcon.2025.035
AN  - WOS:001503042200001
ER  -

TY  - CPAPER
AU  - Chheang, V
AU  - Weston, BT
AU  - Cerda, RW
AU  - Au, B
AU  - Giera, B
AU  - Bremer, PT
AU  - Miao, HC
A1  - ASSOC COMPUTING MACHINERY
TI  - A Virtual Environment for Collaborative Inspection in Additive Manufacturing
T2  - EXTENDED ABSTRACTS OF THE 2024 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2024
CP  - CHI Conference on Human Factors in Computing Sytems (CHI)
AB  - Additive manufacturing (AM) techniques have been used to enhance the design and fabrication of complex components for various applications in the medical, aerospace, energy, and consumer products industries. A defining feature for many AM parts is the complex internal geometry enabled by the printing process. However, inspecting these internal structures requires volumetric imaging, i.e., X-ray CT, leading to the well-known challenge of visualizing complex 3D geometries using 2D desktop interfaces. Furthermore, existing tools are limited to single-user systems making it difficult to jointly discuss or share findings with a larger team, i.e., the designers, manufacturing experts, and evaluation team. In this work, we present a collaborative virtual reality (VR) for the exploration and inspection of AM parts. Geographically separated experts can virtually inspect and jointly discuss data. It also supports VR and non-VR users, who can be spectators in the VR environment. Various features for data exploration and inspection are developed and enhanced via real-time synchronization. We followed usability and interface verification guidelines using Nielsen's heuristics approach. Furthermore, we conducted exploratory and semi-structured interviews with domain experts to collect qualitative feedback. Results reveal potential benefits, applicability, and current limitations. The proposed collaborative VR environment provides a new basis and opens new research directions for virtual inspection and team collaboration in AM settings.
SN  - 979-8-4007-0331-7
PY  - 2024
DO  - 10.1145/3613905.3650730
AN  - WOS:001227587701114
ER  -

TY  - JOUR
AU  - Holliman, NS
AU  - Antony, M
AU  - Charlton, J
AU  - Dowsland, S
AU  - James, P
AU  - Turner, M
TI  - Petascale Cloud Supercomputing for Terapixel Visualization of a Digital Twin
T2  - IEEE TRANSACTIONS ON CLOUD COMPUTING
AB  - Background-Photo-realistic terapixel visualization is computationally intensive and to date there have been no such visualizations of urban digital twins, the few terapixel visualizations that exist have looked towards space rather than earth. Objective-Our aims are: creating a scalable cloud supercomputer software architecture for visualization; a photo-realistic terapixel 3D visualization of urban IoT data supporting daily updates; a rigorous evaluation of cloud supercomputing for our application. Method-We migrated the Blender Cycles path tracer to the public cloud within a new software framework designed to scale to petaFLOP performance. Results-We demonstrate that we can compute a terapixel visualization in under one hour, the system scaling at 98 percent efficiency to use 1024 public cloud GPU nodes delivering 14 petaFLOPS. The resulting terapixel image supports interactive browsing of the city and its data at a wide range of sensing scales. Conclusion-The GPU compute resource available in the cloud is greater than anything available on our national supercomputers providing access to the globally competitive resources. The direct financial cost of access, compared to procuring and running these systems, was low. The indirect cost, in overcoming teething issues with cloud software development, should reduce significantly over time.
SN  - 2168-7161
DA  - JAN 1
PY  - 2022
VL  - 10
IS  - 1
SP  - 583
EP  - 594
DO  - 10.1109/TCC.2019.2958087
AN  - WOS:000766635400044
ER  -

TY  - CPAPER
AU  - Ippolito, A
AU  - Bartolomei, C
AU  - Mezzino, D
AU  - Attenni, M
AU  - Darwa, R
ED  - Tucci, G
ED  - Balletti, C
ED  - Bonora, V
ED  - Fassi, F
ED  - Spano, A
ED  - Parisi, EI
ED  - Previtali, M
ED  - Sammartano, G
TI  - DIGITAL DOCUMENTATION STRATEGIES FOR THE KNOWLEDGE OF THE BASILICA OF SANTA MARIA IN TRASTEVERE
T2  - 29TH CIPA SYMPOSIUM DOCUMENTING, UNDERSTANDING, PRESERVING CULTURAL HERITAGE. HUMANITIES AND DIGITAL TECHNOLOGIES FOR SHAPING THE FUTURE, VOL. 48-M-2
CP  - 29th CIPA Symposium on Documenting, Understanding, Preserving Cultural Heritage - Humanities and Digital Technologies for Shaping the Future
AB  - Within the Cultural Heritage documentation field, this contribution illustrates the adopted methods for the first digital documentation of the Basilica of Santa Maria in Trastevere in Rome.
   The research activity carried out included the survey of this historic building as well as direct and indirect research connected with its transformations. The tested workflow illustrates the relevance of the activities carried out in terms of knowledge implementation, conservation, monitoring, and dissemination of tangible and intangible aspects of this layered religious architecture.
SN  - 1682-1750
SN  - 2194-9034
PY  - 2023
SP  - 735
EP  - 742
DO  - 10.5194/isprs-archives-XLVIII-M-2-2023-735-2023
AN  - WOS:001185045000095
ER  -

TY  - JOUR
AU  - Saretta, E
AU  - Bonomo, P
AU  - Maeder, W
AU  - Nguyen, VK
AU  - Frontini, F
AU  - Kenny, R
AU  - Serra, JM
TI  - Digitalization as a driver for supporting PV deployment and cost reduction
T2  - EPJ PHOTOVOLTAICS
AB  - Digitalization is providing advantages to all sectors around the world and it can be of relevance also for the photovoltaic (PV) sector. As an example, the current value chain of the European PV sector is often characterized by analogue and fragmented processes that should be overcame to support greater PV deployment. The adoption of a more open and collaborative digital-based approach characterized by datasharing among different stakeholders and more integrated information thread from the design till O&M can provide direct benefits in optimizing the PV process, increasing performances, and reducing of costs. Therefore, a novel PV Information Management (PIM) approach has been drawn within the European 112020 project "SuperPV". In accordance with PIM objectives, a workflow for seamlessly transferring data along main PV work-stages has been developed, as well as new digital features to specifically address collaborative approach in the PV sector such as: (i) advanced functionalities introduced in the existing BIMSolar (R) software for improving the simultaneous design, performance simulation and cost assessment of medium and large PV systems, (ii) a proof-of-concept for aggregating all relevant information into a Digital Twin platform aimed at setting the ground for post-construction management and lifecycle assessment of the whole PV system.
SN  - 2105-0716
DA  - JAN 13
PY  - 2022
VL  - 13
C7  - 1
DO  - 10.1051/epjpv/2021013
AN  - WOS:000742174200001
ER  -

TY  - JOUR
AU  - Jonkers, RK
AU  - Shahroudi, KE
TI  - A Design Change, Knowledge, and Project Management Flight Simulator for Product and Project Success
T2  - IEEE SYSTEMS JOURNAL
AB  - Cost overruns and schedule delays are pervasive in complex projects despite the use of systems engineering and traditional project management models and tools. These disciplines can often work in isolation leading to inconsistencies in product information, tracking of design changes, and challenges in decision-making. While the literature proposes theoretical approaches to integrating these disciplines, there does not appear to be a practical approach offered. In response, this article presents a management flight simulator that represents a digital twin of set-based design, design change, knowledge, and agile project management practices. It integrates discipline-specific submodels through key linkages that are derived from the intrinsic properties of a system case study and intangible assets such as knowledge, communication, culture, and process maturity. It captures the techno-socio-economic and cultural factors involved in design change decisions and project management. The simulator provides immediate feedback on whether a change is going to help or disrupt design integrity through the monitoring of system attribute trends and cues. It also provides the impact on lifecycle management curves using a system dynamics submodel. From this feedback, several system, policy, and process levers are available within the simulator for what-if scenarios with the goal to improve product, organizational, and project performance.
SN  - 1932-8184
SN  - 1937-9234
DA  - MAR
PY  - 2021
VL  - 15
IS  - 1
SP  - 1130
EP  - 1139
DO  - 10.1109/JSYST.2020.3006747
AN  - WOS:000628985900111
ER  -

TY  - CPAPER
AU  - Aglietti, A
AU  - Biagini, C
AU  - Bongini, A
AU  - Ottobri, P
ED  - Tucci, G
ED  - Balletti, C
ED  - Bonora, V
ED  - Fassi, F
ED  - Spano, A
ED  - Parisi, EI
ED  - Previtali, M
ED  - Sammartano, G
TI  - HISTORIC BRIDGES MONITORING THROUGH SENSOR DATA MANAGEMENT WITH BIM METHODOLOGIES
T2  - 29TH CIPA SYMPOSIUM DOCUMENTING, UNDERSTANDING, PRESERVING CULTURAL HERITAGE. HUMANITIES AND DIGITAL TECHNOLOGIES FOR SHAPING THE FUTURE, VOL. 48-M-2
CP  - 29th CIPA Symposium on Documenting, Understanding, Preserving Cultural Heritage - Humanities and Digital Technologies for Shaping the Future
AB  - The implementation of geometric, alphanumerical and documentary data within BIM models is opening up interesting scenarios in the integrated management of existing infrastructure works. In particular, data from the application of sensors for structural monitoring of historic bridges can originate a flow of information exchange between real artifacts and Digital Twin capable of activating effective reactive or planned responses in the operation and maintenance phase of the asset by the facility manager. This paper intends to outline a BIM-oriented process workflow, which from the creation of parametric objects for infrastructural works using Scan-to-BIM acquisition techniques and procedures, arrives at the implementation of models aimed at the management of data from incoming and outgoing sensors towards analysis, supervision and control systems of the facilities organization. The case study of the Toppoli Bridge over the Arno River along the S.P. 64 road in the Province of Arezzo (IT), an artifact dating back to the early decades of the 1900s and built using traditional construction techniques and recently subject to renovation, is addressed. The experience conducted has highlighted how the BIM methodology for information management of existing assets, effectively responds to the current needs of monitoring and control of historical infrastructure assets, being able to integrate with multiple information management systems of the facilities organization, as well as the latest AI and AR technologies.
SN  - 1682-1750
SN  - 2194-9034
PY  - 2023
SP  - 33
EP  - 41
DO  - 10.5194/isprs-archives-XLVIII-M-2-2023-33-2023
AN  - WOS:001185045000006
ER  -

TY  - CPAPER
AU  - Cui, YS
AU  - Kara, S
AU  - Chan, KC
A1  - IEEE
TI  - Monitoring and Control of Unstructured Manufacturing Big Data
T2  - 2020 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL ENGINEERING AND ENGINEERING MANAGEMENT (IEEE IEEM)
CP  - IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)
AB  - Unstructured manufacturing big data silos are challenging for enabling various data-driven applications such as digital threads and digital twins in manufacturing. The management of big data silos requires to address the issues of large volume, data inconsistency, data redundancy, information silos and data security. This research developed a systematic approach to managing data silos using the state of art big data software. Applying this approach in the product life cycle can control data silos, data consistency, redundancy, timely update and enable the automatic workflow of each system.
SN  - 2157-3611
SN  - 978-1-5386-7220-4
PY  - 2020
SP  - 928
EP  - 932
DO  - 10.1109/ieem45057.2020.9309975
AN  - WOS:000821932400180
ER  -

TY  - JOUR
AU  - Hajirasouli, A
AU  - Assadimoghadam, A
AU  - Bashir, MA
AU  - Banihashemi, S
TI  - Exploring the Impact of Construction 4.0 on Industrial Relations: A Comprehensive Thematic Synthesis of Workforce Transformation in the Digital Era of Construction
T2  - BUILDINGS
AB  - The rise of Construction 4.0-driven by digitalisation, automation, and data-intensive technologies-is radically reshaping the construction industry. While its technological innovations are widely acknowledged, their implications for industrial relations remain underexplored. In this study, we conduct a systematic literature review (SLR) of 91 peer-reviewed articles published between 2010 and 2024, aiming to synthesise emerging knowledge on how Construction 4.0 is transforming workforce dynamics, employment models, and labour relations. Using NVivo software and an inductive thematic approach, we identify seven key themes: workforce transformation, the attraction of new generations and women, skill requirements and workforce development, supply chain and logistics optimisation, digital twin technology in project management, the emergence of new business models, and safety and risk assessment. Our findings highlight both opportunities-such as improved collaboration, skill diversification, and enhanced productivity-and challenges, including job displacement, digital ethics, and widening disparities between developed and developing countries. Recent studies from 2023 and 2024 underscore routine-biased changes in workforce structure, evolving project management practices through digital twins, and critical skill shortages within the sector. Furthermore, contemporary policy shifts and increasing labour tensions in some regions reveal deeper socio-economic implications of digital construction. This review contributes to a more holistic understanding of how technological innovation intersects with social systems in the built environment. The insights presented offer valuable guidance for policymakers, educators, and industry leaders seeking to navigate the evolving landscape of Construction 4.0.
SN  - 2075-5309
DA  - APR 24
PY  - 2025
VL  - 15
IS  - 9
C7  - 1428
DO  - 10.3390/buildings15091428
AN  - WOS:001486517900001
ER  -

TY  - JOUR
AU  - Abdelalim, AM
AU  - Said, SO
AU  - Alnaser, AA
AU  - Sharaf, A
AU  - Elsamadony, A
AU  - Kontoni, DPN
AU  - Tantawy, M
TI  - Agent-Based Modeling for Construction Resource Positioning Using Digital Twin and BLE Technologies
T2  - BUILDINGS
AB  - In response to the critical need for enhanced resource management in the construction industry, this research develops an innovative, integrated methodology that synergistically combines Agent-Based Modeling (ABM), Building Information Modeling (BIM), and Bluetooth Low Energy (BLE) technologies. Central to our approach is a sophisticated technological framework that incorporates a Client Early Warning System (CEWS) and a Decision Support System (DSS). These systems facilitate real-time monitoring and management of construction resources, ensuring operational efficiency and optimal resource utilization. Our methodology was empirically validated through a comprehensive case study at Helwan University's College of Engineering. The results demonstrated a significant enhancement in operational efficiency, particularly in resource allocation and progress tracking. Key practical outcomes include the development of a CEWS master dashboard that provides in-depth, real-time insights into project metrics. This dashboard was crucial for managing compliance with health protocols during the COVID-19 pandemic, showcasing the framework's adaptability to critical health standards. Further, the integration of indoor tracking technology revolutionized attendance tracking by replacing outdated manual methods with automated processes. This capability not only underscores the practical applicability of our research but also establishes a new benchmark for future technological advancements in construction project management. Our study sets the stage for subsequent innovations, paving the way for a more connected, efficient, and data-driven approach in the construction industry.
SN  - 2075-5309
DA  - JUN
PY  - 2024
VL  - 14
IS  - 6
C7  - 1788
DO  - 10.3390/buildings14061788
AN  - WOS:001254551000001
ER  -

TY  - JOUR
AU  - Ohene, E
AU  - Nani, G
AU  - Antwi-Afari, MF
AU  - Darko, A
AU  - Addai, LA
AU  - Horvey, E
TI  - Big data analytics in the AEC industry: scientometric review and synthesis of research activities
T2  - ENGINEERING CONSTRUCTION AND ARCHITECTURAL MANAGEMENT
AB  - Purpose - Unlocking the potential of Big Data Analytics (BDA) has proven to be a transformative factor for the Architecture, Engineering and Construction (AEC) industry. This has prompted researchers to focus attention on BDA in the AEC industry (BDA-in-AECI) in recent years, leading to a proliferation of relevant research. However, an in-depth exploration of the literature on BDA-in-AECI remains scarce. As a result, this study seeks to systematically explore the state-of-the-art review on BDA-in-AECI and identify research trends and gaps in knowledge to guide future research. Design/methodology/approach - This state-of-the-art review was conducted using a mixed-method systematic review. Relevant publications were retrieved from Scopus and then subjected to inclusion and exclusion criteria. A quantitative bibliometric analysis was conducted using VOSviewer software and Gephi to reveal the status quo of research in the domain. A further qualitative analysis was performed on carefully screened articles. Based on this mixed-method systematic review, knowledge gaps were identified and future research agendas of BDA-in-AECI were proposed. Findings - The results show that BDA has been adopted to support AEC decision-making, safety and risk assessment, structural health monitoring, damage detection, waste management, project management and facilities management. BDA also plays a major role in achieving construction 4.0 and Industry 4.0. The study further revealed that data mining, cloud computing, predictive analytics, machine learning and artificial intelligence methods, such as deep learning, natural language processing and computer vision, are the key methods used for BDA-in-AECI. Moreover, several data acquisition platforms and technologies were identified, including building information modeling, Internet of Things (IoT), social networking and blockchain. Further studies are needed to examine the synergies between BDA and AI, BDA and Digital twin and BDA and blockchain in the AEC industry. Originality/value - The study contributes to the BDA-in-AECI body of knowledge by providing a comprehensive scope of understanding and revealing areas for future research directions beneficial to the stakeholders in the AEC industry.
SN  - 0969-9988
SN  - 1365-232X
DA  - 2024 SEP 24
PY  - 2024
DO  - 10.1108/ECAM-01-2024-0144
C6  - SEP 2024
AN  - WOS:001318492700001
ER  -

TY  - JOUR
AU  - Zheng, Y
AU  - Masood, MK
AU  - Seppänen, O
AU  - Törmä, S
AU  - Aikala, A
TI  - Ontology-Based Semantic Construction Image Interpretation
T2  - BUILDINGS
AB  - Image-based techniques have become integral to the construction sector, aiding in project planning, progress monitoring, quality control, and documentation. In this paper, we address two key challenges that limit our ability to fully exploit the potential of images. The first is the "semantic gap" between low-level image features and high-level semantic descriptions. The second is the lack of principled integration between images and other digital systems used in construction, such as construction schedules and building information modeling (BIM). These challenges make it difficult to effectively incorporate images into digital twins of construction (DTC), a critical concept that addresses the construction industry's need for more efficient project management and decision-making. To address these challenges, we first propose an ontology-based construction image interpretation (CII) framework to formalize the interpretation and integration workflow. Then, the DiCon-SII ontology is developed to provide a formalized vocabulary for visual construction contents and features. DiCon-SII also acts as a bridge between images and other digital systems to help construct an image-involved DTC. To evaluate the practical application of DiCon-SII and CII in supporting construction management tasks and as a precursor to DTC, we conducted a case study involving drywall installation. Via this case study, we demonstrate how the proposed methods can be used to infer the operational stage of a construction process, estimate labor productivity, and retrieve specific images based on user queries.
SN  - 2075-5309
DA  - NOV
PY  - 2023
VL  - 13
IS  - 11
C7  - 2812
DO  - 10.3390/buildings13112812
AN  - WOS:001120970100001
ER  -

TY  - CPAPER
AU  - Bossecker, E
AU  - Calepso, AS
AU  - Kaiser, B
AU  - Verl, A
AU  - Sedlmair, M
ED  - Stolze, M
ED  - Loch, F
ED  - Baldauf, M
ED  - Alt, F
ED  - Schneedass, C
ED  - Kosch, T
ED  - Hirzle, T
ED  - Sadeghian, S
ED  - Draxler, F
ED  - Bektas, K
ED  - Lohan, K
ED  - Knierim, P
TI  - A Virtual Reality Simulator for Timber Fabrication Tasks Using Industrial Robotic Arms
T2  - PROCEEDINGS OF 2023 MENSCH UND COMPUTER, MUC 2023: Building Bridges
CP  - Conference on Mensch und Computer (MuC) - Building Bridges
AB  - Virtual Reality (VR) simulators are well known applications for immersive spaces. When it comes to Human-Robot Collaboration(HRC), training and safety are important aspects that can be supported by simulators. In this demo, we propose a workflow between ROS and a VR application that allows the use of real robot programming plans to control the robot's digital twin. We also provide the results of a short evaluation that was done with 6 experts, where they provided insights for future improvement and use cases. Our demo is the first step towards a full integrated system where path planning and fabrication steps can be tested with users without any of the risk and cost that are involved when using real industrial robots.
SN  - 979-8-4007-0771-1
PY  - 2023
SP  - 568
EP  - 570
DO  - 10.1145/3603555.3609316
AN  - WOS:001117817900075
ER  -

TY  - CPAPER
AU  - Abbasianfar, V
AU  - Karim, MR
AU  - Hague, S
AU  - Mohamed, Y
ED  - Francis, A
ED  - Miresco, E
ED  - Melhado, S
TI  - Evaluating the Interoperability of 3D Visualization Platforms in Federated Construction Simulation Settings
T2  - ADVANCES IN INFORMATION TECHNOLOGY IN CIVIL AND BUILDING ENGINEERING, ICCCBE 2024-VOL 1
CP  - 20th International Conference on Computing in Civil and Building Engineering-ICCCBE
AB  - In modern construction project management, the adoption of advanced 3D technologies is becoming a pivotal strategy for improving interoperability and visualization in construction management practices. However, challenges arise when these 3D applications need to be seamlessly connected to any network to project real-time data obtained from different sources, such as IoT sensors, Federated and Distributed Simulation models, and cameras, as part of the Digital Twin concept. This study examines three prominent 3D applications-Blender, Unreal Engine, and NVIDIA Omniverse-as key components in a federated simulation framework as visualizer federates. The primary goal is to enhance construction project visualization efficiency within federated simulation. Blender, known for its open-source nature, simplifies data integration and model creation. Unreal Engine, famous for real-time rendering, offers an immersive and lifelike visualization experience. NVIDIA Omniverse, an extensible 3D platform, introduces collaborative design possibilities, facilitating real-time data exchange and interactive simulations for construction project stakeholders. However, challenges arise when integrating these applications into federated simulations using High Level Architecture (HLA). These challenges include creating application connectors to enable seamless data exchange with other platforms using modeling and simulation standards like HLA, as well as converting 3D models between different formats. This study systematically evaluates the strengths and weaknesses of these three applications and suggests strategies to address the challenges encountered during their incorporation into federated simulations. By comprehensively assessing their potential as visualizer federates and addressing data exchange and model compatibility issues, the research aims to fully harness these 3D applications as representative platforms for visualizing construction project status in a federated simulation. This will ultimately enhance user experience when monitoring construction project progress.
SN  - 2366-2557
SN  - 2366-2565
SN  - 978-3-031-84210-8
SN  - 978-3-031-84208-5
SN  - 978-3-031-84207-8
PY  - 2025
VL  - 628
SP  - 258
EP  - 271
DO  - 10.1007/978-3-031-84208-5_21
AN  - WOS:001485259200021
ER  -

TY  - JOUR
AU  - Dang, ZY
AU  - Yang, Q
AU  - Deng, ZK
AU  - Han, JC
AU  - He, YH
AU  - Wang, SY
TI  - Digital Twin-Based Skill Training With a Hands-On User Interaction Device to Assist in Manual and Robotic Ultrasound Scanning
T2  - IEEE JOURNAL OF RADIO FREQUENCY IDENTIFICATION
AB  - Although teleconsultation and robot-based remote teleoperation in ultrasonography have facilitated reliable image acquisition, lack of training still seriously affects the accuracy of ultrasound diagnosis. In order to improve physicians' scanning skills, a more efficient training process is needed. However, present ultrasound training methods, such as the training phantom, are too costly for individual training. To explore new methods for training, the combined use of vision-based location tracking, motion sensing, and virtual ultrasound generation methods was studied in this work to offer a more low-cost and intuitive interaction system for virtual ultrasound scanning. The feasibility of the proposed workflow was verified by a user study on simple tasks and a clinical evaluation on a virtual ultrasound examination of three fetal heart specimens. As a result, the proposed interaction device facilitated virtual ultrasound examination by making it more efficient and intuitive, according to objective and subjective measurements. Therefore, while virtual ultrasound provides great data extensibility, the proposed device provides a smooth interaction, making the system simple and cost-effective for the end user. It is therefore concluded that the proposed method could provide a promising education system for the trainee to understand standard views of echocardiography and the corresponding anatomical correlations.
SN  - 2469-7281
PY  - 2022
VL  - 6
SP  - 787
EP  - 793
DO  - 10.1109/JRFID.2022.3205049
AN  - WOS:000899250200020
ER  -

TY  - JOUR
AU  - Buisson, B
AU  - Lakehal, D
TI  - Towards an integrated machine-learning framework for model evaluation and uncertainty quantification
T2  - NUCLEAR ENGINEERING AND DESIGN
AB  - We introduce a new paradigm for treating and exploiting simulation data, serving in parallel as an alternative workflow for model evaluation and uncertainty quantification. Instead of reporting simulations of base-case and specific variations scenarios, databases covering a wide spectrum of operational conditions are built by means of machine-learning using sophisticated mathematical algorithms. While the approach works for all sorts of computer-aided engineering applications, the present contribution addresses the CFD/CMFD sub-branch, with application to a widely used benchmark of convective flow boiling. In addition to comparing simulation and experimental results on a case-by-case basis, machine-learning is used to create their respective (CFD and experiment) data-driven models (DDM), which will in a later stage serve for assessing the predictive performance of the CFD models over a wider range of experimental conditions, hence providing a high-level classification of their range of applicability.
SN  - 0029-5493
SN  - 1872-759X
DA  - DEC 1
PY  - 2019
VL  - 354
C7  - 110197
DO  - 10.1016/j.nucengdes.2019.110197
AN  - WOS:000481647400045
ER  -

TY  - CPAPER
AU  - Jaros, M
AU  - Jaros, J
A1  - Assoc Computing Machinery
TI  - k-Dispatch: Enabling Cost-Optimized Biomedical-Workflow Offloading
T2  - PROCEEDINGS OF THE 33RD INTERNATIONAL SYMPOSIUM ON HIGH-PERFORMANCE PARALLEL AND DISTRIBUTED COMPUTING, HPDC 2024
CP  - 33rd International Symposium on High-Performance Parallel and Distributed Computing (HPDC)
AB  - Automated execution of computational workflows has become a critical issue in achieving high productivity in various research and development fields. Over the last few years, workflows have emerged as a significant abstraction of numerous real-world processes and phenomena, including digital twins, personalized medicine, and simulation-based science in general. k-Dispatch is a novel tool designed for the efficient offloading of biomedical workflows to remote high-performance computing clusters or cloud. In addition to data transfers, reporting, error handling and remote computations monitoring, k-Dispatch leverages a set of optimizations to dynamically determine suitable execution parameters for individual tasks within workflows, aiming to meet predefined constraints and optimization criteria. k-Dispatch has been successfully deployed within k-Plan, an advanced modelling tool for planning transcranial ultrasound stimulation (TUS) procedures.
SN  - 979-8-4007-0413-0
PY  - 2024
DO  - 10.1145/3625549.3658828
AN  - WOS:001305813000031
ER  -

TY  - JOUR
AU  - Bukantaite, S
AU  - Juzenas, K
TI  - Evaluation and Analysis of Production Processes Affected by Unmeasured Situations: Case Study of a Metal Processing Company
T2  - MECHANIKA
AB  - As the unpredicted situations can occur anytime, the production should be prepared to maintain the same pace as always. The pandemic of 2019 proved that proper production processes are vital. A case study was made in a metal processing company to examine how the workflow has changed after the implementation of virus prevention rules. Welding operation of metal furniture leg was observed and calculations presented - 115 minutes are wasted due to additional daily tasks caused by prevention of the pandemic. The recommendations were described and 160 minutes could be saved in accordance with them. The robotization of processes and DSS implementation were presented.
SN  - 1392-1207
SN  - 2029-6983
PY  - 2022
VL  - 28
IS  - 2
SP  - 152
EP  - 158
DO  - 10.5755/j02.mech.29285
AN  - WOS:000931768900008
ER  -

TY  - JOUR
AU  - Bounouioua, F
AU  - Saffidine, DR
AU  - Korichi, A
TI  - AN ENHANCED HBIM FRAMEWORK INTEGRATING ADVANCED TECHNOLOGIES TO STRENGTHEN THE CULTURAL HERITAGE
T2  - JOURNAL OF INFORMATION TECHNOLOGY IN CONSTRUCTION
AB  - Cultural heritage (CH) conveys values through every physical element and its intrinsic essence, necessitating careful attention to its preservation and longevity. In an era increasingly shaped by technology, digital conversion is an essential component of relevant research and is consistently considered for application in future CH strategies. This study addresses the unavailability of historical, graphical and technical records, in addition to the disparities in responsibilities that hinder the recognition and management of heritage in Algeria. This highlights the capability of digital exploration to initiate innovative approaches aimed at valorising heritage assets through documentation based on advanced technologies. An effective solution to represent and safeguard built heritage for professional application and dissemination via an implementation-based study that elucidates the proposed workflow across the National Theatre building in Algiers. Demonstrating that replication through a digital environment (DE) generates new ways for both physical and intangible interpretation using the digital twin (DT). Acknowledging the potential of the enhanced HBIM, this paper first describes digital surveying using 3D laser scanning. Secondly, it explores the pioneering application of artificial intelligence (AI) to process point clouds and improve semantic recognition, segmentation, and outputs. Finally, virtual reality (VR) combined with a software suite enriches the DT. The contribution to the body of knowledge lies in establishing a robust framework to investigate the relationship between AI, automated data preprocessing, and postprocessing to enhance the Scanto-BIM process and support conservation of historical buildings. It demonstrates efficient time and resource consumption, accuracy, and overall effectiveness. The main findings include the virtual extension of existing assets by linking various representation tools throughout a comprehensive prototype, where machine learning (ML) reinforced connections between reality capture (RC), cloud processing, and ultimately BIM. Meanwhile, VR provided an immersive experience that directly impacted user engagement and stakeholder interactions, thus facilitating decision-making related to building management and enabling remote assessment.
SN  - 1874-4753
PY  - 2024
VL  - 30
DO  - 10.36680/j.itcon.2025.024
AN  - WOS:001482983500001
ER  -

TY  - JOUR
AU  - Farghaly, K
AU  - Soman, R
AU  - Whyte, J
TI  - cSite ontology for production control of construction sites
T2  - AUTOMATION IN CONSTRUCTION
AB  - In the realm of construction production control, effective communication across operational levels and the rapid influx of diverse data are essential. Yet, integrating this data faces challenges due to disparate systems and a lack of common terminology, resulting in data silos and hindered interoperability. An ontology-based solution emerges as promising for enhancing interoperability. This research paper introduces the development, implementation, and assessment of the cSite ontology, encompasses several crucial facets necessary for efficient production control such as location, activities, and documents. To evaluate its practicality, a real-case study was conducted, wherein the ontology was employed to answer competency questions through SPARQL queries. Furthermore, interactive dashboards, situated within the construction control rooms, were developed to present the information visually. This paper underscores the transformative potential of integrated and visualised production information in construction projects. Additionally, it illuminates how the cSite ontology can facilitate the development and implementation of construction digital twins.
SN  - 0926-5805
SN  - 1872-7891
DA  - FEB
PY  - 2024
VL  - 158
C7  - 105224
DO  - 10.1016/j.autcon.2023.105224
C6  - DEC 2023
AN  - WOS:001137465300001
ER  -

TY  - JOUR
AU  - Jajcevic, D
AU  - Remmelgas, J
AU  - Toson, P
AU  - Matic, M
AU  - Hörmann-Kincses, T
AU  - Beretta, M
AU  - Rehrl, J
AU  - Poms, J
AU  - Boehling, P
AU  - O'Connor, T
AU  - Koolivand, A
AU  - Tian, G
AU  - Krull, SM
AU  - Khinast, JG
TI  - Development of a high-fidelity digital twin using the discrete element method for a continuous direct compression process. Part 2. Validation of calibration workflow
T2  - INTERNATIONAL JOURNAL OF PHARMACEUTICS
AB  - This paper is the second in a series of two that describes the application of discrete element method (DEM) and reduced order modeling to predict the effect of disturbances in the concentration of drug substance at the inlet of a continuous powder mixer on the concentration of the drug substance at the outlet of the mixer. In the companion publication, small-scale material characterization tests, a careful DEM parameter calibration and DEM simulations of the manufacturing process were used to develop a reliable RTD models. In the current work, the same calibration workflow was employed to evaluate the predictive ability of the resulting reduced-order model for an extended design space. DEM simulations were extrapolated using a relay race method and the cumulative RTD was accurately parameterized using the n-CSTR model. By performing experiments and simulations, a calibrated DEM model predicted the response of a continuous powder mixer to step changes in the inlet concentration of an API. Thus, carefully calibrated DEM models was used to guide and reduce experimental work and to establish an adequate control strategy. In addition, a further reduction in the computational effort was obtained by using the relay race method to extrapolate results. The predicted RTD curves were then parameterized to develop reduced order models and used to simulate the process in a matter of seconds. Overall, a control strategy evaluation tool based on high-fidelity DEM simulations was developed using material-sparing small-scale characterization tests.
SN  - 0378-5173
SN  - 1873-3476
DA  - DEC 5
PY  - 2024
VL  - 666
C7  - 124797
DO  - 10.1016/j.ijpharm.2024.124797
C6  - OCT 2024
AN  - WOS:001338567600001
ER  -

TY  - CPAPER
AU  - Hirman, M
AU  - Benesova, A
AU  - Steiner, F
AU  - Tupa, J
ED  - Ryan, A
ED  - Gordon, S
ED  - Tiernan, P
TI  - Project Management during the Industry 4.0 Implementation with Risk Factor Analysis
T2  - 29TH INTERNATIONAL CONFERENCE ON FLEXIBLE AUTOMATION AND INTELLIGENT MANUFACTURING (FAIM 2019): BEYOND INDUSTRY 4.0: INDUSTRIAL ADVANCES, ENGINEERING EDUCATION AND INTELLIGENT MANUFACTURING
CP  - 29th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM) - Beyond Industry 4.0 - Industrial Advances, Engineering Education and Intelligent Manufacturing
AB  - This paper analyses the process of Industry 4.0 implementation into the companies. The main purpose of this article was to determine the project framework for implementation of Industry 4.0 concept. The first part describes the basic principles of Industry 4.0 concept and 7 phases of Industry 4.0 implementation process. The second part describes the current state of art in the field of the project management in company with Industry 4.0. The third part deals with the implementation of Industry 4.0 project phase description and the definition of used methodology. The next part consist of the methodology application for the definition, evaluation and suggestion of solutions to reducing the risk threads for the typical production company in middle Europe. The last part deals with the project team members recommendations in case of small, medium and large companies. (C) 2019 The Authors. Published by Elsevier B.V.
SN  - 2351-9789
PY  - 2019
VL  - 38
SP  - 1181
EP  - 1188
DO  - 10.1016/j.promfg.2020.01.208
AN  - WOS:000889468900143
ER  -

TY  - JOUR
AU  - Pan, YX
AU  - Luo, KY
AU  - Liu, YM
AU  - Xu, C
AU  - Liu, Y
AU  - Zhang, L
TI  - Mobile edge assisted multi-view light field video system: Prototype design and empirical evaluation
T2  - FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE
AB  - Metaverse is recently envisioned as the main driver for immersive multimedia in future networks. Light field video (LFV), considered an intermediate transition scheme towards the metaverse, is conducive to sensing and reconstructing realistic scenes in the digital twins. However, the research on LFV systems still lacks comprehensive investigation, impeding their practical application. This paper addresses major technical challenges involved in LFV delivery through mobile networks, showing a trade-off between equipment cost, calibration effort, and workflow simplicity. It also considers the balance of communications and computations latency when maintaining reconstruction performance. Leveraging edge and cloud infrastructure, a novel Mobile Edge Assisted Multi-view Light-field-video System (MEAMLS) is proposed, integrating LFV collection, volumetric video construction, and an immersive viewing scheme. A custom-made LFV capture array is designed to capture real-world scenes, while employing a learning method-integrated edge server to facilitate adaptive LFV coding. Moreover, a fast sparse reconstruction algorithm is established, leveraging edge-cloud collaboration to minimize computation latency during volumetric video construction. Intelligent service for users is deployed on edge to enable viewport-driven virtual reality viewing. By providing an immersive visual experience, the proposed MEAMLS bridges the physical world and its digital twins. The system is prototyped on a realistic 5G network to empirically validate the performance under static and dynamic circumstances. Experimental results yield fundamental insights for designing mobile edge networks-assisted LFV system from the perspective of transmission resource, adaptability, and deployment.
SN  - 0167-739X
SN  - 1872-7115
DA  - APR
PY  - 2024
VL  - 153
SP  - 154
EP  - 168
DO  - 10.1016/j.future.2023.11.023
C6  - NOV 2023
AN  - WOS:001129817300001
ER  -

TY  - JOUR
AU  - Porsani, GB
AU  - de Lersundi, KD
AU  - Gutiérrez, ASO
AU  - Bandera, CF
TI  - Interoperability between Building Information Modelling (BIM) and Building Energy Model (BEM)
T2  - APPLIED SCIENCES-BASEL
AB  - Building information modelling (BIM) is the first step towards the implementation of the industrial revolution 4.0, in which virtual reality and digital twins are key elements. At present, buildings are responsible for 40% of the energy consumption in Europe and, so, there is a growing interest in reducing their energy use. In this context, proper interoperability between BIM and building energy model (BEM) is paramount for integrating the digital world into the construction sector and, therefore, increasing competitiveness by saving costs. This paper evaluates whether there is an automated or semi-automated BIM to BEM workflow that could improve the building design process. For this purpose, a residential building and a warehouse are constructed using the same BIM authoring tool (Revit), where two open schemas were used: green building extensible markup language (gbXML) and industry foundation classes (IFC). These transfer files were imported into software compatible with the EnergyPlus engine-Design Builder, Open Studio, and CYPETHERM HE-in which simulations were performed. Our results showed that the energy models were built up to 7.50% smaller than in the BIM and with missing elements in their thermal envelope. Nevertheless, the materials were properly transferred to gbXML and IFC formats. Moreover, the simulation results revealed a huge difference in values between the models generated by the open schemas, in the range of 6 to 900 times. Overall, we conclude that there exists a semi-automated workflow from BIM to BEM which does not work well for big and complex buildings, as they present major problems when creating the energy model. Furthermore, most of the issues encountered in BEM were errors in the transfer of BIM data to gbXML and IFC files. Therefore, we emphasise the need to improve compatibility between BIM and model exchange formats by their developers, in order to promote BIM-BEM interoperability.
SN  - 2076-3417
SN  - 2076-3417
DA  - MAR
PY  - 2021
VL  - 11
IS  - 5
C7  - 2167
DO  - 10.3390/app11052167
AN  - CCC:000627981100001
ER  -

TY  - JOUR
AU  - Frede, TA
AU  - Nikbin, N
AU  - Kockmann, N
TI  - Reactor performance estimation in microscale flow calorimeter for rapid characterization of exothermic reactions
T2  - JOURNAL OF FLOW CHEMISTRY
AB  - Continuous flow calorimeters are a promising tool in process development and safety engineering, especially for flow chemistry applications to characterize the heat release and kinetic parameters of rapid chemical reactions. In this study, the digital accompaniment of an isoperibolic flow calorimeter for characterization of exothermic reactions is presented. To support experimental planning and evaluation, computational fluid dynamic simulations are carried out for single-phase flow in the microreactor. The residence time distribution is obtained and used for estimation of conversion and temperature profiles along the microreactor channel. This leads to an integration of CFD simulations into the calorimeter's software-guided workflow reducing the experimental effort regarding the determination of thermokinetic data. The approach is tested for a highly exothermic test reaction, which provides further hints for future investigations.
SN  - 2062-249X
SN  - 2063-0212
DA  - MAR
PY  - 2023
VL  - 13
IS  - 1
SP  - 31
EP  - 44
DO  - 10.1007/s41981-022-00251-z
C6  - DEC 2022
AN  - WOS:000898309200002
ER  -

TY  - JOUR
AU  - Terkaj, W
AU  - Gaboardi, P
AU  - Trevisan, C
AU  - Tolio, T
AU  - Urgo, M
TI  - A digital factory platform for the design of roll shop plants
T2  - CIRP JOURNAL OF MANUFACTURING SCIENCE AND TECHNOLOGY
AB  - Replying to requests for quotation in a fast and effective way is a key need for technology providers, in particular machine tool builders and system integrators. Digital factory technologies provide the opportunity of speeding up the generation of technical offers through the development of a digital twin of the system under study, thus enabling the assessment of different candidate configurations and the associated performance. In this paper we present a set of integrated digital tools to support the design of roll shop plants, i.e. plants dedicated to grinding cylinders for rolling mills. These digital tools are aimed to engineers and provide a configuration workflow, a 3D environment and performance evaluation tools. The interoperability among the software modules and the reuse of knowledge is enhanced by semantic web technologies and the definition of a common data model as an ontology relying on technical standards. (C) 2019 CIRP.
SN  - 1755-5817
DA  - AUG
PY  - 2019
VL  - 26
SP  - 88
EP  - 93
DO  - 10.1016/j.cirpj.2019.04.007
AN  - WOS:000497727400008
ER  -

TY  - JOUR
AU  - Turk, Z
AU  - Klinc, R
TI  - A social-product-process framework for construction
T2  - BUILDING RESEARCH AND INFORMATION
AB  - This paper introduces a new framework for understanding, modelling and software engineering in construction information activities. The current framework is based on understanding that products are the results of processes, which are performed by actors. Such frameworks are influenced by the available technology. The Internet of today is supporting also other kinds of human activities: communication and social interactions among humans. The construction industry uses them as well but without having a proper understanding of their role. There is a gap in the current framework. In this paper, the relevant theories to specify this gap in terms of what could exist in theory and what is offered by technology are analysed. As a result, a new framework of construction information activities that fills the gap is proposed. The key concepts and relations among them are identified and elaborated within the existing framework. The framework introduces the third major integrative element of the otherwise fragmented construction information activities - the social network - the existing two being the physical building and its digital twin. The framework provides a theoretical and conceptual basis for designing, planning, creating, monitoring and evaluating construction-related online services that include a strong social component and use social media services.
SN  - 0961-3218
SN  - 1466-4321
DA  - OCT 2
PY  - 2020
VL  - 48
IS  - 7
SP  - 747
EP  - 762
DO  - 10.1080/09613218.2019.1691487
C6  - NOV 2019
AN  - WOS:000499769700001
ER  -

TY  - CPAPER
AU  - Yang, JS
AU  - Wilde, A
AU  - Menzel, K
AU  - Sheikh, MZ
AU  - Kuznetsov, B
ED  - Camarinha-Matos, LM
ED  - Boucher, X
ED  - Ortiz, A
TI  - Computer Vision for Construction Progress Monitoring: A Real-Time Object Detection Approach
T2  - COLLABORATIVE NETWORKS IN DIGITALIZATION AND SOCIETY 5.0, PRO-VE 2023
CP  - 24th IFIP WG 5.5 Working Conference on Virtual Enterprises (PRO-VE)
AB  - Construction progress monitoring (CPM) is essential for effective project management, ensuring on-time and on-budget delivery. Traditional CPM methods often rely on manual inspection and reporting, which are time-consuming and prone to errors. This paper proposes a novel approach for automated CPM using state-of-the-art object detection algorithms. The proposed method leverages e.g. YOLOv8's real-time capabilities and high accuracy to identify and track construction elements within site images and videos. A dataset was created, consisting of various building elements and annotated with relevant objects for training and validation. The performance of the proposed approach was evaluated using standard metrics, such as precision, recall, and F1-score, demonstrating significant improvement over existing methods. The integration of Computer Vision into CPM provides stakeholders with reliable, efficient, and cost-effective means to monitor project progress, facilitating timely decision-making and ultimately contributing to the successful completion of construction projects.
SN  - 1868-4238
SN  - 1868-422X
SN  - 978-3-031-42624-7
SN  - 978-3-031-42622-3
SN  - 978-3-031-42621-6
PY  - 2023
VL  - 688
SP  - 660
EP  - 672
DO  - 10.1007/978-3-031-42622-3_47
AN  - WOS:001359444600047
ER  -

TY  - JOUR
AU  - Li, SP
AU  - Zhang, B
AU  - Tong, GQ
AU  - Li, YH
AU  - Liu, ZQ
AU  - Shi, B
AU  - Geng, J
AU  - Liu, DM
AU  - Wang, HW
AU  - Ai, QS
AU  - Ding, JX
AU  - Gan, Z
TI  - Online Intelligent Monitoring System and Key Technologies for Dam Operation Safety
T2  - ADVANCES IN CIVIL ENGINEERING
AB  - To realize the comprehensive intelligent upgrade of the Three Gorges Dam safety intelligent monitoring system (IMS), we focus on three core pillars real-time information processing, professional analytical evaluation, and digital management control systematically overcoming critical technical bottlenecks. By deeply integrating artificial intelligence (AI), Internet of Things (IOT), big data analysis, and geographic information system + building information modeling (GIS + BIM) ecosystems, we conducted a holistic diagnosis of existing monitoring systems to precisely identify operational pain points. Leveraging our proprietary innovations, including a GIS + BIM digital base, smart algorithm matrix, and BIM-based finite element computing system, we successfully developed the Three Gorges Dam intelligent monitoring platform, delivering five core value propositions: (1) Achieve real-time and historical aggregation of comprehensive data with dam safety management as the core, fully encompassing various types of environmental monitoring data. (2) Utilizing "GIS + BIM" as the technical foundation, construct a digital twin geometric model of the hub monitoring physical world, enabling intuitive and precise representation of engineering status. (3) Implement online rapid structural calculation, analysis, and early warning based on "BIM + Finite Element" technology, providing timely and reliable support for safety decision-making. (4) Establish a monitoring data analysis model through machine learning intelligent algorithms, deeply mining data value to enable intelligent prediction of potential safety hazards. (5) Promote digital transformation of manual inspection workflows using "IOT + Micro-INS" technology, enhancing inspection efficiency and accuracy. Additionally, our workflow engine ensures full-process digital collaboration across safety monitoring operations, guaranteeing seamless interdepartmental coordination. These innovations have not only enhanced safety management efficiency but also cemented the Three Gorges Dam's global leadership in hydraulic engineering. As a landmark achievement in national strategic infrastructure, it exemplifies the digital transformation of mega-scale engineering projects in the modern era.
SN  - 1687-8086
SN  - 1687-8094
PY  - 2025
VL  - 2025
IS  - 1
C7  - 9983255
DO  - 10.1155/adce/9983255
AN  - WOS:001451893000001
ER  -

TY  - JOUR
AU  - García, FLD
AU  - Cruz, AJG
AU  - Menéndez, CA
AU  - Arauz, DS
AU  - Zunzunegui, JRA
AU  - Crespo, MP
AU  - Morales, SG
AU  - Sánchez-Aparicio, LJ
TI  - A Unified Virtual Model for Real-Time Visualization and Diagnosis in Architectural Heritage Conservation
T2  - BUILDINGS
AB  - The aim of this paper is to propose a workflow for the real-time visualization of virtual environments that supports diagnostic tasks in heritage buildings. The approach integrates data from terrestrial laser scanning (3D point clouds and meshes), along with panoramic and thermal images, into a unified virtual model. Additionally, the methodology incorporates several post-processing stages designed to enhance the user experience in visualizing both the building and its associated damage. The methodology was tested on the Medieval Templar Church of Vera Cruz in Segovia, utilizing a combination of visible and infrared data, along with manually prepared damage maps. The project results demonstrate that the use of a hybrid digital model-combining 3D point clouds, polygonal meshes, and panoramic images-is highly effective for real-time rendering, providing detailed visualization while maintaining adaptability for mobile devices with limited computational power.
SN  - 2075-5309
DA  - NOV
PY  - 2024
VL  - 14
IS  - 11
C7  - 3396
DO  - 10.3390/buildings14113396
AN  - WOS:001366742600001
ER  -

TY  - CPAPER
AU  - Panella, F
AU  - Lucy, J
AU  - Fisk, E
AU  - Huang, ST
AU  - Loo, Y
ED  - Benardos, A
ED  - Anagnostou, G
ED  - Marinos, VP
TI  - Computer vision and machine learning for cost-effective fully automated visual inspection of tunnels: A case study
T2  - PROCEEDINGS OF THE ITA-AITES WORLD TUNNEL CONGRESS 2023, WTC 2023: Expanding Underground-Knowledge and Passion to Make a Positive Impact on the World
CP  - ITA-AITES World Tunnel Congress (WTC) / 49th General Assembly of the International-Tunnelling-and-Underground-Space-Association (ITA-AITES)
AB  - Effective asset management of underground infrastructure requires timely and detailed visual inspections for condition monitoring. To date, the common approach for visual inspections is heavily manual and can be slow, expensive, and subjective to the engineer's experience. The present paper describes the end-to-end development of an automated visual inspection workflow. The proposed two-stage pipeline applies the state-of-the-art Deep Learning (DL) algorithms for object detection and tracking to identify structural defects in tunnels. Then, via a novel computer vision approach, it detects and tracks natural features for a precise sensor-less in-tunnel defect mapping. The output is a DL-powered digital twin of the inspected infrastructure. It drastically reduces manual input for repetitive tasks and focuses on the employment of highly trained engineers for validation purposes only. This means that engineering knowledge is more effectively spent. These results will impact the construction industry's approach to visual inspections shifting it towards automated strategies.
SN  - 978-1-003-34803-0
PY  - 2023
SP  - 2822
EP  - 2828
DO  - 10.1201/9781003348030-340
AN  - WOS:001150380204056
ER  -

TY  - JOUR
AU  - Lin, SW
AU  - Duan, LP
AU  - Liu, JM
AU  - Xiao, X
AU  - Miao, J
AU  - Zhao, JC
TI  - Automated geometric reconstruction and cable force inference for cable-net structures using 3D point clouds
T2  - AUTOMATION IN CONSTRUCTION
AB  - Laser scanning provides an efficient solution to digital twin construction in civil engineering. The complexity and redundancy of large-scale point clouds substantially prolong the labor-intensive model reconstruction process. This paper presents an automated and high-precision geometric reconstruction approach for cable-net structures with a complete workflow from raw points to the extraction of cable shapes and forces. The strategy involves the extraction of coordinates of key cable regions through the characterization of cable geometry and local registration methods, followed by the computation of cable forces and shapes using segmented catenary theory and nonlinear optimization. The approach is validated with a single-curvature cable-net structure, with the accuracy of cable shapes within +/- 5 mm and errors in cable forces <5%. The method contributes to health monitoring and rapid reconstruction of cable-net structures. The precise geometry and forces will facilitate the creation of the mechanical models reflecting the physical authenticity of structures.
SN  - 0926-5805
SN  - 1872-7891
DA  - SEP
PY  - 2024
VL  - 165
C7  - 105543
DO  - 10.1016/j.autcon.2024.105543
C6  - JUN 2024
AN  - WOS:001258731600001
ER  -

TY  - JOUR
AU  - Lange, J
AU  - Waldschmitt, B
AU  - Costanzi, CB
TI  - 3D-printed columns with exceptional geometry
T2  - STAHLBAU
AB  - Wire Arc Additive Manufacturing (WAAM) is a welding process used to build up three-dimensional structures in steel. Like other Additive Manufacturing technologies, it allows for geometrically-exceptional structures to be fabricated which are otherwise unfeasible or very expensive to manufacture using traditional methods. This paper presents an integrated design approach to the use of WAAM in the context of large-scaled applications, focusing on columns with exceptional geometric complexity. It combines material behaviour and process parameter research, with the aim of providing a digital tool to design and print structures using WAAM. To achieve the desired geometries, necessary welding parameters are stored and applied to a rudimentary digital twin model. This is complimented by multiple process-control checks, which are implemented during the printing process to ensure that an object is generated as planned. Finally, the structures are manufactured and are subjected to a critical evaluation in order to identify the possible future potential. The challenge of combining geometric complexity with manufacturing for large scale represents a next step in the integration of WAAM in steel constructions.
SN  - 0038-9145
SN  - 1437-1049
DA  - JUN
PY  - 2022
VL  - 91
IS  - 6
SP  - 365
EP  - 374
DO  - 10.1002/stab.202200020
C6  - MAY 2022
AN  - WOS:000800928400001
ER  -

TY  - CPAPER
AU  - Mahmood, K
AU  - Otto, T
AU  - Chakraborty, A
ED  - Mpofu, K
ED  - Sacks, N
ED  - Damm, O
TI  - Layout planning and analysis of a Flexible Manufacturing System based on 3D Simulation and Virtual Reality
T2  - 56TH CIRP CONFERENCE ON MANUFACTURING SYSTEMS, CIRP CMS 2023
CP  - 56th Conference on Manufacturing Systems-CMS
AB  - The high competition in the global market where the agile product launch and variable demand of goods by customers persuaded manufacturing companies to adopt Flexible Manufacturing System (FMS) and many companies have already implemented FMS solutions. On the other hand, manufacturing digitalization such as digital twin development, Industrial Virtual Reality (IVR), virtual modeling and 3D simulation of manufacturing systems offer new possibilities for effective facility layout planning, quick and easy modification and validation of production processes, analysis, and optimize the workflow and activities conducted on a factory floor. These new technological developments in digitalization have changed the thinking of manufacturing companies and they are eager to use digitalization solutions in their factory operations. However, there is a lack of harmonized methods and procedures to implement virtual modeling, 3D simulation, and IVR for layout planning and analysis of FMS. This paper proposed an approach for performance analysis of a FMS that is based on the 3D layout creation and simulation in a virtual environment, monitoring of key performance indicators via a digital dashboard, and immersive visualization through virtual reality. The relevance and feasibility of the proposed performance analysis approach are demonstrated by a case study. (c) 2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)
SN  - 2212-8271
PY  - 2023
VL  - 120
SP  - 201
EP  - 206
DO  - 10.1016/j.procir.2023.08.036
DO  - 10.1016/j.procir.2023.08.036
AN  - INSPEC:24672639
ER  -

TY  - JOUR
AU  - Wautelet, Y
AU  - Rouget, X
TI  - Circulise4LCE, a model-driven sustainability development framework for local circular economy engineering and simulation
T2  - SOFTWARE AND SYSTEMS MODELING
AB  - A local circular economy (LCE) is a closed-loop system operating at a community or regional scale where resources are efficiently recycled, repurposed, and managed to achieve sustainability. This paper introduces Circulise4LCE, a model-driven framework designed to support the development of LCEs through conceptual modeling. The framework consists of an i* LCE pattern, a circular BPMN workflow, a derived class diagram and a supporting simulation environment. It is designed to be adaptable, scalable, and replicable across various local contexts. The framework is applied to the Fanyatu case study (a non-profit organization supporting reforestation in the Congo Basin) allowing to show its ability to synchronize social, technological, and environmental stakeholders in a minimal system. The application leads to a simulation environment offering dynamic analysis of circularity and allowing for optimization and evaluation of local sustainability solutions. Results show how the framework helps address sustainability challenges by aligning resource management, community engagement, and technology integration.
SN  - 1619-1366
SN  - 1619-1374
DA  - 2025 AUG 2
PY  - 2025
DO  - 10.1007/s10270-025-01314-0
C6  - AUG 2025
AN  - WOS:001541855300001
ER  -

TY  - JOUR
AU  - Davison, AM
AU  - de Koning, K
AU  - Taubert, F
AU  - Schakel, JK
TI  - Automated near real-time monitoring in ecology: Status quo and ways forward
T2  - ECOLOGICAL INFORMATICS
AB  - In the current epoch of rapid biodiversity decline, monitoring of ecosystems and the species which inhabit them has become increasingly important. A near real-time approach to ecological monitoring facilitates decision making and timely interventions within rapidly changing systems. Despite fast-paced technological advancements making the automated workflows required for near real-time ecological monitoring possible, their use is highly limited and there is yet to be a review of the current capacity for their creation. This paper summarises the current methods and technologies which could be used to create such workflows and the considerations for establishing them in decision-making systems. We identify key barriers to the adoption of a NRT approach across geographies and different fields of study in ecology. We also highlight the need to work collaboratively with technologists and stakeholders to establish efficient and long-lasting NRT workflows which can inform evidencebased decision making.
SN  - 1574-9541
SN  - 1878-0512
DA  - NOV
PY  - 2025
VL  - 89
C7  - 103157
DO  - 10.1016/j.ecoinf.2025.103157
C6  - APR 2025
AN  - WOS:001483104100001
ER  -

TY  - JOUR
AU  - Elmousalami, H
AU  - Maxy, M
AU  - Hui, FKP
AU  - Aye, L
TI  - AI in automated sustainable construction engineering management
T2  - AUTOMATION IN CONSTRUCTION
AB  - The construction industry is responsible for approximately 39 % of global carbon emissions. This paper presents a systematic literature review on the advancements and applications of Artificial Intelligence (AI) in Automated Sustainable Construction Engineering Management. It investigates the transformative role of AI in driving construction automation, sustainability, and improving operational efficiency. The review highlights key IoT, digital twins, and big data analytics developments for real-time construction site monitoring, carbon footprint reduction, and structural health monitoring. In addition, project management automation, including automatic quality control and cost-duration prediction using green AI is reviewed. The article explores the integration of AI in construction robotics, including 4D printing and smart robotics. It highlights the importance of electroencephalogram for human-centric safety, augmented reality for training, and energy-efficient Heating, Ventilation, and Air Conditioning (HVAC) control systems for green buildings. It addresses challenges, limitations, and future research directions for AI in fostering sustainable, efficient, and automated construction.
SN  - 0926-5805
SN  - 1872-7891
DA  - JUL
PY  - 2025
VL  - 175
C7  - 106202
DO  - 10.1016/j.autcon.2025.106202
C6  - APR 2025
AN  - WOS:001480516200001
ER  -

TY  - JOUR
AU  - Gallego-García, S
AU  - Ren, DQ
AU  - Gallego-García, D
AU  - Pérez-García, S
AU  - García-García, M
TI  - Dynamic Innovation Information System (DIIS) for a New Management Age
T2  - APPLIED SCIENCES-BASEL
AB  - Featured Application Generation of a Dynamic Innovation Information System (DIIS) for optimized planning and decision-making thanks to the dynamic evaluation of innovations over their life cycle, applying a methodology for Digital Ecosystems in the Fourth Industrial Revolution and an innovation management model based on the Viable System Model. Innovations are essential for global development and market dynamics. Innovation management is central to organizations for gaining adaptability and dynamic capabilities to ensure their sustainability over time. Right decisions are essential for the implementation of innovations. However, on many occasions, especially in the product development process, decisions are taken based on static analysis, qualitative criteria, questionnaires, and/or quantitative evaluations that are outdated. Moreover, many innovation developments do not consider the existing databases in their information systems of similar innovation projects, especially in the early phases of new innovations when evaluations are mainly driven by area, group, or person. Furthermore, inventions are introduced in different regions, plants, and socio-economic situations, providing different results. In this context, considering that innovations shape our current and future world, including all products and services, as well as how humans, organizations, and machines interact, the significance of the paper is clear. Therefore, it is necessary to develop an innovation management model based on the Viable System Model to cope with any potential future environment based on internal organizational capabilities. For this purpose, the paper designs a Digital Ecosystem for the Fourth Industrial Revolution (DE4.0) based on the Plan-Do-Check-Act methodology applicable to any information system consisting of a digital twin, a simulation model, databases from existing information systems, and quality management techniques. This DE4.0 provides a huge advantage for the applicability and scalability of innovations as it allows one to plan, monitor, assess, and improve. Moreover, based on the conceptual model, a generic project evaluation scheme is developed, providing a platform for innovation project management and control during the whole innovation life cycle. As a result, the research provides a scientific and practical contribution for an integrated management of innovations based on the best information and set of techniques available. Based on this framework, a supply-chain case study is developed. The results show how, depending on the intended goals, the past experiences, the evolution of the innovation, and the innovation scope, indicators can be influenced towards reaching the initial goals and reducing the innovation risks. Finally, a discussion about the potential use and role of the DE4.0 for innovation projects and the related learning process is performed.
SN  - 2076-3417
SN  - 2076-3417
DA  - JUL
PY  - 2022
VL  - 12
IS  - 13
C7  - 6592
DO  - 10.3390/app12136592
AN  - WOS:000823530700001
ER  -

TY  - JOUR
AU  - Di Donato, D
AU  - Dal Pont, G
AU  - Navoni, L
AU  - Massarino, S
AU  - Beretta, A
TI  - An Applicative Industrial Metaverse Experience in Semiconductor Industry
T2  - IEEE ENGINEERING MANAGEMENT REVIEW
AB  - This article proposes a practical example of the application of the emerging concept of Industrial Metaverse in the semiconductor industry, and its impact on the role of humans and sustainability in the contemporary industrial context. It argues that the use of some technologies included in the Industrial Metaverse concept is bringing humans, after being marginalized by machines during the Industrial Revolution and by computers in the subsequent digital era, back to their centrality in the industrial landscape boosting their performance. Furthermore, positive impacts on sustainability have been noted, which, however, require further numerical investigation. On the other hand, the absence of consolidated academic metrics to evaluate those relations calls for further research to fully understand the impact of the Industrial Metaverse on human resources and sustainability in a design and construction project and thereafter in operations and to develop appropriate measures to monitor its effects, through a proposed model.
SN  - 0360-8581
SN  - 1937-4178
SN  - 1937-4178
DA  - JUN
PY  - 2025
VL  - 53
IS  - 3
SP  - 144
EP  - 159
DO  - 10.1109/emr.2025.3529247
DO  - 10.1109/EMR.2025.3529247
AN  - WOS:001554450400009
ER  -

TY  - CPAPER
AU  - Casto, M
AU  - Perrone, D
AU  - Nascimbene, R
AU  - Micelli, F
AU  - Calvi, P
AU  - Aiello, MA
ED  - Strauss, A
ED  - Bergmeister, K
TI  - Toward a BIM-based procedure for the evaluation of a risk prioritization class of bridge structures
T2  - EUROPEAN ASSOCIATION ON QUALITY CONTROL OF BRIDGES AND STRUCTURES, EUROSTRUCT 2023, VOL 6, ISS 5
CP  - Annual Meeting of the European-Association-on-Quality-Control-of-Bridges-and-Structures (EUROSTRUCT)
AB  - Building Information Modelling is receiving increasing attention as an emerging technology for enhancing the life-cycle performance of facilities. The cooperation between all professionals involved in the construction chain allows to reproduce digital twins of the structures including information related to different disciplines, such as structural and architectural aspects. In this context, the use of Building Information Modelling, combined with an automatic tool to predict risk indices, could be particularly useful when dealing with existing bridges due to the significant structural deficiencies recently demonstrated by numerous bridge collapses worldwide. This paper explores the development of a BIM-based procedure to bring greater value in the design, construction, maintenance, and inspection stages. The procedure is applied to existing bridges and it allows to estimate a structural risk prioritization class of bridges according to maintained activities, periodical inspections, structural detailing and modelling uncertainties. The automatic BIM-based tool uses a workflow developed in Dynamo, and is able to automatically update the structural risk class and to provide warning if the structural safety index exceed defined thresholds.
PY  - 2023
SP  - 448
EP  - 455
DO  - 10.1002/cepa.2013
AN  - WOS:001256546000120
ER  -

TY  - CPAPER
AU  - Mitterberger, D
AU  - Angelaki, EM
AU  - Salveridou, F
AU  - Rust, R
AU  - Vasey, L
AU  - Gramazio, F
AU  - Kohler, M
ED  - Gengnagel, C
ED  - Baverel, O
ED  - Betti, G
ED  - Popescu, M
ED  - Thomsen, MR
ED  - Wurm, J
TI  - Extended Reality Collaboration: Virtual and Mixed Reality System for Collaborative Design and Holographic-Assisted On-site Fabrication
T2  - TOWARDS RADICAL REGENERATION
CP  - 8th Design Modelling Symposium on Towards Radical Regeneration
AB  - Most augmented and virtual applications in architecture, engineering, and construction focus on structured and predictable manual activities and routine cases of information exchange such as quality assurance or design review systems. However, collaborative design activities such as negotiation, task specification, and interaction are not yet sufficiently explored. This paper presents a mixed-reality immersive collaboration system that enables bi-directional communication and data exchange between on-site and off-site users, mutually accessing a digital twin. Extended Reality Collaboration (ERC) allows building site information to inform design decisions and new design iterations to be momentarily visualized and evaluated on-site. Additionally, the system allows the developed design model to be fabricated with holographic instructions. In this paper, we present the concept and workflow of the developed system, as well as its deployment and evaluation through an experimental case study. The outlook questions how such systems could be transferred to current design and building tasks and how such a system could reduce delays, avoid misunderstandings and eventually increase building quality by closing the gap between the digital model and the built architecture.
SN  - 978-3-031-13249-0
SN  - 978-3-031-13248-3
PY  - 2023
SP  - 283
EP  - 295
DO  - 10.1007/978-3-031-13249-0_24
AN  - WOS:000870223800024
ER  -

TY  - JOUR
AU  - Cho, J
AU  - Rodrigazo, SA
AU  - Kim, C
AU  - Kim, Y
AU  - Kim, Y
AU  - Yeon, J
TI  - Data-driven monitoring system for ground reinforcement using advanced visualization techniques
T2  - ENGINEERING GEOLOGY
AB  - Slope stability monitoring aims to prevent damage caused by slope failure. Although various methods have been proposed to track overall slope behavior, these approaches often overlook embedded reinforcements that play a critical role in maintaining slope stability. Therefore, this study proposes a method to visualize and monitor the status of embedded reinforcements to enhance current slope stability monitoring practices. To intuitively understand the status of the embedded reinforcement, reinforcement information and measured data were integrated into a Building Information Model (BIM) using Dynamo for visualization. In laboratory-scale slope models subjected to 150 mm/h rainfall, embedded strain gauges captured deformation leading to localized shear failure, validating the proposed method. These strain data were automatically imported into BIM, where color-coded elements highlighted 'Safe', 'Warning', and 'Danger'. This proof-of-concept establishes a scalable digital-twin workflow for proactive maintenance and decision-making, enhancing the resilience and sustainability of slope-reinforced infrastructure. Additionally, this approach is expected to enhance the sustainability of various infrastructure structures through disaster prevention, including structural reinforcement and ground reinforcement of roads, bridges, and tunnels over slopes.
SN  - 0013-7952
SN  - 1872-6917
DA  - SEP
PY  - 2025
VL  - 356
C7  - 108266
DO  - 10.1016/j.enggeo.2025.108266
AN  - WOS:001545804000001
ER  -

TY  - CPAPER
AU  - Oh, TH
AU  - Souza, LFS
AU  - Lee, JM
A1  - IEEE
TI  - Applying Digital Application Platform to Optimize Steam Methane Reforming Process
T2  - 2021 21ST INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS 2021)
CP  - 21st International Conference on Control, Automation and Systems (ICCAS)
AB  - The recent advance in measurement equipment, data transmission, and computational power enables digitalizing the process operation and constructing the digital twin. This implies that various existing model-based applications such as process monitoring, forecasting, decision support, and control can be applied to chemical processes with the most updated model in real-time. However, the systematic method of integrating the process data, model-based applications, database, and the user interface has not been well addressed. Therefore, this paper aims to fill this gap by proposing the workflow on applying the integrating framework called gPROMS Digital Application Platform that can receive and validate the process data, execute the various model-based applications, and visualizing the data and process results in a customized web-based user interface. For the case study, the integrating framework is constructed to optimize the SMR process in real-time. The gPROMS Process was used to construct the process model and the optimization problem was solved to find the optimal operating conditions that maximize the thermal efficiency of the reactor. The database stores the model inputs and outputs, and sends the signals to execute the model-based applications. Lastly, the web-based user interface was constructed to offer an environment to monitor the process status and execute the process optimization.
SN  - 2093-7121
SN  - 978-89-93215-21-2
PY  - 2021
SP  - 388
EP  - 393
DO  - 10.23919/ICCAS52745.2021.9650053
AN  - WOS:000750950700050
ER  -

TY  - JOUR
AU  - Mahanty, B
TI  - Hybrid modeling in bioprocess dynamics: Structural variabilities, implementation strategies, and practical challenges
T2  - BIOTECHNOLOGY AND BIOENGINEERING
AB  - Hybrid modeling, with an appropriate blend of the mechanistic and data-driven framework, is increasingly being adopted in bioprocess modeling, model-based experimental design (digital-twin), identification of critical process parameters, and optimization. However, the development of a hybrid model from experimental data is an inherently complex workflow, involving designed experiments, selection of the data-driven process, identification of model parameters, assessment fitness, and generalization capability. Depending on the complexity of the process system and purpose, each piece of these modules can flexibly be incorporated into the puzzle. However, this extra flexibility can be a cause of concern to trace an "optimal" model structure. In this paper, the development of hybrid models in a common bioprocess system, selection of data-driven components and their mapping to states, choice of parameter identification techniques, and model quality assurance are revisited. The challenges associated with hybrid-model development, and corrective actions have also been reviewed. The review also suggests the lack of data, and code sharing in communal repositories can be a hurdle in the exploration, and expansion of those tools in a bioprocess system.
SN  - 0006-3592
SN  - 1097-0290
DA  - AUG
PY  - 2023
VL  - 120
IS  - 8
SP  - 2072
EP  - 2091
DO  - 10.1002/bit.28503
C6  - JUL 2023
AN  - WOS:001026594800001
ER  -

TY  - JOUR
AU  - Si, JL
AU  - Wan, C
AU  - Hou, LW
AU  - Qu, YA
AU  - Lu, YH
AU  - Chen, TY
AU  - Yang, K
TI  - Self-Organizing Optimization of Construction Project Management Based on Building Information Modeling and Digital Technology
T2  - IRANIAN JOURNAL OF SCIENCE AND TECHNOLOGY-TRANSACTIONS OF CIVIL ENGINEERING
AB  - The growth of technology has led to the involvement of machines, sensors, and intelligent systems in the design, construction, and monitoring processes of the construction industry. These capabilities can interact with each other during the building process. The use of these technologies, while increasing speed and accuracy, reduces operational costs. The main goal of this study is application of the self-organizing digitization concept in data mining and intelligent planning in construction project management linked with building information modeling. An intelligent self-organizing data mining system is proposed for optimizing the complexity of the design and construction process. The framework can be addressed as a decision-making system that considers human activities and economic considerations to improve the workflow. To evaluate the model efficiency, digital twin-driven intelligent construction and grounded theory methodology were incorporated. Results showed that the efficiency of the practical phase in building management was improved after application of the self-organizer model and had a direct effect on time prediction plans. Furthermore, the developed autoregressive integrated moving average model can predict construction progress.
SN  - 2228-6160
SN  - 2364-1843
DA  - DEC
PY  - 2023
VL  - 47
IS  - 6
SP  - 4135
EP  - 4143
DO  - 10.1007/s40996-023-01121-x
C6  - MAY 2023
AN  - WOS:001033237900001
ER  -

TY  - CPAPER
AU  - Loizou, S
AU  - Elgammal, A
AU  - Kumara, I
AU  - Christodoulou, P
AU  - Papazoglou, MP
AU  - Andreou, AS
ED  - Filipe, J
ED  - Smialek, M
ED  - Brodsky, A
ED  - Hammoudi, S
TI  - A Smart Product Co-design and Monitoring Framework Via Gamification and Complex Event Processing
T2  - PROCEEDINGS OF THE 21ST INTERNATIONAL CONFERENCE ON ENTERPRISE INFORMATION SYSTEMS (ICEIS 2019), VOL 2
CP  - 21st International Conference on Enterprise Information Systems (ICEIS)
AB  - In the traditional software development cycle, requirements gathering is considered the most critical phase. Getting the requirements right early has become a dogma in software engineering because the correction of erroneous or incomplete requirements in later software development phases becomes overly expensive. For product-service systems (PSS), this dogma and standard requirements engineering (RE) approaches are not appropriate because classical RE is considered concluded once a product service is delivered. This paper proposes a novel framework that enables the customer and the product engineer to co-design smart products by integrating three novel and advanced technologies to support: view-based modelling, visualization and monitoring, i.e., Product-Oriented Configuration Language (PoCL), gamification and Complex Event Processing (CEP), respectively. These create a "digital-twin" model of the connected 'smart' factory of the future. The framework is formally founded on the novel concept of manufacturing blueprints, which are formalized knowledge-intensive structures that provide the basis for actionable PSS and production "intelligence" and a move toward more fact-based manufacturing decisions. Implementation and validation of the proposed framework through real-life case studies are ongoing to validate the applicability, utility and efficacy of the proposed solutions.
SN  - 978-989-758-372-8
PY  - 2019
SP  - 237
EP  - 244
DO  - 10.5220/0007720902370244
AN  - WOS:000570422800022
ER  -

TY  - JOUR
AU  - Assadzadeh, A
AU  - Arashpour, M
AU  - Li, H
AU  - Hosseini, R
AU  - Elghaish, F
AU  - Baduge, S
TI  - Excavator 3D pose estimation using deep learning and hybrid datasets
T2  - ADVANCED ENGINEERING INFORMATICS
AB  - Earthwork operations are crucial parts of most construction projects. Heavy construction equipment and workers are often required to work in limited workspaces simultaneously. Struck-by accidents resulting from poor worker and equipment interactions account for a large proportion of accidents and fatalities on construction sites. The emerging technologies based on computer vision and artificial intelligence offer an opportunity to enhance construction safety through advanced monitoring utilizing site cameras. A crucial pre-requisite to the development of safety monitoring applications is the ability to identify accurately and localize the position of the equipment and its critical components in 3D space. This study proposes a workflow for excavator 3D pose estimation based on deep learning using RGB images. In the proposed workflow, an articulated 3D digital twin of an excavator is used to generate the necessary data for training a 3D pose estimation model. In addition, a method for generating hybrid datasets (simulation and laboratory) for adapting the 3D pose estimation model for various scenarios with different camera parameters is proposed. Evaluations prove the capability of the workflow in estimating the 3D pose of excavators. The study concludes by discussing the limitations and future research opportunities.
SN  - 1474-0346
SN  - 1873-5320
DA  - JAN
PY  - 2023
VL  - 55
C7  - 101875
DO  - 10.1016/j.aei.2023.101875
C6  - JAN 2023
AN  - WOS:000925228600001
ER  -

TY  - JOUR
AU  - Siyaev, A
AU  - Jo, GS
TI  - Towards Aircraft Maintenance Metaverse Using Speech Interactions with Virtual Objects in Mixed Reality
T2  - SENSORS
AB  - Metaverses embedded in our lives create virtual experiences inside of the physical world. Moving towards metaverses in aircraft maintenance, mixed reality (MR) creates enormous opportunities for the interaction with virtual airplanes (digital twin) that deliver a near-real experience, keeping physical distancing during pandemics. 3D twins of modern machines exported to MR can be easily manipulated, shared, and updated, which creates colossal benefits for aviation colleges who still exploit retired models for practicing. Therefore, we propose mixed reality education and training of aircraft maintenance for Boeing 737 in smart glasses, enhanced with a deep learning speech interaction module for trainee engineers to control virtual assets and workflow using speech commands, enabling them to operate with both hands. With the use of the convolutional neural network (CNN) architecture for audio features and learning and classification parts for commands and language identification, the speech module handles intermixed requests in English and Korean languages, giving corresponding feedback. Evaluation with test data showed high accuracy of prediction, having on average 95.7% and 99.6% on the F1-Score metric for command and language prediction, respectively. The proposed speech interaction module in the aircraft maintenance metaverse further improved education and training, giving intuitive and efficient control over the operation, enhancing interaction with virtual objects in mixed reality.
SN  - 1424-8220
DA  - MAR
PY  - 2021
VL  - 21
IS  - 6
C7  - 2066
DO  - 10.3390/s21062066
AN  - WOS:000652711100001
ER  -

TY  - JOUR
AU  - Zabala-Vargas, S
AU  - Jaimes-Quintanilla, M
AU  - Jimenez-Barrera, MH
TI  - Big Data, Data Science, and Artificial Intelligence for Project Management in the Architecture, Engineering, and Construction Industry: A Systematic Review
T2  - BUILDINGS
AB  - The high volume of information produced by project management and its quality have become a challenge for organizations. Due to this, emerging technologies such as big data, data science and artificial intelligence (ETs) have become an alternative in the project life cycle. This article aims to present a systematic review of the literature on the use of these technologies in the architecture, engineering, and construction industry. A methodology of collection, purification, evaluation, bibliometric, and categorical analysis was used. A total of 224 articles were found, which, using the PRISMA method, finally generated 57 articles. The categorical analysis focused on determining the technologies used, the most common methodologies, the most-discussed project management areas, and the contributions to the AEC industry. The review found that there is international leadership by China, the United States, and the United Kingdom. The type of research most used is quantitative. The areas of knowledge where ETs are most used are Cost, Quality, Time, and Scope. Finally, among the most outstanding contributions are as follows: prediction in the development of projects, the identification of critical factors, the detailed identification of risks, the optimization of planning, the automation of tasks, and the increase in efficiency; all of these to facilitate management decision making.
SN  - 2075-5309
DA  - DEC
PY  - 2023
VL  - 13
IS  - 12
C7  - 2944
DO  - 10.3390/buildings13122944
AN  - WOS:001130679900001
ER  -

TY  - CPAPER
AU  - Subramanian, T
AU  - Schlichtherle, F
AU  - Remlinger, W
A1  - IEEE
TI  - Workflow for Evaluating Vehicle Interiors Using Serious Gaming
T2  - 2024 IEEE GAMING, ENTERTAINMENT, AND MEDIA CONFERENCE, GEM 2024
CP  - Gaming, Entertainment and Media Conference (GEM)
AB  - The evaluation of vehicle interiors is necessary for ensuring user comfort, and safety, as well as overall acceptance and satisfaction. Carrying out these evaluations with a user-centric approach helps to understand the requirements from the user's point of view. Implementing the virtual reality (VR) method using real-time rendering to create a Digital Twin (DT) of the vehicle interior and environment provides a multi-dimensional opportunity for simulating and assessing user interactions with the vehicle interior elements and interfaces along with user experience in vehicle interiors. A workflow for creating virtual environments, focusing on scenario-based evaluations in vehicle interiors is presented in this paper. The workflow comprises multiple stages, including the creation of 3D models, integrating 3D assets, defining functionalities using blueprint mode of scripting, creating sequences to display animations, creating triggers for events within scenarios, and activating VR preview mode. The workflow described in this paper can be implemented based on the use cases and aspects that require evaluation.
   A use case of evaluating user acceptance of the interior of a shared automated vehicle (SAV) is considered to explain the workflow, by describing how it was implemented for this particular use case, and by discussing the results of the evaluation.
SN  - 2831-5510
SN  - 979-8-3503-7453-7
SN  - 979-8-3503-7454-4
PY  - 2024
SP  - 100
EP  - 103
DO  - 10.1109/GEM61861.2024.10585510
AN  - WOS:001281983200055
ER  -

TY  - JOUR
AU  - Rampini, L
AU  - Cecconi, FR
TI  - ARTIFICIAL INTELLIGENCE IN CONSTRUCTION ASSET MANAGEMENT: A REVIEW OF PRESENT STATUS, CHALLENGES AND FUTURE OPPORTUNITIES
T2  - JOURNAL OF INFORMATION TECHNOLOGY IN CONSTRUCTION
AB  - The built environment is responsible for roughly 40% of global greenhouse emissions, making the sector a crucial factor for climate change and sustainability. Meanwhile, other sectors (like manufacturing) adopted Artificial Intelligence (AI) to solve complex, non-linear problems to reduce waste, inefficiency, and pollution. Therefore, many research efforts in the Architecture, Engineering, and Construction community have recently tried introducing AI into building asset management (AM) processes. Since AM encompasses a broad set of disciplines, an overview of several AI applications, current research gaps, and trends is needed. In this context, this study conducted the first state-of-the-art research on AI for building asset management. A total of 578 papers were analyzed with bibliometric tools to identify prominent institutions, topics, and journals. The quantitative analysis helped determine the most researched areas of AM and which AI techniques are applied. The areas were furtherly investigated by reading in-depth the 83 most relevant studies selected by screening the articles' abstracts identified in the bibliometric analysis. The results reveal many applications for Energy Management, Condition assessment, Risk management, and Project management areas. Finally, the literature review identified three main trends that can be a reference point for future studies made by practitioners or researchers: Digital Twin, Generative Adversarial Networks (with synthetic images) for data augmentation, and Deep Reinforcement Learning.
SN  - 1874-4753
PY  - 2022
VL  - 27
DO  - 10.36680/j.itcon.2022.043
AN  - WOS:000868340500001
ER  -

TY  - CPAPER
AU  - Doria, E
AU  - Morandotti, M
ED  - Corrao, R
ED  - Campisi, T
ED  - Colajanni, S
ED  - Saeli, M
ED  - Vinci, C
TI  - Information Systems and Automated Census Using Object Detection for the Management and Valuation of Built Cultural Heritage
T2  - PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE OF AR.TEC. (SCIENTIFIC SOCIETY OF ARCHITECTURAL ENGINEERING), VOL 1, COLLOQUI.AT.E 2024
CP  - 11th International Conference of Ar.Tec.
AB  - The contribution concerns the development of methods and protocols for the monitoring of historical built Cultural Heritage with an integrated multidisciplinary approach consisting of documentation activities and analysis procedures. In particular, the latter concern the state of the art, morphology, construction technologies, and the conservation state of materials and construction packages, to structure the corpus of knowledge of the asset under examination. The analysis procedures aim to understand the spatial components of the structure in relation to construction, technological, and material aspects, and to define the digitization actions to be activated on the artifact based on the observed condition. Understanding and monitoring of the cultural asset is achieved through the construction of synthesis frameworks, functional for describing its complexity, which culminate in the use of semi-automated and automated censusing systems of Information Systems for the formulation of monitoring protocols and programmatic frameworks for management. The methodology for automating the monitoring and the architectural census using Object Detection algorithms on georeferenced orthoimages is tested on the follow-up of the AICS co-funded cooperation and research project "Management and control of urban growth for the development of Heritage and improvement of life in the city of Bethlehem" (3D Bethlehem), in which the University of Pavia had scientific responsibility and the research laboratories STEP and PLAY of the Department of Civil Engineering and Architecture were involved.
SN  - 2366-2557
SN  - 2366-2565
SN  - 978-3-031-71857-1
SN  - 978-3-031-71855-7
SN  - 978-3-031-71854-0
PY  - 2025
VL  - 610
SP  - 299
EP  - 314
DO  - 10.1007/978-3-031-71855-7_19
AN  - WOS:001417254100019
ER  -

TY  - JOUR
AU  - Fortunato, T
AU  - Bruno, S
AU  - Fatiguso, F
AU  - De Fino, M
TI  - From GIS to HBIM and Back: Multiscale Performance and Condition Assessment for Networks of Public Heritage Buildings and Construction Components
T2  - INTERNATIONAL JOURNAL OF ARCHITECTURAL HERITAGE
AB  - The paper addresses the integration of Geographic Information Systems (GIS) and Historic Building Information Modeling (HBIM) as a framework for structuring and managing information related to networks of public historical buildings. The approach aims to support administrators and decision-makers in assessing both performance and condition of these assets. To this end, the study proposes a methodological workflow encompassing: (i) the development of an interoperable GIS-BIM database that aggregates heterogeneous data at multiple scales, including district, building, and construction component levels, with the latter enhanced by data derived from experimental measurements of both onsite and laboratory tests; (ii) the creation of tailored data exchange routines facilitating bidirectional transfer between GIS and BIM environments, enabling parametric modeling of building elements based on unified descriptors drawn from predefined glossaries; and (iii) the semantic enrichment of HBIM models, incorporating descriptions of decay patterns and enabling integrated visualization of both "as-built" and "as-damaged" conditions within the GIS platform. The methodology is applied to a real case study focusing on the historical building assets of a municipality in southern Italy, within a specialized context, namely systematizing knowledge to support seismic vulnerability assessment. This case study demonstrates the outcomes and potential applications of the proposed framework, contributing to the debate on its implications for enhancing contemporary heritage evaluation and management practices.
SN  - 1558-3058
SN  - 1558-3066
DA  - 2025 MAR 27
PY  - 2025
DO  - 10.1080/15583058.2025.2482053
C6  - MAR 2025
AN  - WOS:001452403900001
ER  -

TY  - JOUR
AU  - Dias, JA
AU  - Campos, NP
AU  - Boavida, M
TI  - Toward an IoT Framework for the New Generation of XR Applications
T2  - IEEE ACCESS
AB  - In recent years, industry and academia have grown interested in Extended Reality (XR) and the development of creative and immersive applications that take advantage of it. This interest has led to new forms of interaction and immersion being considered for this type of application. However, many of these applications are still tied to traditional input devices, such as a mouse, touchscreen, or game controller, and output to mundane output devices, such as screens or projectors. With the democratization of affordable Internet of Things (IoT) solutions, there is an opportunity to integrate these devices into the next generation of creative applications. However, such integrations may be costly or involve much custom work to develop. This work aims to bridge this gap and create a framework that can be used by developers of such applications to leverage IoT devices for the creation of digital twins or as input and output devices for their applications, opening up software development to a whole new world of possibilities. Throughout this research work, a framework to enable the integration of any IoT device into the next generation of XR applications was proposed, as well as a detailed proposal of a pairing scheme for IoT devices that allows them to be matched with confidence to clients automatically. Performance studies conducted using the industry standard IoT communication protocol MQTT proved the viability of the proposed framework in providing reliable means of low-latency communication for wireless input devices.
SN  - 2169-3536
SN  - 2169-3536
PY  - 2025
VL  - 13
SP  - 68726
EP  - 68752
DO  - 10.1109/access.2025.3560547
DO  - 10.1109/ACCESS.2025.3560547
AN  - WOS:001476512100046
ER  -

TY  - JOUR
AU  - Wang, J
AU  - Ma, YC
AU  - Li, R
AU  - Zhang, SX
TI  - Applications of Building Information Modeling (BIM) and BIM-Related Technologies for Sustainable Risk and Disaster Management in Buildings: A Meta-Analysis (2014-2024)
T2  - BUILDINGS
AB  - Sustainable risk and disaster management in the built environment has become a critical research focus amid escalating environmental challenges. Building Information Modeling (BIM) is recognized as a key digital tool for enhancing disaster resilience through simulation, data integration, and collaborative management. This study systematically reviews BIM applications in sustainable risk and disaster management from 2014 to 2024, employing the PRISMA framework, literature coding, and network analysis. Five primary research clusters are identified: (a) sustainable construction and life cycle assessment, (b) performance evaluation and implementation, (c) technology integration and digital innovation, (d) Historic Building Modeling (HBIM) and post-disaster reconstruction, and (e) project management and technology adoption. Despite increasing scholarly attention, the field remains dominated by conceptual studies, with limited empirical exploration of emerging technologies such as artificial intelligence (AI). Four key challenges are highlighted: weak foundational integration with structural risk research, technological bottlenecks in AI and digital applications, limited practical implementation, and insufficient linkage between sustainability and risk management. Future trends are expected to focus on achieving Industry 4.0 interoperability, advancing AI-driven intelligent disaster response, and adopting multi-objective optimization strategies balancing resilience, sustainability, and cost-effectiveness. This study provides a comprehensive overview of the field's evolution and offers insights into strategic directions for future research and practical innovation.
SN  - 2075-5309
DA  - JUN 29
PY  - 2025
VL  - 15
IS  - 13
C7  - 2289
DO  - 10.3390/buildings15132289
AN  - WOS:001526454000001
ER  -

TY  - JOUR
AU  - Daoud, MA
AU  - Haouari, KB
AU  - Mechkouri, MH
AU  - Ennawaoui, A
AU  - El Hadraoui, H
AU  - Naser, I
AU  - Ouardouz, M
AU  - Reklaoui, K
TI  - Optimizing additive manufacturing workflows using model-based systems engineering
T2  - RESULTS IN ENGINEERING
AB  - Additive Manufacturing (AM) is a transformative technology enabling the production of complex geometries with reduced waste and lead times. However, optimizing AM workflows involves addressing challenges such as design validation, process parameters, and compliance with industry standards. In this paper, a Model-Based Systems Engineering (MBSE) approach is proposed to support the conceptual design of the Advisor System for Additive Manufacturing (ASAM), a framework aimed at streamlining AM workflows. The ASAM framework utilizes the CESAM architecture and System Modelling Language (SysML) to create multi-architecture models of the AM process, integrating operational, functional, and constructional perspectives. These models formalize the system architecture, providing a foundation for developing automated design validation, flaw detection, and optimization mechanisms. Future iterations of ASAM will incorporate advanced algorithms for real-time parameter optimization and predictive analytics. To demonstrate the feasibility of the proposed approach, the initial phase focuses on conceptual modelling and scalability assessment. Illustrative scenarios across aerospace, healthcare, and education industries are used to evaluate the adaptability and potential impact of the framework. The results highlight ASAM's capability to formalize AM workflow architectures and lay the groundwork for improved efficiency, compliance, and scalability in future implementations.
SN  - 2590-1230
DA  - SEP
PY  - 2025
VL  - 27
C7  - 105926
DO  - 10.1016/j.rineng.2025.105926
AN  - WOS:001525888600001
ER  -

TY  - JOUR
AU  - Ali, KN
AU  - Alhajlah, HH
AU  - Kassem, MA
TI  - Collaboration and Risk in Building Information Modelling (BIM): A Systematic Literature Review
T2  - BUILDINGS
AB  - Building information modelling (BIM) has become increasingly popular in construction projects in recent years. Simultaneously, project management has received more attention from academics and practitioners worldwide. Many studies have suggested that perceiving collaboration and risk are critical for successful construction project management. This study investigates the current status and future trends in building information modeling (BIM) literature from the Web of Science database. This review systematically uses bibliometric and systematic literature review (SLR) methods through co-occurrence and co-citation analysis. First, 650 academic documents were retrieved from the Web of Science database. Then, co-occurrence and co-citation analyses were performed along with network visualization to examine research interconnections' patterns. As a result, relevant keywords, productive authors, and important journals have been highlighted. The prominent research topics within the literature on building information modelling focus on the following topics: collaborative in BIM, integration of BIM, GIS and Internet of Things (IoT), barriers to the integration of BIM, sustainability and BIM, and risk assessment and uncertainty. Finally, the potential research directions are developing towards digital twin technology, integration of BIM and AI, and Augmented Reality (AR) and BIM. The presented findings of only 88 articles discuss the collaboration and risk issue in BIM for the construction industry and thus confirms the need for more studies on this topic to enhance the chances of successfully building information modelling projects. The review focuses only on the academic documents retrieved from the Web of Science database, thus restricting the coverage of the reviewed literature relating to building information modelling collaboration and risk.
SN  - 2075-5309
DA  - MAY
PY  - 2022
VL  - 12
IS  - 5
C7  - 571
DO  - 10.3390/buildings12050571
AN  - WOS:000801733900001
ER  -

TY  - CPAPER
AU  - Thakur, AM
AU  - Hitefield, S
AU  - McDonnell, M
AU  - Wolf, M
AU  - Archibald, R
AU  - Drane, L
AU  - Roccapriore, K
AU  - Ziatdinov, M
AU  - McGaha, J
AU  - Smith, R
AU  - Hetrick, J
AU  - Abraham, M
AU  - Yakubov, S
AU  - Watson, G
AU  - Chance, B
AU  - Nguyen, C
AU  - Baker, M
AU  - Michael, R
AU  - Arenholz, E
AU  - Mintz, B
ED  - Doug, K
ED  - Al, G
ED  - Pophale, S
ED  - Liu, H
ED  - Parete-Koon , S
TI  - Towards a Software Development Framework for Interconnected Science Ecosystems
T2  - ACCELERATING SCIENCE AND ENGINEERING DISCOVERIES THROUGH INTEGRATED RESEARCH INFRASTRUCTURE FOR EXPERIMENT, BIG DATA, MODELING AND SIMULATION, SMC 202
CP  - 22nd Smoky Mountains Computational Sciences and Engineering Conference (SMC)
AB  - The innovative science of the future must be multi-domain and interconnected to usher in the next generation of "self-driving" laboratories enabling consequential discoveries and transformative inventions. Such a disparate and interconnected ecosystem of scientific instruments will need to evolve using a system-of-systems (SoS) approach. The key to enabling application integration with such an SoS will be the use of Software Development Kits (SDKs). Currently, SDKs facilitate scientific research breakthroughs via algorithmic automation, databases and storage, optimization and structure, pervasive environmental monitoring, among others. However, existing SDKs lack instrument-interoperability and reusability capabilities, do not effectively work in an open federated architectural environment, and are largely isolated within silos of the respective scientific disciplines. Inspired by the scalable SoS framework, this work proposes the development of INTERSECT-SDK to provide a coherent environment for multi-domain scientific applications to benefit from the open federated architecture in an interconnected ecosystem of instruments. This approach will decompose functionality into loosely coupled software services for interoperability among several solutions that do not scale beyond a single domain and/or application. Furthermore, the proposed environment will allow operational and managerial inter-dependence while providing opportunities for the researchers to reuse software components from other domains and build universal solution libraries. We demonstrate this research for microscopy use-case, where we show how INTERSECT-SDK is developing the tools necessary to enable advanced scanning methods and accelerate scientific discovery.
SN  - 1865-0929
SN  - 1865-0937
SN  - 978-3-031-23605-1
SN  - 978-3-031-23606-8
PY  - 2022
VL  - 1690
SP  - 206
EP  - 224
DO  - 10.1007/978-3-031-23606-8_13
AN  - WOS:000972629000013
ER  -

TY  - CPAPER
AU  - Gaspari, F
AU  - Ioli, F
AU  - Barbieri, F
AU  - Belcore, E
AU  - Pinto, L
ED  - Yilmaz, A
ED  - Wegner, JD
ED  - Qin, R
ED  - Remondino, F
ED  - Fuse, T
ED  - Toschi, I
TI  - INTEGRATION OF UAV-LIDAR AND UAV-PHOTOGRAMMETRY FOR INFRASTRUCTURE MONITORING AND BRIDGE ASSESSMENT
T2  - XXIV ISPRS CONGRESS IMAGING TODAY, FORESEEING TOMORROW, COMMISSION II
CP  - 24th ISPRS Congress on Imaging Today, Foreseeing Tomorrow
AB  - The health assessment of strategic infrastructures and bridges represents a critical variable for planning appropriate maintenance operations. The high costs and complexity of traditional periodical monitoring with elevating platforms have driven the search for more efficient and flexible methods. Indeed, recent years have seen the growing diffusion and adoption of non-invasive approaches consisting in the use of Unmanned Aerial Vehicles (UAVs) for applications that range from visual inspection with optical sensors to LiDAR technologies for rapid mapping of the territory. This study defines two different methodologies for bridge inspection. A first approach involving the integration of traditional topographic and GNSS techniques with TLS and photogrammetry with cameras mounted on UAV was compared with a UAV-LiDAR method based on the use of a DJI Matrice 300 equipped with a LiDAR DJI Zenmuse L 1 sensor for a manual flight and an automatic one. While the first workflow resulted in a centimetric accurate but time-consuming model, the UAV-LiDAR resulting point cloud's georeferencing accuracy resulted to be less accurate in the case of the manual flight under the bridge for GNSS signal obstruction. However, a photogrammetric model reconstruction phase made with Ground Control Points and photos taken by the L1-embedded camera improved the overall accuracy of the workflow, that could be employed for flexible low-cost mapping of bridges when medium level accuracy (5-10 cm) is accepted. In conclusion, a solution for integrating interactively final 3D products in a Bridge Management System environment is presented.
SN  - 1682-1750
SN  - 2194-9034
PY  - 2022
VL  - 43-B2
SP  - 995
EP  - 1002
DO  - 10.5194/isprs-archives-XLIII-B2-2022-995-2022
AN  - WOS:000855635300134
ER  -

TY  - JOUR
AU  - Sadeghnejad, S
AU  - Reinhardt, M
AU  - Enzmann, F
AU  - Arnold, P
AU  - Brandsta, B
AU  - Ott, H
AU  - Wilde, F
AU  - Hupfer, S
AU  - Schäfer, T
AU  - Kersten, M
TI  - Minkowski functional evaluation of representative elementary volume of rock microtomography images at multiple resolutions
T2  - ADVANCES IN WATER RESOURCES
AB  - Pore-scale properties can be obtained by building a reliable digital twin of porous media through the digital rock physics (DRP) workflow. The two prerequisites of DRP are reliable imaging and computing power. Determining a proper image resolution that can reveal the actual pore-scale properties is challenging as there is a trade-off between the resolution and the representative elementary volume (REV). The REV is the smallest volume that reproduces the properties of the whole porous medium. The REV is a function of heterogeneities on the pore scale, the parameter of interest, and the scale range. Although the REV analysis for hydraulic properties is straightforward, it is computationally expensive. This study aims to estimate hydraulic pore-scale properties during REV evaluations by the geometric characterization of porous media using the Minkowski morphological functionals. Two sandstone and one carbonate rock samples were scanned at multiple imaging resolutions by both laboratory and synchrotron tomography. The REVs of various parameters of interest (porosity, permeability, surface area, tortuosity, Minkowski functionals) were computed, and the effect of image resolution and artificial rebinning on the final REV values was examined. After reaching the REV for porosity, the REV for the integrals of mean and total curvature agreed well with the permeability REV for large-volume image sizes. At constant porosity, the Minkowski integrals were found to be indicators for pore throat sizes. We also showed that the properties obtained from the rebinned (or coarsened) images differ entirely from that of actual scans at the same resolution.
SN  - 0309-1708
SN  - 1872-9657
DA  - SEP
PY  - 2023
VL  - 179
C7  - 104501
DO  - 10.1016/j.advwatres.2023.104501
C6  - AUG 2023
AN  - WOS:001052024900001
ER  -

TY  - JOUR
AU  - Chaves, E
AU  - Aguilar, J
AU  - Barontini, A
AU  - Mendes, N
AU  - Compán, V
TI  - Digital Tools for the Preventive Conservation of Built Heritage: The Church of Santa Ana in Seville
T2  - HERITAGE
AB  - Historic Building Information Modelling (HBIM) plays a pivotal role in heritage conservation endeavours, offering a robust framework for digitally documenting existing structures and supporting conservation practices. However, HBIM's efficacy hinges upon the implementation of case-specific approaches to address the requirements and resources of each individual asset and context. This paper defines a flexible and generalisable workflow that encompasses various aspects (i.e., documentation, surveying, vulnerability assessment) to support risk-informed decision making in heritage management tailored to the peculiar conservation needs of the structure. This methodology includes an initial investigation covering historical data collection, metric and condition surveys and non-destructive testing. The second stage includes Finite Element Method (FEM) modelling and structural analysis. All data generated and processed are managed in a multi-purpose HBIM model. The methodology is tested on a relevant case study, namely, the church of Santa Ana in Seville, chosen for its historical significance, intricacy and susceptibility to seismic action. The defined level of detail of the HBIM model is sufficient to inform the structural analysis, being balanced by a more accurate representation of the alterations, through linked orthophotos and a comprehensive list of alphanumerical parameters. This ensures an adequate level of information, optimising the trade-off between model complexity, investigation time requirements, computational burden and reliability in the decision-making process. Field testing and FEM analysis provide valuable insight into the main sources of vulnerability in the building, including the connection between the tower and nave and the slenderness of the columns.
SN  - 2571-9408
DA  - JUL
PY  - 2024
VL  - 7
IS  - 7
SP  - 3470
EP  - 3494
DO  - 10.3390/heritage7070164
AN  - WOS:001277325700001
ER  -

TY  - CPAPER
AU  - Bianconi, F
AU  - Filippucci, M
AU  - Cornacchini, F
AU  - Migliosi, A
ED  - Bayram, B
ED  - Isikdag, U
TI  - THE IMPACT OF GOOGLE'S APIS ON LANDSCAPE VIRTUAL REPRESENTATION
T2  - 8TH INTERNATIONAL CONFERENCE ON GEOINFORMATION ADVANCES, GEOADVANCES 2024, VOL. 48-4
CP  - 8th International Conference on GeoInformation Advances (GeoAdvances)
AB  - The field of territorial representation has undergone significant transformations in response to the proliferation of the internet, leading to the emergence of platforms dedicated to global exploration. Notably, Google Earth has assumed a pivotal role and stands as one of the most widely utilized tools for making territorial information universally accessible. Google recently introduced direct access to photorealistic 3DTiles via dedicated APIs, ushering in a new era of possibilities. This integration forms a robust foundation for crafting customized applications and interactive experiences within a geospatial three-dimensional environment. The primary objective of this research is the assessment of the accuracy and potential of the resources provided by Google within a workflow focused on digital twin processing and geospatial data visualization. To achieve this goal, a comparative analysis of distinct models was conducted, with each model representing a unique approach to three-dimensional reconstruction. The research introduces a methodology designed for easy replication in other case studies, demonstrating intrinsic scalability suitable for more complex or diverse scenarios. Furthermore, the study offers a comprehensive assessment of the differences and characteristics of the three methods analyzed, providing insights into their potential and limitations.
SN  - 1682-1750
SN  - 2194-9034
PY  - 2024
SP  - 91
EP  - 98
DO  - 10.5194/isprs-archives-XLVIII-4-W9-2024-91-2024
AN  - WOS:001234953400013
ER  -

TY  - JOUR
AU  - Gimpel, AL
AU  - Stark, WJ
AU  - Heckel, R
AU  - Grass, RN
TI  - Challenges for error-correction coding in DNA data storage: photolithographic synthesis and DNA decay
T2  - DIGITAL DISCOVERY
AB  - Efficient error-correction codes are crucial for realizing DNA's potential as a long-lasting, high-density storage medium for digital data. At the same time, new workflows promising low-cost, resilient DNA data storage are challenging their design and error-correcting capabilities. This study characterizes the errors and biases in two new additions to the state-of-the-art workflow in DNA data storage: photolithographic synthesis and DNA decay. Photolithographic synthesis offers low-cost, scalable oligonucleotide synthesis but suffers from high error rates, necessitating sophisticated error-correction schemes, for example codes introducing within-sequence redundancy combined with clustering and alignment techniques for retrieval. On the other hand, the decoding of oligo fragments after DNA decay promises unprecedented storage densities, but complicates data recovery by requiring the reassembly of full-length sequences or the use of partial sequences for decoding. Our analysis provides a detailed account of the error patterns and biases present in photolithographic synthesis and DNA decay, and identifies considerable bias stemming from sequencing workflows. We implement our findings into a digital twin of the two workflows, offering a tool for developing error-correction codes and providing benchmarks for the evaluation of codec performance.
SN  - 2635-098X
DA  - DEC 4
PY  - 2024
VL  - 3
IS  - 12
SP  - 2497
EP  - 2508
DO  - 10.1039/d4dd00220b
C6  - OCT 2024
AN  - WOS:001339544000001
ER  -

TY  - JOUR
AU  - Milosheski, L
AU  - Mohorcic, M
AU  - Fortuna, C
TI  - Spectrum Sensing With Deep Clustering: Label-Free Radio Access Technology Recognition
T2  - IEEE OPEN JOURNAL OF THE COMMUNICATIONS SOCIETY
AB  - The growth of the number of connected devices and network densification is driving an increasing demand for radio network resources, particularly Radio Frequency (RF) spectrum. Given the dynamic and complex nature of contemporary wireless environments, characterized by a wide variety of devices and multiple RATs, spectrum sensing is envisioned to become a building component of future 6G, including as a components within O-RAN or digital twins. However, the current SotA research for RAT classification predominantly revolves around supervised Convolutional Neural Network (CNN)- based approach that require extensive labeled dataset. Due to this, it is unclear how existing models behave in environments for which training data is unavailable thus leaving open questions regarding their generalization capabilities. In this paper, we propose a new spectrum sensing workflow in which the model training does not require any prior knowledge of the RATs transmitting in that area (i.e., no labelled data) and the class assignment can be easily done through manual mapping. Furthermore, we adaptat a SSL deep clustering architecture capable of autonomously extracting spectrum features from raw 1D Fast Fourier Transform (FFT) data. We evaluate the proposed architecture on three real-world datasets from three European cities, in the 868 MHz, 2.4 GHz and 5.9 GHz bands containing over 10 RATs and show that the developed model achieves superior performance by up to 35 percentage points with 22% fewer trainable parameters and 50% less floating-point operations per second (FLOPS) compared to an SotA AE-based reference architecture.
SN  - 2644-125X
PY  - 2024
VL  - 5
SP  - 4746
EP  - 4763
DO  - 10.1109/OJCOMS.2024.3436601
AN  - WOS:001288233100001
ER  -

TY  - JOUR
AU  - Xiahou, XE
AU  - Chen, GT
AU  - Li, ZR
AU  - Xu, X
AU  - Li, QM
TI  - Knowledge Management in Construction Quality Management: Current State, Challenges, and Future Directions
T2  - IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT
AB  - Construction quality management (CQM), as one of the major activities in construction project management, relies heavily on knowledge. Unfortunately, the knowledge of CQM is diverse in format and scattered in different stakeholders within the whole construction processes. Therefore, knowledge management (KM) of CQM is underinvestigated. To offering a comprehensive view of KM in CQM, this article employed a mixed review method to critically review 87 related articles. The results indicate 1) building information modeling, ontology, and natural language processing are identified as critical technologies in KM, 2) expert system and decision support, structural health monitoring, and project management are the major application domains. This article conducts an in-depth analysis of the literature based on the three phases of quality control: pre-construction, in-construction, and post-construction. The results are discussed to critically assess the critical technologies in KM. A framework is proposed to guide the effective implementation of KM in CQM, alongside a discussion of the current challenges and opportunities. The article further identifies potential development directions for KM in CQM, including total quality management, digital twins, development of large language models, construction of "No-cost" KM platforms, uniform evaluation and standardization mechanisms, tacit knowledge capture, and confidentiality and security. A novel paradigm for knowledge-driven quality management decision-making is first introduced. This article offers a comprehensive perspective on the application of KM in CQM, which will significantly enhance the effectiveness of CQM implementation in the future.
SN  - 0018-9391
SN  - 1558-0040
PY  - 2025
VL  - 72
SP  - 1069
EP  - 1088
DO  - 10.1109/TEM.2025.3550354
AN  - WOS:001457722000004
ER  -

TY  - JOUR
AU  - Haerter, J
AU  - Veyskarami, M
AU  - Schneider, M
AU  - Mueller, JC
AU  - Wu, HC
AU  - Helmig, R
AU  - Weigand, B
AU  - Lamanna, G
AU  - Poser, R
TI  - Self-Pumping Transpiration Cooling: A Joint Experimental and Numerical Study
T2  - TRANSPORT IN POROUS MEDIA
AB  - A joint experimental and numerical study is presented to close the current gap in fully coupled data and modeling capabilities for self-pumping transpiration cooling (SPTC). An experimental setup was developed to investigate the effects of the porous medium properties, the flow conditions, and the interactions between solid and coolant on SPTC. Additionally, a two-reference-point, locally emissivity-corrected evaluation methodology for analyzing infrared (IR) measurements was developed, which is valid for quasi-steady evaporation regimes and achieves a better repeatability. For the numerical simulations, we developed an upscaling workflow with pore-network models derived from micro computed tomography (CT) data to accurately describe effective representative elementary volume (REV)-scale parameters and relations. Using upscaled properties, we created a non-isothermal, two-phase Darcy-scale model for the porous medium and modeled free-flow with Reynolds-averaged Navier-Stokes equations, employing an shear stress transport (SST) k-omega\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$k\text {-}\omega$$\end{document} turbulence closure to capture near-wall shear stress effects. Coupling conditions ensured mass, momentum, and energy transfer at the interface. The experimental results show a high reproducibility and new insights for the surface temperature at SPTC with the new IR method. The comparison between experimental and numerical results show good agreements. The developed simulation workflow is a major step toward creating a digital twin of an experimental SPTC system. This work lays the foundation for investigating the influence of parameters on SPTC systems and optimizing their efficiency.
SN  - 0169-3913
SN  - 1573-1634
DA  - JUL 2
PY  - 2025
VL  - 152
IS  - 8
C7  - 56
DO  - 10.1007/s11242-025-02198-w
AN  - WOS:001536525700002
ER  -

TY  - JOUR
AU  - Gu, DL
AU  - Yue, QR
AU  - Li, L
AU  - Sun, CJ
AU  - Lu, XZ
TI  - Vision-Based Digital Shadowing to Reveal Hidden Structural Dynamics of a Real Supertall Building
T2  - ENGINEERING
AB  - Vision-based digital shadowing is a highly efficient way to monitor the health of buildings in use. However, previous studies on digital shadowing have been limited to laboratory experiments. This paper proposes a novel computer-vision-based digital shadow workflow and presents its successful application in a real engineering case. In this case, a 345.8-m supertall building experienced unexpected shaking under normal meteorological conditions. This study established a digital shadow of the building using three-dimensional displacement measurements based on super-resolution monocular vision, revealing the hidden structural dynamics and inherent mechanical reasons for the abnormal shaking. The proposed digital shadowing workflow is a feasible roadmap for developing vision-based digital shadows of real- world structures using low-cost cameras. The abnormal vibration event in the supertall building considered in this study is the first of its type worldwide. The results of this study offer practical strategies and invaluable insights into the prevention and mitigation of this type of global risk, thereby contributing to the lifespan extension of buildings in use worldwide. Furthermore, with the increasing number of general sensing devices, such as surveillance cameras in cities, the proposed method may unleash the immense potential of general sensing devices in achieving the leap from structural health monitoring to city health monitoring. (c) 2024 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
SN  - 2095-8099
SN  - 2096-0026
SN  - 2096-0026
DA  - DEC
PY  - 2024
VL  - 43
SP  - 146
EP  - 158
C7  - 2095-8099(2024)43<146:VBDSTR>2.0.TX;2-3
DO  - 10.1016/j.eng.2024.10.002
C6  - DEC 2024
AN  - CCC:001402459300001
ER  -

TY  - JOUR
AU  - Kaewunruen, S
AU  - Sresakoolchai, J
AU  - Zhou, ZH
TI  - Sustainability-Based Lifecycle Management for Bridge Infrastructure Using 6D BIM
T2  - SUSTAINABILITY
AB  - A number of bridge infrastructures are rising significantly due to economic expansion and growing numbers of railway and road infrastructures. Owing to the complexity of bridge design, traditional design methods always create tedious and time-consuming construction processes. In recent years, Building Information Modelling (BIM) has been developed rapidly to provide a faster solution to generate and process the integration of information in a shared environment. This paper aims to highlight an innovative 6D BIM approach for the lifecycle asset management of a bridge infrastructure by using Donggou Bridge as a case study. This paper adopts 6D modelling, incorporating 3D model information with time schedule, cost estimation, and carbon footprint analysis across the lifecycle of the bridge project. The results of this paper reveal that raw materials contribute the most embodied carbon emissions, and as the 6D BIM model was developed in the early stage of the lifecycle, stakeholders can collaborate within the BIM environment to enhance a more sustainable and cost-effective outcome in advance. This study also demonstrates the possibility of BIM applications to bridge infrastructure projects throughout the whole lifecycle. The 6D BIM can save time by transforming 2D information to 3D information and reducing errors during the pre-construction and construction stages through better visualisation for staff training. Moreover, 6D BIM can promote efficient asset and project management since it can be applied for various purposes simultaneously, such as sustainability, lifecycle asset management and maintenance, condition monitoring and real-time structural simulations. In addition, BIM can promote cooperation among working parties and improve visualisation of the project for various stakeholders.
SN  - 2071-1050
DA  - MAR 2
PY  - 2020
VL  - 12
IS  - 6
C7  - 2436
DO  - 10.3390/su12062436
AN  - WOS:000523751400283
ER  -

TY  - JOUR
AU  - Hu, ZQ
AU  - Brilakis, I
TI  - Matching design-intent planar, curved, and linear structural instances in point clouds
T2  - AUTOMATION IN CONSTRUCTION
AB  - The lack of timely progress monitoring and quality control contributes to cost-escalation, lowering of productivity, and broadly poor project performance. This paper addressed the challenge of high-precision structural instance segmentation from point clouds by leveraging as-designed IFC models in Scan-vs-BIM contexts. We proposed an automatic method to segment the entire points corresponding to the as-designed instance. The workflow contains: 1) Instance descriptor generation; 2) PROSAC-based shape detection; 3) DBSCAN-based cluster optimization. The method matches design-intent planar, curved, and linear structural instances in complex scenarios including: 1) the as-built point cloud is noisy with high occlusions and clutter; 2) deviations between as-built instances and as-designed models in terms of position, orientation, and scale; 3) both Manhattan-World and non-Manhattan-World instances. The experimental results from five diverse real-world datasets showed excellent performance with mPrecision 0.962, mRecall 0.934, and mIoU 0.914. Benchmarking against state-of-the-art methods showed that the proposed method outperforms all existing ones.
SN  - 0926-5805
SN  - 1872-7891
DA  - FEB
PY  - 2024
VL  - 158
C7  - 105219
DO  - 10.1016/j.autcon.2023.105219
C6  - DEC 2023
AN  - WOS:001140177400001
ER  -

TY  - JOUR
AU  - Karamanlioglu, A
AU  - Demirel, B
AU  - Tural, O
AU  - Dogan, OT
AU  - Alpaslan, FN
TI  - Privacy-Preserving Clinical Decision Support for Emergency Triage Using LLMs: System Architecture and Real-World Evaluation
T2  - APPLIED SCIENCES-BASEL
AB  - This study presents a next-generation clinical decision-support architecture for Clinical Decision Support Systems (CDSS) focused on emergency triage. By integrating Large Language Models (LLMs), Federated Learning (FL), and low-latency streaming analytics within a modular, privacy-preserving framework, the system addresses key deployment challenges in high-stakes clinical settings. Unlike traditional models, the architecture processes both structured (vitals, labs) and unstructured (clinical notes) data to enable context-aware reasoning with clinically acceptable latency at the point of care. It leverages big data infrastructure for large-scale EHR management and incorporates digital twin concepts for live patient monitoring. Federated training allows institutions to collaboratively improve models without sharing raw data, ensuring compliance with GDPR/HIPAA, and FAIR principles. Privacy is further protected through differential privacy, secure aggregation, and inference isolation. We evaluate the system through two studies: (1) a benchmark of 750+ USMLE-style questions validating the medical reasoning of fine-tuned LLMs; and (2) a real-world case study (n = 132, 75.8% first-pass agreement) using de-identified MIMIC-III data to assess triage accuracy and responsiveness. The system demonstrated clinically acceptable latency and promising alignment with expert judgment on reviewed cases. The infectious disease triage case demonstrates low-latency recognition of sepsis-like presentations in the ED. This work offers a scalable, audit-compliant, and clinician-validated blueprint for CDSS, enabling low-latency triage and extensibility across specialties.
SN  - 2076-3417
DA  - JUL 29
PY  - 2025
VL  - 15
IS  - 15
C7  - 8412
DO  - 10.3390/app15158412
AN  - WOS:001548986400001
ER  -

TY  - JOUR
AU  - Chioni, C
AU  - Maragno, A
AU  - Pianegonda, A
AU  - Ciolli, M
AU  - Favargiotti, S
AU  - Massari, GA
TI  - Low-Cost 3D Virtual and Dynamic Reconstruction Approach for Urban Forests: The Mesiano University Park
T2  - SUSTAINABILITY
AB  - Urban forests, parks, and gardens are fundamental components of urban sustainability, resilience, and regenerative dynamics. Designers, architects, and landscape architects could smartly manage these dynamic ecosystems if efficiently provided with design-oriented digital tools, technologies, and techniques. However, practitioners lack knowledge and standardized procedures for their uses. The rise of low-cost sensors to generate 3D data (e.g., point clouds) in forestry can also effectively support monitoring, analysis, and visualization purposes for greenery in urban contexts. Adopting an interdisciplinary approach-involving the fields of forestry, geomatics, and computer science-this contribution addresses these issues and proposes a low-cost workflow for 3D virtual reconstructions of urban forests to support information management activities and thus landscape architecture applications. By connecting a wide range of methods (i.e., spherical photogrammetry, point cloud modeling), tools (i.e., 360 degrees camera, tablet with lidar sensor), and software (i.e., Agisoft Metashape, CloudCompare, Autodesk AutoCAD), the proposed workflow is defined and tested in the development of dynamic virtual representations for a plot of the Mesiano University park in Trento (Italy). Finally, comparing acquisition, processing, and elaboration methodologies and their results, the possibility of developing digital twins of urban forests is envisioned.
SN  - 2071-1050
DA  - OCT
PY  - 2023
VL  - 15
IS  - 19
C7  - 14072
DO  - 10.3390/su151914072
AN  - WOS:001145636400001
ER  -

TY  - JOUR
AU  - Yang, YQ
AU  - Yang, ML
AU  - Shangguan, SW
AU  - Cao, YF
AU  - Jiang, PY
TI  - A novel method to build knowledge graph models for the configuration and operation design of smart and connected industrial products
T2  - JOURNAL OF COMPUTATIONAL DESIGN AND ENGINEERING
AB  - Smart and connected industrial products (SCIPs), characterized by their capabilities of self-monitoring, environment awareness, machine-machine/machine-human communication and collaboration, intelligent decision-making, etc., have become the fundamental elements for cyber-physical systems, digital twin, industrial internet of things, etc. Configuring the components in SCIPs and modeling their interaction and operation mechanisms are important during SCIPs design. However, existing product design methods were originally developed for none smart and connected products. This could limit the accuracy of SCIP modeling during the design stage and consequently, it may cause more reworks during the implementation stage of the designed SCIPs. In this regard, a SCIP configuration and operation design method is established, including (i) meta knowledge graph (KG)-based configuration of the components in the physical system and status monitoring system of a required SCIP, (ii) event-state swimlane flowchart-based analysis of the dynamic interaction, operation, and data monitoring mechanisms among the components, and (iii) event-state KG based modeling of the overall workflow, monitoring data self-updating and intelligent operation mechanisms of the SCIP. Compared with existing SCIP design methods, the work provides a specific method for not only the configuration of the static components in customized SCIPs, but also the dynamic interaction, data acquisition/storing/transmitting, and intelligent function implementation mechanisms of the configured SCIP using a kind of event-state KG. The event-state KG is both human-readable and computer-programmable, and it can self-update according to predefined reasoning algorithms during the operation of the SCIP. The configuration and operation design modeling of a robot-based grinding processing line is used as a case study.
   Graphical Abstract
SN  - 2288-5048
DA  - MAR 6
PY  - 2024
VL  - 11
IS  - 2
SP  - 327
EP  - 344
DO  - 10.1093/jcde/qwae033
C6  - APR 2024
AN  - WOS:001206250100001
ER  -

TY  - JOUR
AU  - Giannopoulos, AE
AU  - Spantideas, ST
AU  - Levis, G
AU  - Kalafatelis, AS
AU  - Trakadas, P
TI  - COMIX: Generalized Conflict Management in O-RAN xApps-Architecture, Workflow, and a Power Control Case
T2  - IEEE ACCESS
AB  - Open Radio Access Network (O-RAN) is transforming the telecommunications landscape by enabling flexible, intelligent, and multi-vendor networks. Central to its architecture are xApps hosted on the Near-Real-Time RAN Intelligent Controller (Near-RT RIC), which optimize network functions in real time. However, the concurrent operation of multiple xApps with conflicting objectives can lead to suboptimal performance. This paper introduces a generalized Conflict Management scheme for Multi-Channel Power Control in O-RAN xApps (COMIX), designed to detect and resolve conflicts between xApps. To demonstrate COMIX, we focus on two Deep Reinforcement Learning (DRL)-based xApps for power control: one maximizes the data rare across UEs, and the other optimizes system-level energy efficiency. COMIX employs a standardized Conflict Mitigation Framework (CMF) for conflict detection and resolution and leverages the Network Digital Twin (NDT) to evaluate the impact of conflicting actions before applying them to the live network. We validate the framework using a realistic multi-channel power control scenario under various conflict resolution policies, demonstrating its effectiveness in balancing antagonistic objectives. Evaluation results show that COMIX achieves up to 60% energy savings across different Service-Level Agreement (SLA) policies compared to a baseline conflict-unaware system, with negligible impact (around 3%) on system throughput. While this study considers power control xApps, the COMIX framework is generalizable and can be applied to any xApp conflict scenario involving resource contention or KPI interdependence.
SN  - 2169-3536
PY  - 2025
VL  - 13
SP  - 116684
EP  - 116700
DO  - 10.1109/ACCESS.2025.3585774
AN  - WOS:001527231900037
ER  -

TY  - JOUR
AU  - Meyendorf, N
AU  - Ida, N
AU  - Singh, R
AU  - Vrana, J
TI  - NDE 4.0: Progress, promise, and its role to industry 4.0
T2  - NDT & E INTERNATIONAL
AB  - "Industry 4.0" stands for the fourth industrial revolution, the transition from production by computer controlled isolated machines to the concept of a smart factory, where machines, materials, and personnel are digitally connected, to actively adapt to changes in workflow. It emerges from the confluence of operational tech-nologies (OT) and information technologies (IT), which allows for higher flexibility and manufacturing of unique custom products to suit individual customer requirements. To assure quality, embracing digital transformation of non-destructive evaluation is essential. It can be an integral part of the cyber-controlled production and asset life-cycle maintenance. These two value propositions covering the asset life cycle, require digitally controlled NDE procedures and qualitative data to support automated decision making, for most known situations. With that scenario, NDE must be considered in conjunction with other engineering disciplines. Reliability assessments become a necessary prerequisite for the use of NDE data. In addition, the approach to inspector certification and experience needs to be revised. The digital technologies that are being deployed to enhance production under industry 4.0 can very well be the enablers of digitalized NDE and their integration with the asset and its owner. Model-based definition, smart robots, artificial intelligence, augmented reality, and digital twins can all be used to enhance NDE inspections to a new level of performance in quality and safety assurance. These new NDE trends are summarized under the term "NDE 4.0" to meet the needs of Industry 4.0. The present review paper is aimed at capturing recent advances in digital technologies for non-destructive inspections, examples of their use, and other aspects that must be addressed to shift the paradigm. The paper also highlights the global collaboration and current trends, with an intent to inspire industry professionals to engage with NDE 4.0 - a must for Industry 4.0.
SN  - 0963-8695
SN  - 1879-1174
DA  - DEC
PY  - 2023
VL  - 140
C7  - 102957
DO  - 10.1016/j.ndteint.2023.102957
C6  - OCT 2023
AN  - WOS:001098349700001
ER  -

TY  - JOUR
AU  - Yang, C
AU  - Lin, JR
AU  - Yan, KX
AU  - Deng, YC
AU  - Hu, ZZ
AU  - Liu, C
TI  - Data-Driven Quantitative Performance Evaluation of Construction Supervisors
T2  - BUILDINGS
AB  - The performances of construction supervisors are essential for the monitoring, control, and coordination of the construction process of a project in order to adhere to a predefined schedule, cost, quality and other factors. However, it is challenging to evaluate their performance due to limitations such as data deficiency, human error, etc. Thus, this paper proposes an approach to data-driven quantitative performance evaluation of construction supervisors by integrating an analytic hierarchy process (AHP) and activity tracking. The proposed approach contains three parts, namely, index extraction, weighting, data-driven index calculation, and then validation by case study. Firstly, performance indexes were developed based on a literature review as well as surveys and function analysis of the information system for construction supervision (CSI system). Then, the weights of and relationships among of the indexes are determined by AHP. After that, with daily workflow and inspection activities tracked in the CSI system, a method and a software module for automatic calculation of indexes were developed. Lastly, the proposed approach was validated by a real-world case. The result showed that the proposed approach can quantify the performance of a construction supervisor systematically and automatically, which shed lights on how to evaluate the performance of a worker based on the tracking of daily activities. The data-driven process enhanced our strong interpretation of member actions and evaluation indexes, and can boost the performance of every member in an organization.
SN  - 2075-5309
SN  - 2075-5309
DA  - MAY 11
PY  - 2023
VL  - 13
IS  - 5
C7  - 1264
DO  - 10.3390/buildings13051264
AN  - WOS:000996531900001
ER  -

TY  - JOUR
AU  - Hosseinihaghighi, S
AU  - Panchabikesan, K
AU  - Dabirian, S
AU  - Webster, J
AU  - Ouf, M
AU  - Eicker, U
TI  - Discovering, processing and consolidating housing stock and smart thermostat data in support of energy end-use mapping and housing retrofit program planning
T2  - SUSTAINABLE CITIES AND SOCIETY
AB  - There is growing interest in energy mapping amongst municipal planners and policymakers to accelerate greenhouse gas (GHG) emissions reduction program implementation. Responding to this interest partly involves addressing challenges related to building stock data collection and processing. The present study, carried out in conjunction with Natural Resources Canada's Canadian Energy End-use Mapping (CEE Map) project, aims to develop the inputs for housing energy modeling and mapping using property assessment data, building, heating permit data, and smart thermostat data. In this context, a systematic workflow is presented to extract useful information from various data sources to support housing energy simulations and municipal retrofit program planning. Permit data analysis supported refinement of housing data in Kelowna's urban digital twin. Results from building permit data analysis serve to update housing attributes including construction year and dwelling type. Results from heating permit analysis suggest that 17.5% of Kelowna dwellings could be potential candidates for heating system upgrades. Regarding thermostat setpoint temperature and occupancy for energy simulations, results obtained from smart thermostat data analysis were compared with EnerGuide Rating System (ERS) assumptions to investigate the potential improvements that can be made in energy simulation inputs. Comparative results indicated variations of up to 2 degrees C between smart thermostat data and EnerGuide assumptions for thermostat setpoint temperatures. Also, smart thermostat data suggests that 87% of dwellings were occupied for more than 50% of the time, whereas in ERS, occupancy is assumed to be 50%. Together, the overall data workflow and the detailed investigation of different datasets contributes to the development of a best practice methodology for housing energy modeling and mapping for municipalities in support of GHG emission reductions. Further, recommendations for permit data collection and scope for future research works are provided.
SN  - 2210-6707
SN  - 2210-6715
DA  - MAR
PY  - 2022
VL  - 78
C7  - 103640
DO  - 10.1016/j.scs.2021.103640
AN  - WOS:000780336800001
ER  -

TY  - CPAPER
AU  - Meuzelaar, T
AU  - Zahuranec, SD
AU  - Jonas, JP
ED  - Kleinmann, B
ED  - Skousen, J
ED  - Wolkersdorfer, C
TI  - Innovative data collection and management strategies for improved water treatment efficiency
T2  - PROCEEDINGS OF THE INTERNATIONAL MINE WATER ASSOCIATION CONFERENCE & WEST VIRGINIA MINE DRAINAGE TASK FORCE SYMPOSIUM 2024
CP  - 2024 International Mine Water Association Conference & West Virginia Mine Drainage Task Force Symposium
AB  - The performance of active and passive water treatment systems can be negatively influenced by seasonal and diurnal water quality and quantity fluctuations of the feedwater. Treatment performance can be improved by proactively modifying the water management strategy in response to these fluctuations. Employing modern, full, or partially automated configurations as part of the water management strategy can efficiently optimize these types of water treatment systems.
   These automated system configurations build upon existing components with emerging technologies resulting in the following innovative data workflow: (a) automated, frequent collection of data at various treatment system monitoring points using sensor-based technologies, (b) automated alarms that can signal remote system upset conditions (compliance exceedances, pump malfunction, clogging, fouling etc.), (c) telemetry-based data upload and ingestion by a cloud-based data management system, (d) automated data cleaning and preparation pipelines, and (e) the use of conventional statistical and computational techniques and, when necessary, more advanced algorithms such as machine learning to analyze incoming data streams.
   This workflow promotes intelligent, real-time guidance on water treatment and management decisions, such as treatment methods, dosage frequency, water diversion, and more, can be provided in near real-time through visualization, reporting, dashboards, and PLC controls. Results of the implementation of various components of this workflow demonstrate benefits such as improved treatment efficiency, more reliable operation, compliance with standards at discharge points, and overall reduction in labor, reagent costs, and energy demand. Additionally, transferring all system data, including sensor data, images, operator logs, and legacy PDFs, to a cloud-hosted data warehouse opens important opportunities for enhancing value extracted from collected data.
   An example application is provided for a remote semi-passive treatment system designed to treat waste rock drainage prone to upset conditions due predominantly to large fluctuations in water volume and difficulty staffing an experienced operator. Finally, the authors discuss how this workflow could be used to optimize a full-scale active treatment system with multiple sensor locations and numerous real-time data streams using a digital twin/machine learning approach.
SN  - 978-3-9825-2932-5
PY  - 2024
SP  - 435
EP  - 439
AN  - WOS:001517409300087
ER  -

TY  - JOUR
AU  - Muenzinger, M
AU  - Prechtel, N
AU  - Behnisch, M
TI  - Mapping the urban forest in detail: From LiDAR point clouds to 3D tree models
T2  - URBAN FORESTRY & URBAN GREENING
AB  - Trees are an integral component of the urban environment and important for human well-being, adaption measures to climate change and sustainable urban transformation. Understanding the small-scale impacts of urban trees and strategically managing the ecosystem services they provide requires high-resolution information on urban forest structure, which is still scarce. In contrast, there is an abundance of data portraying urban areas and an associated trend towards smart cities and digital twins as analysis platforms. A GIS workflow is presented in this paper that may close this data gap by classifying the urban forest from LiDAR point clouds, detecting and reconstructing individual crowns, and enabling a tree representation within semantic 3D city models. The workflow is designed to provide robust results for point clouds with a density of at least 4 pts/m2 that are widely available. Evaluation was conducted by mapping the urban forest of Dresden (Germany) using a point cloud with 4 pts/m2. An object-based data fusion approach is implemented for the classification of the urban forest. A classification accuracy of 95 % for different urban settings is achieved by combining LiDAR with multispectral imagery and a 3D building model. Individual trees are detected by local maxima filtering and crowns are segmented using marker-controlled watershed segmentation. Evaluation highlights the influences of both urban and forest structure on individual tree detection. Substantial differences in detection accuracies are evident between trees along streets (72 %) and structurally more complex tree stands in green areas (31 %), as well as dependencies on tree height and crown diameter. Furthermore, an approach for parameterized reconstruction of tree crowns is presented, which enables efficient and realistic city-wide modeling. The suitability of LiDAR to measure individual tree metrics is illustrated as well as a framework for modeling individual tree crowns via geometric primitives.
SN  - 1618-8667
SN  - 1610-8167
DA  - AUG
PY  - 2022
VL  - 74
C7  - 127637
DO  - 10.1016/j.ufug.2022.127637
AN  - WOS:000819385200003
ER  -

TY  - JOUR
AU  - Hoier, D
AU  - Gross-Ophoff-Mueller, C
AU  - Franklin, C
AU  - Hallek, M
AU  - von Stebut, E
AU  - Elter, T
AU  - Mauch, C
AU  - Kreuzberg, N
AU  - Koll, P
TI  - Digital decision support for structural improvement of melanoma tumor boards: using standard cases to optimize workflow
T2  - JOURNAL OF CANCER RESEARCH AND CLINICAL ONCOLOGY
AB  - PurposeChoosing optimal cancer treatment is challenging, and certified cancer centers must present all patients in multidisciplinary tumor boards (MDT). Our aim was to develop a decision support system (DSS) to provide treatment recommendations for apparently simple cases already at conference registration and to classify these as "standard cases". According to certification requirements, discussion of standard cases is optional and would thus allow more time for complex cases.MethodsWe created a smartphone query that simulated a tumor conference registration and requested all information needed to provide a recommendation. In total, 111 out of 705 malignant melanoma cases discussed at a skin cancer center from 2017 to 2020 were identified as potential standard cases, for which a digital twin recommendation was then generated by DSS.ResultsThe system provided reliable advice in all 111 cases and showed 97% concordance of MDT and DSS for therapeutic recommendations, regardless of tumor stage. Discrepancies included two cases (2%) where DSS advised discussions at MDT and one case (1%) with deviating recommendation due to advanced patient age.ConclusionsOur work aimed not to replace clinical expertise but to alleviate MDT workload and enhance focus on complex cases. Overall, our DSS proved to be a suitable tool for identifying standard cases as such, providing correct treatment recommendations, and thus reducing the time burden of tumor conferences in favor for the comprehensive discussion of complex cases. The aim is to implement the DSS in routine tumor board software for further qualitative assessment of its impact on oncological care.
SN  - 0171-5216
SN  - 1432-1335
DA  - MAR 8
PY  - 2024
VL  - 150
IS  - 3
C7  - 115
DO  - 10.1007/s00432-024-05627-3
AN  - WOS:001177624600001
ER  -

TY  - JOUR
AU  - Moreno, SA
AU  - Loup, M
AU  - Lebre, M
AU  - Deschamps, L
AU  - Bacher, JP
AU  - Mahecha, SD
TI  - Virtual Tours as Effective Complement to Building Information Models in Computer-Aided Facility Management Using Internet of Things
T2  - APPLIED SCIENCES-BASEL
AB  - Featured Application using Virtual Tours (VTs) to complement Building Information Models (BIMs) and Internet of Things (IoT) systems as a solution for remote control, building automation systems, and other tasks involved in Computer-Aided Facility Management (CAFM). The aim is to provide contextual access to information and opportunities for interaction, simplifying the task of those responsible for managing and maintaining building assets.Abstract This study investigates the integration of Building Information Models (BIMs) and Virtual Tour (VT) environments in the Architecture, Engineering and Construction (AEC) industry, focusing on Computer-Aided Facility Management (CAFM), Computerized Maintenance Management Systems (CMMSs), and data Life-Cycle Assessment (LCA). The interconnected nature of tasks throughout a building's life cycle increasingly demands a seamless integration of real-time monitoring, 3D models, and building data technologies. While there are numerous examples of effective links between IoT and BIMs, as well as IoT and VTs, a research gap exists concerning VT-BIM integration. This article presents a technical solution that connects BIMs and IoT data using VTs to enhance workflow efficiency and information transfer. The VT is developed upon a pilot based on the Controlled Environments for Living Lab Studies (CELLS), a unique facility designed for flexible monitoring and remote-control processes that incorporate BIMs and IoT technologies. The findings offer valuable insights into the potential of VTs to complement and connect to BIMs from a life-cycle perspective, improving the usability of digital twins for beginner users and contributing to the advancement of the AEC and CAFM industries. Our technical solution helps complete the connectivity of BIMs-VT-IoT, providing an intuitive interface (VT) for rapid data visualisation and access to dashboards, models and building databases. The practical field of application is facility management, enhancing monitoring and asset management tasks. This includes (a) sensor data monitoring, (b) remote control of connected equipment, and (c) centralised access to asset-space information bridging BIM and visual (photographic/video) data.
SN  - 2076-3417
DA  - SEP
PY  - 2024
VL  - 14
IS  - 17
C7  - 7998
DO  - 10.3390/app14177998
AN  - WOS:001311305000001
ER  -

TY  - JOUR
AU  - Jiang, YS
AU  - Liu, XL
AU  - Kang, K
AU  - Wang, ZC
AU  - Zhong, RY
AU  - Huang, GQ
TI  - Blockchain-enabled cyber-physical smart modular integrated construction
T2  - COMPUTERS IN INDUSTRY
AB  - Modular Integrated Construction (MiC) has been one of the most innovative solutions to address the ever-growing housing demands in megacities such as Hong Kong. MiC offers a range of benefits including cost-effectiveness, high productivity, and high sustainability for construction industry. Multiple stakeholders in MiC project, such as project client, module manufacturer, logistics company, and building contractor, normally use their own proprietary and centralized Enterprise Information System (EIS). However, stake-holders are facing challenges to share information throughout MiC projects. "Islands of Information" issue is commonly existing and creates information fragmentation and discontinuity. As an emerging Information and Communication Technology (ICT), blockchain provides unified standards and protocols for information sharing based on decentralized P2P framework with enhanced transparency and security. Aiming to address the information fragmentation and discontinuity in MiC project, this paper proposes a blockchain-enabled cyber-physical smart MiC platform to facilitate cross-enterprise information sharing among multiple sta-keholders based on User-Centered Design (UCD) method. A practical roadmap is presented for the design, development, deployment, and application of MiC blockchain with new opportunities and guidelines. Initial investigations based on an industrial collaborated company are analyzed and the blockchain-enabled cyber-physical MiC workflow is designed. The user-centered blockchain explorers with high-fidelity digital twins are illustrated. Some preliminary results are found that MiC blockchain not only facilitates cyber-physical construction progress traceability, and real-time KPI visualization & evaluation, but also improves the in-formation reliability, immutability, and transparency in MiC projects. (c) 2021 Elsevier B.V. All rights reserved.
SN  - 0166-3615
SN  - 1872-6194
DA  - DEC
PY  - 2021
VL  - 133
C7  - 103553
DO  - 10.1016/j.compind.2021.103553
C6  - OCT 2021
AN  - WOS:000710563900003
ER  -

TY  - JOUR
AU  - Huang, H
AU  - Liu, ZM
AU  - Wang, YD
AU  - Wang, HN
TI  - Parametric modeling and safety simulation of pit excavation affecting adjacent tunnels based on BIM-FEM framework
T2  - TUNNELLING AND UNDERGROUND SPACE TECHNOLOGY
AB  - This research puts forward an intelligent analysis framework to tackle key issues in the collaborative analysis of Building Information Modeling (BIM) and the Finite Element Method (FEM) within urban foundation pit projects. These issues include model conversion distortion, poor mesh convergence, and low efficiency in multi-case iterations. By integrating BIM parametric modeling with FEM automated analysis, this framework enables the deformation analysis and safety assessment of existing tunnels during foundation pit excavation. Firstly, a 3D geology-support-tunnel parametric collaborative modeling workflow on the BIM platform is utilized to enhance collaborative design efficiency and facilitate dynamic updates. Secondly, through the secondary development of TCL (Tool Command Language) and Python scripts for the meshing software Hypermesh and the numerical simulation software ABAQUS, a hexahedral mesh optimization algorithm and an automated preprocessing process are established. This effectively resolves problems of geometric aberration and mesh convergence in traditional BIM-FEM data conversion. Finally, an interactive platform for multi-case analysis is employed to analyze the spatio-temporal evolution of neighboring tunnel deformation during excavation. Validation using typical soft-soil pit projects reveals that the simulation error for tunnel deformation is maintained within the range of 1.36% to 4.7%, while the errors for surface settlement and ground wall displacement remain below 6.31%. The displacement characteristics of the ground wall in the soft-soil layer are captured with an average accuracy of 97.45%. The study shows that the horizontal and vertical displacements of the tunnel decay exponentially with the distance from and depth of the foundation pit. It identifies the deformation-sensitive areas of existing tunnels and sets up critical position control parameters for displacement thresholds. This framework overcomes the efficiency bottlenecks of traditional manual modeling, reducing the time for a single design change from 8 to 12 h to less than 30 min. Thus, it offers a high-precision and efficient solution for safety assessments in sensitive surrounding environments through digital twin technology.
SN  - 0886-7798
SN  - 1878-4364
DA  - OCT
PY  - 2025
VL  - 164
C7  - 106800
DO  - 10.1016/j.tust.2025.106800
AN  - WOS:001517289700005
ER  -

TY  - JOUR
AU  - Luczak, D
TI  - Nonlinear Identification with Constraints in Frequency Domain of Electric Direct Drive with Multi-Resonant Mechanical Part
T2  - ENERGIES
AB  - Knowledge of a direct-drive model with a complex mechanical part is important in the synthesis of control algorithms and in the predictive maintenance of digital twins. The identification of two-mass drive systems with one low mechanical resonance frequency is often described in the literature. This paper presents an identification workflow of a multi-resonant mechanical part in direct drive with up to three high-frequency mechanical resonances. In many methods, the identification of a discrete time (DT) model is applied, and its results are transformed into a continuous-time (CT) representation. The transformation from a DT model to a CT model has limitations due to nonlinear mapping of discrete to continuous frequencies. This problem may be overcome by identification of CT models in the frequency domain. This requires usage of a discrete Fourier transform to obtain frequency response data as complex numbers. The main work presented in this paper is the appropriate fitting of a CT model of a direct-drive mechanical part to complex number datasets. Fitting to frequency response data is problematic due to the attraction of unexcited high frequency ranges, which lead to wrong identification results of multi-mass (high order) drive systems. Firstly, a CT fitting problem is a nonlinear optimization problem, and, secondly, complex numbers may be presented in several representations, which leads to changes in the formulation of the optimization problem. In this paper, several complex number representations are discussed, and their influence on the optimization process by simulation evaluation is presented. One of the best representations is then evaluated using a laboratory setup of direct drive with unknown parameters of three high mechanical resonance frequencies. The mechanical part of the examined direct drive is described by three mechanical resonances and antiresonances, which are characteristic of a four-mass drive system. The main finding is the addition of frequency boundaries in the identification procedure, which are the same as those in the frequency range of the excitation signal. Neither a linear least-square algorithm nor a nonlinear least-square algorithm is suitable for this approach. The usage of nonlinear least-square algorithm with constraints as a fitting algorithm allows one to solve the issue of modeling multi-mass direct-drive systems in the frequency domain. The second finding of this paper is a comparison of different cost functions evaluated to choose the best complex number representation for the identification of multi-mass direct-drive systems.
SN  - 1996-1073
DA  - NOV
PY  - 2021
VL  - 14
IS  - 21
C7  - 7190
DO  - 10.3390/en14217190
AN  - WOS:000718802400001
ER  -

TY  - JOUR
AU  - Smaldone, F
AU  - Lagger, J
AU  - Mainolfi, G
TI  - Profiling employability skills for life cycle professionals amid the circular and digital economy
T2  - INTERNATIONAL JOURNAL OF LIFE CYCLE ASSESSMENT
AB  - PurposeThis study aims to identify the most in-demand employability skills for Life Cycle Assessment (LCA) professionals by analysing job advertisements through textual big data techniques. The research addresses four main questions: (a) which skills are most frequently required for LC-related roles; (b) what thematic areas emerge in the skill profiles of LC professionals; (c) how these skills are correlated and co-requested across job postings; and (d) how future demand for key skills may evolve. The goal is to support employers, educators, and professionals in understanding the shifting landscape of skill requirements within sustainability-oriented industries, especially in light of the circular and digital twin transition.MethodsUsing a text mining approach, a corpus of 32,783 job descriptions relevant to life cycle professionals were obtained from major US job portals. Term frequency, stylometry, clustering and topic modeling identified critical skills in this domain. Subsequently, Markov Chain Monte Carlo simulations were applied to model interview scenarios and analyze skill correlations.Results and discussionResults revealed that the most frequently requested skills for life cycle professionals include energy transition planning, carbon footprint analysis, and environmental legislation compliance, indicating a strong emphasis on sustainability practices. Topic modeling and network analysis identified significant correlations between skills such as project management and performance quality management, underscoring their co-requirement in job ads.ConclusionsFindings demonstrate that life cycle professionals must possess a comprehensive skill set that balances technical competencies in sustainability with leadership and project management abilities. As environmental regulations and corporate sustainability targets evolve, professionals with expertise in energy transition and environmental impact will be highly sought after. These results provide valuable insights for professionals aiming to remain competitive in a rapidly changing job market.LimitationsThe study's data collection concluded in September 2024. Consequently, any trend changes beyond this point are not reflected, and would need to be addressed in subsequent studies. Furthermore, the methods employed in this study, while valuable and comprehensive, come with inherent limitations. Despite these constraints, significant insights into the skill requirements and employability landscape for life cycle professionals were provided.RecommendationsIt is recommended that both educational institutions and employers prioritize the development of the identified skills in their training programs. Additionally, further research would provide the opportunity to explore how emerging technologies, such as AI and machine learning, as well as regulatory shifts may impact the future demand for specific competencies in the field.
SN  - 0948-3349
SN  - 1614-7502
DA  - 2025 JUN 6
PY  - 2025
DO  - 10.1007/s11367-025-02490-3
C6  - JUN 2025
AN  - WOS:001503310200001
ER  -

TY  - JOUR
AU  - Dorosan, M
AU  - Chen, YL
AU  - Zhuang, QY
AU  - Lam, SWS
TI  - In Silico Evaluation of Algorithm-Based Clinical Decision Support Systems: Protocol for a Scoping Review
T2  - JMIR RESEARCH PROTOCOLS
AB  - Background: Integrating algorithm-based clinical decision support (CDS) systems poses significant challenges in evaluating their actual clinical value. Such CDS systems are traditionally assessed via controlled but resource-intensive clinical trials. Objective: This paper presents a review protocol for preimplementation in silico evaluation methods to enable broadened impact analysis under simulated environments before clinical trials. Methods: We propose a scoping review protocol that follows an enhanced Arksey and O'Malley framework and PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) guidelines to investigate the scope and research gaps in the in silico evaluation of algorithm-based CDS models-specifically CDS decision-making end points and objectives, evaluation metrics used, and simulation paradigms used to assess potential impacts. The databases searched are PubMed, Embase, CINAHL, PsycINFO, Cochrane, IEEEXplore, Web of Science, and arXiv. A 2-stage screening process identified pertinent articles. The information extracted from articles was iteratively refined. The review will use thematic, trend, and descriptive analyses to meet scoping aims. Results: We conducted an automated search of the databases above in May 2023, with most title and abstract screenings completed by November 2023 and full-text screening extended from December 2023 to May 2024. Concurrent charting and full-text analysis were carried out, with the final analysis and manuscript preparation set for completion inJuly2024. Publication of the review results is targeted from July 2024 to February 2025. As of April 2024, a total of 21 articles have been selected following a 2-stage screening process; these will proceed to data extraction and analysis. Conclusions:We refined our data extraction strategy through a collaborative, multidisciplinary approach, planning to analyze results using thematic analyses to identify approaches to in silico evaluation. Anticipated findings aim to contribute to developing a unified in silico evaluation framework adaptable to various clinical workflows, detailing clinical decision-making characteristics, impact measures, and reusability of methods. The study's findings will be published and presented in forums combining artificial intelligence and machine learning, clinical decision-making, and health technology impact analysis. Ultimately, we aim to bridge the development-deployment gap through in silico evaluation-based potential impact assessments.
SN  - 1929-0748
PY  - 2025
VL  - 14
C7  - e63875
DO  - 10.2196/63875
AN  - WOS:001411092500003
ER  -

TY  - JOUR
AU  - Solís-Lemus, JA
AU  - Baptiste, T
AU  - Barrows, R
AU  - Sillett, C
AU  - Gharaviri, A
AU  - Raffaele, G
AU  - Razeghi, O
AU  - Strocchi, M
AU  - Kotadia, I
AU  - Bodagh, N
AU  - O'Hare, D
AU  - O'Neill, M
AU  - Williams, SE
AU  - Roney, C
AU  - Niederer, S
TI  - Evaluation of an open-source pipeline to create patient-specific left atrial models: A reproducibility study
T2  - COMPUTERS IN BIOLOGY AND MEDICINE
AB  - This work presents an open-source software pipeline to create patient-specific left atrial models with fibre orientations and a fibrDEFAULTosis map, suitable for electrophysiology simulations, and quantifies the intra and inter observer reproducibility of the model creation. The semi-automatic pipeline takes as input a contrast enhanced magnetic resonance angiogram, and a late gadolinium enhanced (LGE) contrast magnetic resonance (CMR). Five operators were allocated 20 cases each from a set of 50 CMR datasets to create a total of 100 models to evaluate inter and intra-operator variability. Each output model consisted of: (1) a labelled surface mesh open at the pulmonary veins and mitral valve, (2) fibre orientations mapped from a diffusion tensor MRI (DTMRI) human atlas, (3) fibrosis map extracted from the LGE-CMR scan, and (4) simulation of local activation time (LAT) and phase singularity (PS) mapping. Reproducibility in our pipeline was evaluated by comparing agreement in shape of the output meshes, fibrosis distribution in the left atrial body, and fibre orientations. Reproducibility in simulations outputs was evaluated in the LAT maps by comparing the total activation times, and the mean conduction velocity (CV). PS maps were compared with the structural similarity index measure (SSIM). The users processed in total 60 cases for inter and 40 cases for intra-operator variability. Our workflow allows a single model to be created in 16.72 +/- 12.25 min. Similarity was measured with shape, percentage of fibres oriented in the same direction, and intra-class correlation coefficient (ICC) for the fibrosis calculation. Shape differed noticeably only with users' selection of the mitral valve and the length of the pulmonary veins from the ostia to the distal end; fibrosis agreement was high, with ICC of 0.909 (inter) and 0.999 (intra); fibre orientation agreement was high with 60.63% (inter) and 71.77% (intra). The LAT showed good agreement, where the median +/- IQR of the absolute difference of the total activation times was 2.02 +/- 2.45 ms for inter, and 1.37 +/- 2.45 ms for intra. Also, the average +/- sd of the mean CV difference was-0.00404 +/- 0.0155 m/s for inter, and 0.0021 +/- 0.0115 m/s for intra. Finally, the PS maps showed a moderately good agreement in SSIM for inter and intra, where the mean +/- sd SSIM for inter and intra were 0.648 +/- 0.21 and 0.608 +/- 0.15, respectively. Although we found notable differences in the models, as a consequence of user input, our tests show that the uncertainty caused by both inter and intra-operator variability is comparable with uncertainty due to estimated fibres, and image resolution accuracy of segmentation tools.
SN  - 0010-4825
SN  - 1879-0534
DA  - AUG
PY  - 2023
VL  - 162
C7  - 107009
DO  - 10.1016/j.compbiomed.2023.107009
C6  - JUN 2023
AN  - WOS:001120683700001
ER  -

TY  - JOUR
AU  - Motaei, E
AU  - Ganat, T
TI  - Smart proxy models art and future directions in the oil and gas industry: A review
T2  - GEOENERGY SCIENCE AND ENGINEERING
AB  - This paper presents a review of the state of the art of proxy models application in the oil and gas industry from upstream, midstream, and downstream. The focus on proxy models is because they are the pillar of the digital twins. They compensate for the slow response of the numerical simulation models. Proxy models can get updated very quickly with new data compared to numerical models. The proxy models apply to cross-disciplines such as fluid mechanics, thermodynamics, and electronics. Proxy models are tools for the swift response to the process events when the cost of decision time is high. The history of the proxy model is discussed first followed by a review of the literature. First, a literature review and the history of the proxy model are covered. The objectives and uses of proxy models are then thoroughly described while the evolution and development process of proxy models are detailed. A few proxy model examples are provided to emphasize the significance of the proxy model and technical advancement. Regression, artificial neural networks, fuzzy logic, and support vector machines are the major approaches for delivering proxy models. Each method's strengths and drawbacks are addressed. These case studies show the need for data cleansing, data transformation to the appropriate domain, and a precise model verification and validation strategy for creating a trustworthy proxy model. Industry-wide, the gaps, and difficulties are addressed, including data accessibility, workflow, model health check, operability, and model uncertainty. A review of proxy models revealed that both technical solution acceptance and workflow robustness have greatly increased at the maturity level in proxy model applications in the oil and gas industry. However, the typical workflow is not being used, and many case studies show that crucial steps like verification and validation are not being taken. The key issue identified in the review was that proxy models are derived from analytical and numerical models, which is in contrast to proxy models, which by their nature rely on the data at hand to inform predictions and optimizations. The astounding potential of proxy models in prediction and optimization is still untapped, and more research is needed to improve model quality, incorporate data into the core of the proxy models, increase their robustness, and develop robust performance metrics to address and evaluate the performance, dependability, and timeliness of the proxy models. Currently, proxy models are secondary tools for optimization of the processes and decision making, while in the future outlook is to be the main tool not only in daily surveillance and monitoring but for the optimization process. In this paper, a best practice workflow for the proxy model is proposed for the oil and gas industry. The evolution of proxy models and their level of complexity is reviewed. Most published work on proxy models is built based on the synthetic data and simulation model output. In the applied workflow, most model validation and verification are missed or ignored. Except for a few published works on the real-time usage, the models are barely connected to real-time and they are used for static decision-making process other than real-time usage.
SN  - 2949-8929
SN  - 2949-8910
DA  - AUG
PY  - 2023
VL  - 227
C7  - 211918
DO  - 10.1016/j.geoen.2023.211918
C6  - MAY 2023
AN  - WOS:001053726900001
ER  -

TY  - CPAPER
AU  - El-Gayar, OF
AU  - Thota, S
ED  - Callaos, N
ED  - Loutfi, M
ED  - Justan, M
TI  - Software project management: The role of modeling
T2  - 6TH WORLD MULTICONFERENCE ON SYSTEMICS, CYBERNETICS AND INFORMATICS, VOL VIII, PROCEEDINGS: CONCEPTS AND APPLICATIONS OF SYSTEMICS, CYBERNETICS AND INFORMATICS II
CP  - 6th World Multi-Conference on Systemics, Cybernetics and Informatics (SCI 2002)/8th International Conference on Information Systems Analysis and Synthesis (ISAS 2002)
AB  - With the ubiquity of software and in the quest towards improving software development activities, the software engineering research community engaged in an active research agenda on modeling software processes. The modeling objectives ranged from process understanding, improvement, and management, to project management. In spite of the significant achievements in the field, few (if any) of the research results are adopted by the industry.
   This paper presents a brief review and a critical evaluation of the status of software process modeling practices with particular emphasis on their applications to project management from a practitionees point of view as well as recommendations for future work.
PY  - 2002
SP  - 293
EP  - 298
AN  - WOS:000179559600055
ER  -

TY  - CPAPER
AU  - Benjamin, P
AU  - Painter, M
AU  - Erraguntla, M
AU  - Marshall, C
AU  - Mayer, R
A1  - IEEE
A1  - IEEE
TI  - A Framework for Adaptive Process Modeling and Execution (FAME)
T2  - SEVENTH IEEE INTERNATIONAL WORKSHOPS ON ENABLING TECHNOLOGIES: INFRASTRUCTURE FOR COLLABORATIVE ENTERPRISES (WET ICE '98)
CP  - 7th IEEE International Workshops on Enabling Technologies - Infrastructure for Collaborative Enterprises (WET ICE 98)
AB  - This paper describes the architecture and concept of operation of a Framework for Adaptive Process Modeling and Execution (FAME). The research addresses the absence of robust methods for supporting the software process management life cycle. FAME employs a novel, model-based approach in providing automated support for different activities in the software development life cycle including project definition, process design, process analysis, process enactment, process execution status monitoring, and execution status-triggered process redesign. FAME applications extend beyond the software development domain to areas such as agile manufacturing, project management, logistics planning, and business process reengineering.
SN  - 0-8186-8751-7
PY  - 1998
SP  - 3
EP  - 9
DO  - 10.1109/ENABL.1998.725659
AN  - WOS:000076610700001
ER  -

TY  - CPAPER
AU  - Santos, C
AU  - Shariati, B
AU  - Emmerich, R
AU  - Schmidt-Langhorst, C
AU  - Schubert, C
AU  - Fischer, JK
A1  - IEEE
TI  - Automated Dataset Generation for QoT Estimation in Coherent Optical Communication Systems
T2  - 2022 EUROPEAN CONFERENCE ON OPTICAL COMMUNICATION (ECOC)
CP  - European Conference on Optical Communication (ECOC)
AB  - We demonstrate sophisticated laboratory automation and data pipeline capable of generating large, diverse, and high-quality public datasets. The demo covers the full workflow from setup reconfiguration to data monitoring and storage, represented on a digital replica of the setup and updated in near real-time. (C) 2022 The Author(s)
SN  - 978-1-957171-15-9
PY  - 2022
AN  - WOS:000925380500240
ER  -

TY  - CPAPER
AU  - Maldonado, FJ
AU  - Oonk, S
AU  - Selmic, R
A1  - IEEE
TI  - Complex System Health Analysis by the Graphical Evolutionary Hybrid Neuro-Observer (GNeuroObs)
T2  - 2016 IEEE AUTOTESTCON PROCEEDINGS
CP  - IEEE AUTOTESTCON Conference
AB  - Obtaining methodologies that enable predictive health monitoring of components degradation and the propagation of related effects across the overall system is a need when designing complex systems (such as autonomous vehicles, robotic systems, and aerospace platforms). In this paper, a current software development is presented for workflow generation and visualization to evaluate how component degradation impacts an entire system. Relevant technical aspects of this "Graphical Evolutionary Hybrid Neuro-Observer" (GNeuroObs) include: (a) highly accurate system modeling; (b) techniques for system level analysis; and (c) low level entity instantiations that builds on health monitoring and root cause analysis. The GNeuroObs is described with the application of a fuel subsystem. In that system, the methodology allows for describing interrelations among a set of heterogeneous sensors, where Health Monitoring algorithms are used to analyze failures in entities and propagation of effects across the system.
SN  - 1088-7725
SN  - 978-1-5090-0789-9
PY  - 2016
AN  - WOS:000386738000038
ER  -

TY  - CHAP
AU  - Cahill, B
AU  - Carrington, D
AU  - Song, B
AU  - Strooper, P
ED  - Richardson, I
ED  - Runeson, P
ED  - Messnarz, R
TI  - An industry-based evaluation of process modeling techniques
T2  - SOFTWARE PROCESS IMPROVEMENT, PROCEEDINGS
CP  - 13th European Conference on Software Process Improvement
AB  - There are many ways to model software development processes. This paper reports a feature analysis of four process modeling techniques using criteria specified by a software development organization. The evaluation used a single process, peer review, modeled using all four techniques. Performing the modeling activity highlighted the usefulness of the modeling activity and the usefulness of metamodels in structuring processes.
SN  - 0302-9743
SN  - 1611-3349
SN  - 3-540-47695-4
PY  - 2006
VL  - 4257
SP  - 111
EP  - 122
AN  - WOS:000241651600011
ER  -

TY  - CPAPER
AU  - DEWAL, S
ED  - Loucopoulos, P
TI  - A METHODOLOGY FOR REQUIREMENTS ANALYSIS AND EVALUATION OF SDES
T2  - ADVANCED INFORMATION SYSTEMS ENGINEERING
CP  - 4TH INTERNATIONAL CONF ON ADVANCED INFORMATION SYSTEMS ENGINEERING ( CAISE 92 )
SN  - 0302-9743
SN  - 3-540-55481-5
PY  - 1992
VL  - 593
SP  - 394
EP  - 409
AN  - WOS:A1992BY26R00024
ER  -

TY  - CPAPER
AU  - de Souza, VF
AU  - L'Erario, A
AU  - Fabri, JA
AU  - Gonçalves, JA
ED  - Rocha, A
ED  - Reis, LP
ED  - Cota, MP
ED  - Suarez, OS
ED  - Goncalves, R
TI  - Model for Monitoring in Distributed Projects: An experiment using Kanban and Business Process Modeling Notation (BPMN)
T2  - 2016 11TH IBERIAN CONFERENCE ON INFORMATION SYSTEMS AND TECHNOLOGIES (CISTI)
CP  - 11th Iberian Conference on Information Systems and Technologies (CISTI)
AB  - The objective of this paper is to propose a model to assist in monitoring the software development process in distributed environments. For an initial validation of the model was developed an experiment with IT professionals and were used as tools Kanban and BPMN
SN  - 2166-0727
SN  - 978-989-98434-6-2
PY  - 2016
AN  - WOS:000382923300021
ER  -

TY  - JOUR
AU  - Margaria, T
AU  - Bosselmann, S
AU  - Kujath, B
TI  - Simple Modeling of Executable Role-Based Workflows: An Application in the Healthcare Domain
T2  - JOURNAL OF INTEGRATED DESIGN & PROCESS SCIENCE
AB  - Process modeling has become an established technique to document, analyse and automate workflows. Workflows that model the procedures followed by professionals require the immediate contribution of these professionals, who usually lack broad knowledge of formal models as well as software engineering skills. Simplicity of the selected modeling approach throughout each step of the design phase is therefore a key factor for the success of a workflow management project. In this paper we show a process modelling framework that combines simplicity and intuitiveness in the modeling phase with immediate evaluation of the models via execution. This combination allows practitioners and professionals to play a key role in the rapid development of IT applications and systems that support role-based workflows. We discuss the practicability and benefits of this approach on an exemplary case study from the healthcare domain: analysis and optimization of outpatient clinic processes of a leading German hospital.
SN  - 1092-0617
SN  - 1875-8959
PY  - 2013
VL  - 17
IS  - 3
SP  - 25
EP  - 45
DO  - 10.3233/jid-2013-0017
AN  - WOS:000211955600003
ER  -

TY  - JOUR
AU  - Borchert, D
AU  - Suarez-Zuluaga, DA
AU  - Thomassen, YE
AU  - Herwig, C
TI  - Risk assessment and integrated process modeling-an improved QbD approach for the development of the bioprocess control strategy
T2  - AIMS BIOENGINEERING
AB  - A Process characterization is a regulatory imperative for process validation within the biopharmaceutical industry. Several individual steps must be conducted to achieve the final control strategy. For that purpose, tools from the Quality by Design (QbD) toolbox are often considered. These tools require process knowledge to conduct the associated data analysis. They include cause and effect analysis, multivariate data analysis, risk assessment and design space evaluation. However, this approach is limited to the evaluation of single unit operations. This is risky as the interactions of the operations may render the control strategy invalid. Hence, a holistic process evaluation is required. Here, we present a novel workflow that shows how simple data analysis tools can be used to investigate the process holistically. This results in a significant reduction of the experimental effort and in the development of an integrated process control strategy. This novel QbD workflow is based on a novel combination of risk assessment and integrated process modeling. We demonstrate this workflow in a case study and show that the herein presented approach can be applied to any biopharmaceutical process. We demonstrate a workflow that can reduce the number of factors and increase the amount of responses within a Design of Experiments (DoE). Consequently, this result demonstrates that experimental costs and time can be reduced by investing more time in thoughtful data analysis.
SN  - 2375-1495
PY  - 2020
VL  - 7
IS  - 4
SP  - 254
EP  - 271
DO  - 10.3934/bioeng.2020022
AN  - WOS:000582760200007
ER  -

TY  - JOUR
AU  - Recker, J
AU  - Rosemann, M
AU  - Krogstie, J
TI  - ONTOLOGY-VERSUS PATTERN-BASED EVALUATION OF PROCESS MODELING LANGUAGES: A COMPARISON
T2  - COMMUNICATIONS OF THE ASSOCIATION FOR INFORMATION SYSTEMS
AB  - Selecting an appropriate process modeling language forms an important task for organizations engaging in business process management initiatives. A plethora of process modeling languages has been developed over the last decades, leading to a need for rigorous theory to assist in the evaluation and comparison of the capabilities of these languages. While substantial academic progress in the area of process modeling language evaluation has been made in at least two areas, using an ontology-based theory of representation or the framework of workflow patterns, it remains unclear how these frameworks relate to each other. We use a generic framework for language evaluation to establish similarities and differences between these acknowledged reference frameworks and discuss how and to what extent they corroborate each other. Our line of investigation follows the case of the popular BPMN modeling language, whose evaluation from the perspectives of representation theory and workflow patterns is comparatively assessed in this paper. We also show which tenets of modeling quality these frameworks address and that further research is needed, especially in the area of evaluating the pragmatic quality of modeling.
SN  - 1529-3181
PY  - 2007
VL  - 20
SP  - 774
EP  - 799
C7  - 48
AN  - WOS:000414838100048
ER  -

TY  - CPAPER
AU  - Recker, J
AU  - Wohed, P
AU  - Rosemann, M
ED  - Embley, DW
ED  - Olive, A
ED  - Ram, S
TI  - Representation theory versus workflow patterns - The case of BPMN
T2  - CONCEPTUAL MODELING - ER 2006, PROCEEDINGS
CP  - 25th International Conference on Conceptual Modeling
AB  - Selecting an appropriate process modeling language forms an important task within business process management projects. A wide range of process modeling languages has been developed over the last decades, leading to an obvious need for rigorous theory to assist in the evaluation and comparison of the capabilities of these languages. While academic progress in the area of process modeling language evaluation has been made on at least two premises, Representation Theory and Workflow Patterns, it remains unclear how these frameworks relate to each other. We use a generic framework for language evaluation to establish similarities and differences between these acknowledged reference frameworks and discuss how and to what extent they complement respectively substitute each other. Our line of investigation follows the case of the popular BPMN modeling language, whose evaluation from the perspectives of Representation Theory and Workflow Patterns is reconciled in this paper.
SN  - 0302-9743
SN  - 1611-3349
SN  - 3-540-47224-X
PY  - 2006
VL  - 4215
SP  - 68
EP  - +
AN  - WOS:000243115800007
ER  -

TY  - CPAPER
AU  - Babar, Z
AU  - Lapouchnian, A
AU  - Yu, E
ED  - Ralyte, J
ED  - Espana, S
ED  - Pastor, O
TI  - Modeling DevOps Deployment Choices Using Process Architecture Design Dimensions
T2  - PRACTICE OF ENTERPRISE MODELING, POEM 2015
CP  - 8th IFIP WG 8.1 Working Conference on the Practice of Enterprise Modeling (PoEM)
AB  - DevOps is a software development approach that enables enterprises to rapidly deliver software product features through process automation, greater inter-team collaboration and increased efficiency introduced through monitoring and measuring activities. No two enterprise-adopted DevOps approaches would be similar as each enterprise has unique characteristics and requirements. At present, there is no structured method in enterprise architecture modeling that would enable enterprises to devise a DevOps approach suitable for their requirements while considering possible process reconfigurations. Any DevOps implementation can have variations at different points across development and operational processes and enterprises need to be able to systematically map these variation points and understand the trade-offs involved in selecting one alternative over another. In this paper, we use our previously proposed Business Process Architecture modeling technique to express and analyze DevOps alternatives and help enterprises select customized DevOps processes that match their contexts and requirements.
SN  - 1865-1348
SN  - 978-3-319-25897-3
SN  - 978-3-319-25896-6
PY  - 2015
VL  - 235
SP  - 322
EP  - 337
DO  - 10.1007/978-3-319-25897-3_21
AN  - WOS:000369183600021
ER  -

TY  - CPAPER
AU  - Neumann, J
AU  - Neumuth, T
A1  - IEEE
TI  - Standardized Semantic Workflow Modeling in the Surgical Domain Proof-of-concept Analysis and Evaluation for a Neurosurgical Use-Case
T2  - 2015 17TH INTERNATIONAL CONFERENCE ON E-HEALTH NETWORKING, APPLICATION & SERVICES (HEALTHCOM)
CP  - 17th International Conference on E-health Networking, Application & Services (HealthCom)
AB  - Surgical processes, activities and medical devices can be managed and monitored by workflow management systems. An essential aspect of workflow management support in operating room environments is the description and visualization of the underlying processes and activities in a machine readable format, as Surgical Process Models (SPM). However, these SPMs often vary in terms of granularity, naming and representation of process elements as well as their modeling structure. Semantic workflow- and process modeling techniques have become state-of-the-art for the standardized representation of business workflows. The aim of this paper is to perform a proof-of-concept analysis and evaluation of a newly developed methodology for semantic workflow modeling in the surgical domain, using a neurosurgical intervention as an example.
SN  - 978-1-4673-8325-7
PY  - 2015
SP  - 11
EP  - 16
AN  - WOS:000380378700003
ER  -

TY  - CHAP
AU  - Lee, CM
ED  - Gavrilova, M
ED  - Gervasi, O
ED  - Kumar, V
ED  - Tan, CJK
ED  - Taniar, D
ED  - Lagana, A
ED  - Mun, Y
ED  - Choo, H
TI  - Design and implementation of B2Bi collaboration workflow tool based on REE
T2  - COMPUTATIONAL SCIENCE AND ITS APPLICATIONS - ICCSA 2006, PT 4
CP  - International Conference on Computational Science and Its Applications (ICCSA 2006)
AB  - In this paper, the business process was easily modeled by distinguishing between the business process and work logic. Based on this model, B2Bi collaboration Workflow modeling tool, which facilitates collaboration, was designed and implemented. The collaboration workflow modeling tool consists of 3 components; business process modeling tool, execution engine and monitoring tool. First, a business process modeling tool is used to build a process map that reflects the business logic of an application in a quick and accurate manner. Second, an execution engine provides a real-time execution environment for business process instance. Third, a monitoring tool provides a real-time monitoring function for the business process that is in operation at the time. In addition to this, it supports flexibility and expandability based on XML and J2EE for the linkage with the legacy system that was used previously, and suggests a solution for a new corporate strategy and operation.
SN  - 0302-9743
SN  - 3-540-34077-7
PY  - 2006
VL  - 3983
SP  - 1012
EP  - 1021
AN  - WOS:000237649800109
ER  -

TY  - CPAPER
AU  - Barbosa, JF
AU  - Marinho, MLM
AU  - de Moura, HP
ED  - Araujo, RD
ED  - Marques, JC
ED  - DeClasse, TM
ED  - DaCunha, MXC
ED  - DeSouza, MF
TI  - An Empirical Evaluation of a Model for dealing with Epistemic Uncertainty in Agile Software Project Management
T2  - PROCEEDINGS OF THE 19TH BRAZILIAN SYMPOSIUM ON INFORMATION SYSTEMS
CP  - 19th Brazilian Symposium on Information Systems - Green Information Systems for Environmental Sustainability
AB  - Context: The current trend of employing agility in software development indicates the need to manage uncertainty through its cycles of inspection and adaptation to changes.
   Problem: Despite the increasing agile methods and uncertainty management approaches, many agile software projects still fail. Some studies show that existing approaches to managing uncertainty do not consider the quantitative aspect of managing uncertainty in agile projects. The construction of approaches that fill the identified gap involves research methods that can generate results artifacts, methods, frameworks, or models. These approaches need to be evaluated before they are made available to practitioners of uncertainty management in the industry.
   Solution: This article describes an empirical evaluation process of a model called Euler (version 1.0) built to deal with epistemic uncertainty in agile software project management.
   IS Theory: This work was conceived under the aegis of Structured process modeling theory (SPMT), particularly concerning constructing process models as more effective and efficient.
   Method: This study used the framework known as Proof of Concept Research (PoCR).
   Summary of Results: As a result of applying the PoCR, four recommendations emerged. These recommendations resulted in version 2.0 of the model.
   Contributions and Impact in the IS area: The industry can use it to improve the performance of organizations and the processes of managing uncertainties in agile projects.
SN  - 979-8-4007-0759-9
PY  - 2023
SP  - 364
EP  - 371
DO  - 10.1145/3592813.3592926
AN  - WOS:001054513000046
ER  -

TY  - CPAPER
AU  - Rusu, L
AU  - Sârbu, M
AU  - Jecan, S
AU  - Sitar, D
ED  - Rusu, C
TI  - A WFMS MODEL FOR AUTO FLEET MONITORING
T2  - MANAGEMENT OF TECHNOLOGICAL CHANGES, VOL 1
CP  - 6th International Conference on the Management of Technological Changes
AB  - The aim of this paper is to build a formal model of a workflow, which allows a graphical representation of the associated workflow focused on fleet management and monitoring. Our major objective is to develop a mobile solution based on workflow management system (WfMS) approach, and GIS monitoring in real time. We develop a WfMS model functional model for transport negotiation and vehicle monitoring process modeling, construct based on two pools: vehicle driver and transport monitor. Main requirements can be grouped in two parts: monitoring fleet and management optimization. The WfMS was implemented based on development model on middleware layers and remote communication Using MultiMap as a digital globe, embedded in distributed application, FM4100 module, AVL packets, TCP/IP protocol, sockets.
SN  - 978-960-89832-7-4
PY  - 2009
SP  - 169
EP  - 172
AN  - WOS:000273225100043
ER  -

TY  - JOUR
AU  - Liang, BS
AU  - Chen, JN
AU  - Wang, FJ
TI  - A project model for software development
T2  - JOURNAL OF INFORMATION SCIENCE AND ENGINEERING
AB  - Uncertainty and dynamic changes in a software project cause iterations during development and the need for decision-making in planning and controlling a project. This paper presents a Software Project Review and Evaluation Model, SPREM, a superset of CPM/PERT which extends CPM/PERT's notation to four types of vertices (AA, AX, XA, and XX vertices) to express the non-deterministic and iterative behaviors of software engineering projects. Several behavioral properties of SPREM and analysis of them are discussed. For example, the enaction capability can be used to evaluate the possibility that a vertex will enact processes beforehand. Project managers can revise a SPREM graph to rescue dead vertices before project execution. Furthermore, enaction ordering allows project managers to calculate the dependency between processes to be enacted. This might help in computing important information such as critical paths among these processes.
SN  - 1016-2364
DA  - MAY
PY  - 2000
VL  - 16
IS  - 3
SP  - 423
EP  - 446
AN  - WOS:000088130200007
ER  -

TY  - CPAPER
AU  - Thom, LH
AU  - Lazarte, IM
AU  - Lochpe, C
AU  - Priego, LM
AU  - Verdier, C
AU  - Chiotti, O
AU  - Villarreal, PD
ED  - Dijkman, R
ED  - Hofstetter, J
ED  - Koehler, J
TI  - On the Capabilities of BPMN for Workflow Activity Patterns Representation
T2  - BUSINESS PROCESS MODEL AND NOTATION (BPMN 2011)
CP  - 3rd International Workshop on Business Process Model and Notation
AB  - This paper provides a complete version of the Workflow Activity Patterns (WAP) in the Business Process Modeling Notation (BPMN) as well as an extended evaluation of the capabilities of BPMN and their strengths and weaknesses when being utilizing for representing WAPs. When implementing the activity patterns in existing Business Process Modeling tools. it is fundamental to represent them in BPMN. This representation may facilitate the adoption of the WAPs by BPMN tools as well as the use of the WAPs in process design.
SN  - 1865-1348
SN  - 978-3-642-25159-7
PY  - 2011
VL  - 95
SP  - 172
EP  - +
AN  - WOS:000301959000018
ER  -

TY  - CPAPER
AU  - Gattnar, E
AU  - Ekinci, O
AU  - Detschew, V
ED  - Helfert, M
ED  - Ge, M
ED  - Pires, LF
ED  - Hammoudi, S
ED  - Toumani, F
ED  - Monfort, V
ED  - Jaziri, W
ED  - CapelTunon, MI
ED  - Li, CP
TI  - Event-based Workflow Analysis in Healthcare
T2  - INFORMATION VALUE MANAGEMENT, FUTURE TRENDS OF MODEL-DRIVEN DEVELOPMENT, RECENT TRENDS IN SOA BASED INFORMATION SYSTEMS AND MODELLING AND SIMULATION, VERIFICATION AND VALIDATION OF ENTERPRISE INFORMATION SYSTEMS
CP  - Int Joint workshop on Informat Value Management, Future Trends of Model-Driven Dev, Recent Trends in SOA Based Informat Syst and Modelling
AB  - Efficiency and high quality of care are nowadays demanded by healthcare organizations. Clinical workflow analysis is an important element of quality.improVement efforts in hospitals. Standardized process models can support process monitoring and mining in healthcare delivery. Therefore our paper presents a promising approach for measuring Key Performance Indicators (KPIs) in hospitals based on a newly developed clinical process model. In order to measure process performance, it addresses the entire treatment process. As standardized method, the model allows the evaluation and analysis of clinical workflow metrics and thus forms the basis for continuous quality improvement. For modeling purposes, we use Event-driven Process Chains (EPC), a common process modeling notation, and the ARTS Toolset. Our process approach allows the analysis and evaluation of specific clinical workflows and furthermore forms the basis for continuous performance measurement, clinical quality improvement and a sustainable risk reduction in health care.
SN  - 978-989-8425-60-7
PY  - 2011
SP  - 61
EP  - 70
AN  - WOS:000396646400007
ER  -

TY  - CPAPER
AU  - Börger, E
ED  - Parent, C
ED  - Schewe, KD
ED  - Storey, VC
ED  - Thalheim, B
TI  - Modeling workflow patterns from first principles
T2  - CONCEPTUAL MODELING - ER 2007, PROCEEDINGS
CP  - 26th International Conference on Conceptual Modeling
AB  - We propose a small set of parameterized abstract models for workflow patterns, starting from first principles for sequential and distributed control. Appropriate instantiations yield the 43 workflow patterns that have been listed recently by the Business Process Modeling Center. The resulting structural classification of those patterns into eight basic categories, four for sequential and four for parallel workflows, provides a semantical foundation for a rational evaluation of workflow patterns.
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-540-75562-3
PY  - 2007
VL  - 4801
SP  - 1
EP  - 20
AN  - WOS:000251544000001
ER  -

TY  - CPAPER
AU  - Zhang, B
AU  - Zhang, RB
ED  - Zhao, C
ED  - Wu, C
ED  - Wang, Y
ED  - Liu, Q
TI  - Research on Fuzzy-Grey Comprehensive Evaluation of Software Process Modeling Methods
T2  - KAM: 2008 INTERNATIONAL SYMPOSIUM ON KNOWLEDGE ACQUISITION AND MODELING, PROCEEDINGS
CP  - International Symposium on Knowledge Acquisition and Modeling
AB  - Large numbers of practices show that software process modeling technique can be effectively used to describe and analyze software process and offer opportunity to discover process improvements. There are many kinds of software process modeling method, so it is significant to make reasonable evaluation of software process modeling methods which can help developers choose the most appropriate modeling method according to specific modeling environment and requirement for achieving the best modeling effect. On the basis of evaluation system of software process modeling methods, comprehensive evaluation method which combines fuzzy evaluation and grey theory is applied to evaluate some modeling methods. It can make full use of fuzziness and grayness of evaluation information by experts which makes the evaluation more objective and accurate. Further, it is proved that the method can estimate modeling methods soundly and have feasibility and engineering value to software development project practice by an example.
SN  - 978-0-7695-3488-6
PY  - 2008
SP  - 754
EP  - 758
DO  - 10.1109/KAM.2008.139
AN  - WOS:000263156800155
ER  -

TY  - JOUR
AU  - Ferilli, S
TI  - WoMan: Logic-Based Workflow Learning and Management
T2  - IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS
AB  - Workflow management is fundamental to efficiently, effectively, and economically carry out complex working and domestic activities. Manual engineering of workflow models is a complex, costly, and error-prone task. The WoMan framework for workflow management is based on first-order logic. Its core is an automatic procedure that learns and refines workflow models from observed cases of process execution. Its innovative peculiarities include incrementality (allowing quick learning even in the presence of noise and changed behavior), strict adherence to the observed practices, ability to learn complex conditions for the workflow components, and improved expressive power compared to the state of the art. This paper presents the entire algorithmic apparatus of WoMan, including translation and learning from a standard log format for case representation, import/export of workflow models from/into standard formalisms (Petri nets), and exploitation of the learned models for process simulation and monitoring. Qualitative and quantitative experimental evaluation shows the power and efficiency of WoMan, both in controlled and in real-world domains.
SN  - 2168-2216
SN  - 2168-2232
DA  - JUN
PY  - 2014
VL  - 44
IS  - 6
SP  - 744
EP  - 756
DO  - 10.1109/TSMC.2013.2273310
AN  - WOS:000337907100007
ER  -

TY  - CPAPER
AU  - Ochara, NM
AU  - Nawa, EL
AU  - Fiodorov, I
AU  - Lebedev, S
AU  - Sotnikov, A
AU  - Telnovl, Y
AU  - Kadyamatimba, A
ED  - Ouma, C
TI  - Digital Transformation of Enterprises: A Transition Using Process Modelling Antecedents
T2  - 2018 OPEN INNOVATIONS CONFERENCE (OI)
CP  - Open Innovations Conference (OI) - Re-Creating Society through Disruptive Digital Transformation
AB  - Process modeling as one of the critical Business Process Management (BPM) disciplines is recognized as a key anchor to digital transformation (DT) efforts currently underway in most enterprises. While enterprises are transforming digitally, the practice of process modeling is also changing with little guidance on key considerations for realizing success process modeling. The focus of the paper was to determine key antecedents for realizing effective processing modeling during DT projects. A case study of a bank transforming its Customer Relationship Management (CRM) was used as a basis to "tease out" key factors critical for successful DT. A number of inferences emanate from the findings of this paper. The first is linked to the increasingly knowledge-based orientation and digitalization of our society that requires a re-think of how current organizations are structured and modelled. The notion of "Digitalization of Project Management" also emerged to undergird current transformations in project management, which affects how process modelling is currently practiced. Secondly, the implications of such digital transitions in process modeling is that the process of organizational modeling and structuring is encouraging more "Knowledge-Enabled Participation", with its emphasis on expertise, team structure and competence of project participants. The last inference conflates the factors of "DT Evaluation" and "DT Modeling" that emerged from this study to make the following claim: that the process of organizational transformation will increasingly be "machine-based", with process modeling and evaluation becoming predominantly digitalized. Therefore, process modeling and its foundational disciplines (such as project management and business process management) will transform, with an orientation elevating older terminologies such as automation and data-intensive processing; with organizational strategy intricately linked to process modeling as its anchor.
SN  - 978-1-5386-5318-0
PY  - 2018
SP  - 325
EP  - 331
AN  - WOS:000495074900051
ER  -

TY  - BOOK
AU  - Pichler, H
AU  - Eder, J
ED  - Embley, DW
ED  - Thalheim, B
TI  - Business Process Modeling and Workflow Design
T2  - HANDBOOK OF CONCEPTUAL MODELING: THEORY, PRACTICE AND RESEARCH CHALLENGES
AB  - Detailed knowledge about the structure and functionality of a business process within an enterprise is of utter importance for a thorough understanding of organizational sequences. This is a crucial requirement in business process management (BPM) and business process re-engineering (BPR), which cover the entire process lifecycle, from modeling and design, to execution, monitoring, and optimization. Throughout this lifecycle, process models are required to represent an enterprise's processes, so that they can be documented, communicated, verified, simulated, analyzed, automated, evaluated, or improved. This chapter provides an overview of business process modeling and workflow design, discusses their commonalities and differences, explains how different process perspectives are modeled, and gives an overview of several business process modeling related research topics.
SN  - 978-3-642-15864-3
PY  - 2011
SP  - 259
EP  - 286
DO  - 10.1007/978-3-642-15865-0
AN  - WOS:000290247300008
ER  -

TY  - CPAPER
AU  - Golra, FR
AU  - Dagnat, F
AU  - Bendraou, R
AU  - Beugnard, A
ED  - Ouhammou, Y
ED  - Ivanovic, M
ED  - Abello, A
ED  - Bellatreche, L
TI  - Continuous Process Compliance Using Model Driven Engineering
T2  - MODEL AND DATA ENGINEERING (MEDI 2017)
CP  - 7th International Conference on Model and Data Engineering (MEDI)
AB  - Software development methods and standards have existed for decades and the software industry is often expected to follow them, especially when it comes to critical systems. They are of vital importance for establishing a common frame of reference and milestones for software life-cycle planning, development, monitoring and evaluation. However, there is hardly any (semi-)automatic method that ensures the compliance of de-facto processes to the adopted de-jure standards throughout the development life cycle i.e. from specification to enactment. We argue that compliance assurance should be dealt by the process modeling methodologies implicitly to facilitate correct by construction approach for process development. This article presents a framework for modeling software development processes that ensures their continuous compliance to an adopted standard from specification to execution.
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-319-66854-3
SN  - 978-3-319-66853-6
PY  - 2017
VL  - 10563
SP  - 42
EP  - 56
DO  - 10.1007/978-3-319-66854-3_4
AN  - WOS:000439935200004
ER  -

TY  - CPAPER
AU  - Nagy, DA
AU  - Rudas, IJ
AU  - Haidegger, T
A1  - IEEE
TI  - OntoFlow, a Software Tool for Surgical Workflow Recording
T2  - 2018 IEEE 16TH WORLD SYMPOSIUM ON APPLIED MACHINE INTELLIGENCE AND INFORMATICS (SAMI 2018): DEDICATED TO THE MEMORY OF PIONEER OF ROBOTICS ANTAL (TONY) K. BEJCZY
CP  - IEEE 16th World Symposium on Applied Machine Intelligence and Informatics (SAMI) Dedicated to the Memory of Pioneer of Robotics Antal (Tony) K. Bejczy
AB  - Surgical Process Modeling is a growing field of biomedical data science, aiming to create and support context aware surgical systems. As a part of it, novel research intends to provide standardized, formal description of surgical processes. Surgical workflow recordings based on ontologies can provide objective measurements of surgical skill, thus standardizing surgical performance. Comparing the operational phase to the calculated optimal process could allow for new, context aware surgical training, evaluation and assistant systems. In this paper, we present a new software tool, named OntoFlow, developed to record ontology-based surgical workflow during the clinical practice, with post-event editing and reviewing capabilities. OntoFlow directly accesses the background ontology, therefore it can speed up the process of ontology development. As a surgical workflow reviewing software it can also be used as a training tool for surgical residents.
SN  - 978-1-5386-4772-1
PY  - 2018
SP  - 119
EP  - 123
AN  - WOS:000458546000024
ER  -

TY  - CHAP
AU  - Vasko, M
AU  - Dustdar, S
ED  - Zhang, LJ
ED  - Jeckle, M
TI  - An analysis of web services workflow patterns in Collaxa
T2  - WEB SERVICES, PROCEEDINGS
CP  - European Conference on Web Services (ECOWS 2004)
AB  - Web services have a substantial impact on today's distributed software systems, especially on the way they are designed and composed. Specialization of different services is leading to a multitude of applications ultimately providing complex solutions. The interaction and modeling aspects of Web services is increasingly becoming important. Based on the needs for Web services conversations, process modeling, and composition, a variety of languages and technologies for Web services composition have evolved. This case study is focused on a systematic evaluation of the support for workflow patterns and their BPEL (Business Process Execution Language for Web Services) implementation in Collaxa, a leading BPEL process modeling and enactment engine for Web services processes.
SN  - 0302-9743
SN  - 1611-3349
SN  - 3-540-23202-8
PY  - 2004
VL  - 3250
SP  - 1
EP  - 14
AN  - WOS:000224322700001
ER  -

TY  - JOUR
AU  - Goldstein, A
AU  - Johanndeiter, T
AU  - Frank, U
TI  - Business process runtime models: towards bridging the gap between design, enactment, and evaluation of business processes
T2  - INFORMATION SYSTEMS AND E-BUSINESS MANAGEMENT
AB  - Business process management (BPM) broadly covers a lifecycle of four distinct phases: design, configuration, enactment, and analysis and evaluation. Most BPM tool suites impose a strict separation between these phases, i.e., in each phase different languages and tools are used and the transition between phases is indirect and costly. This paper presents an environment for integrating all phases of the BPM lifecycle in which business process (BP) types and their instances can be modeled, visualized, managed and automatically synchronized, using a shared representation of models and code. The environment extends the capabilities of BP models to be used not only for specifying BPs but also for: (1) enactmentcreating instance objects that capture BP operational data; (2) monitoring BP instances as they progress; (3) visualizing performance indicators of executed BPs at runtime; and (4) navigating from a BP type model to its respective instance population. As opposed to existing tools, the proposed environment does not require regenerating the workflow schema when BP designs change, nor does it require additional adaptations to support monitoring. Thereby, we facilitate a continuous and dynamic BPM environment, where workflow specifications can be changed at runtime. Our solution integrates a meta-programming language called eXecutable Modeling Facility (XMF) and the multi-perspective enterprise modeling framework (MEMO).
SN  - 1617-9846
SN  - 1617-9854
DA  - MAR
PY  - 2019
VL  - 17
IS  - 1
SP  - 27
EP  - 64
DO  - 10.1007/s10257-018-0374-2
AN  - WOS:000467485100002
ER  -

TY  - JOUR
AU  - Sampathkumaran, PB
AU  - Wirsing, M
TI  - Financial Evaluation and Optimization of Business Processes
T2  - INTERNATIONAL JOURNAL OF INFORMATION SYSTEM MODELING AND DESIGN
AB  - Designing and optimizing a Business Process based on its financial parameters is a challenging task which requires well defined approaches, actions and recommendations which when implemented lead to tangible and quantifiable results. In this paper the authors consider Business Processes represented through the Business Process Modeling Notation with their Costs evaluated through a pattern based methodology. Using this concept of Cost calculation the authors analyze the effect of different well known best practices on the financial parameters of the process. In this study the authors also evaluate the impact of each task in a process on the overall Cost through Sensitivity Analysis leading to a structured approach to parameter variation to achieve financial optimization. The study briefly introduces the Business Process Modeling Notation, Workflow Patterns, and available Performance Measures Evaluation Techniques and recommends an adaptation of Devils Quadrangle suitable for impact evaluation.
SN  - 1947-8186
SN  - 1947-8194
DA  - APR-JUN
PY  - 2013
VL  - 4
IS  - 2
SP  - 91
EP  - 120
DO  - 10.4018/jismd.2013040105
AN  - WOS:000214901200006
ER  -

TY  - CPAPER
AU  - Wang, WS
AU  - Yu, TB
AU  - Zhou, J
AU  - Zhang, Y
A1  - IEEE
TI  - Study on Project Management System Oriented Collaborative Design
T2  - 2009 7TH IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS, VOLS 1 AND 2
CP  - 7th IEEE International Conference on Industrial Informatics
AB  - To meet the requirements of the project management in collaborative design environment, the collaborative design task execution mechanism of task decomposition and allocation based on product structure tree is studied, and the task progress monitoring mechanism which is implemented by recursive weight statistical method is proposed to carry out the task progress control in the collaborative design execution processes. The product collaborative design process is taken as an example to analyze the method of process modeling by jBPM in detail. A project management prototype system based on B/S mode is structured based on MVC design mode by Java technology to verify the correctness of theoretical research and the feasibility of technology.
SN  - 1935-4576
SN  - 978-1-4244-3759-7
PY  - 2009
SP  - 295
EP  - 300
DO  - 10.1109/INDIN.2009.5195820
AN  - WOS:000274890100050
ER  -

TY  - CPAPER
AU  - Li, Y
AU  - Feng, YQ
ED  - Duserick, FG
TI  - Business process mining in BPMS
T2  - FOURTH WUHAN INTERNATIONAL CONFERENCE ON E-BUSINESS: THE INTERNET ERA & THE GLOBAL ENTERPRISE, VOLS 1 AND 2
CP  - 4th Wuhan International Conference on E-Business - Global Business Interface
AB  - Business Process Management System (BPMS) is a software platform extending Workflow Management System (WFMS) that support the definition, analysis, optimization, execution, monitoring and control, and tracking of business processes. At the stage of definition, the traditional process modeling method is time-consuming, costly, error prone and in most cases requires the use of experts. Sometimes the resulting modeling is rigid and not fit for the actual business process. For solving these problems, a novel process modeling idea, business process mining, has arisen. This new modeling method distills a structured process description from a set of process execution logs recorded by BPMS and models the process description with some modeling language. This paper interprets the main reasons that the traditional modeling method is inapplicable, based on which a novel process modeling idea, business process mining is introduced. For formally explaining and stimulating the mining process, this paper develops a simple mining algorithm integrating with Markov matrix. The algorithm constructs a Markov matrix from workflow logs, deduces the relations between events by Markov matrix and at last we can model the workflow process based on the relations. Now process mining is becoming an interesting area in BPMS.
PY  - 2005
SP  - 264
EP  - 269
AN  - WOS:000230520800040
ER  -

TY  - JOUR
AU  - Zhuge, H
AU  - Cheung, TY
AU  - Pung, HK
TI  - A timed workflow process model
T2  - JOURNAL OF SYSTEMS AND SOFTWARE
AB  - An internet-based workflow management system (WfMS) enables business participants to work co-operatively at sites belonging to different time zones. Time-related factors have to be incorporated into the traditional workflow processes so as to adapt to the globally distributed applications. This paper proposes a timed workflow process model through incorporating the time constraints, the duration of activities, the duration of Row, and the activity distribution with respect to the multiple time axes into the conventional workflow processes. The model provides an approach for temporal consistency checking during both build-time and runtime. The proposed model and approach provide a vehicle for global business process modeling, planning and monitoring. (C) 2001 Elsevier Science Inc. All rights reserved.
SN  - 0164-1212
SN  - 1873-1228
DA  - JAN 15
PY  - 2001
VL  - 55
IS  - 3
SP  - 231
EP  - 243
DO  - 10.1016/S0164-1212(00)00073-X
AN  - WOS:000166218600002
ER  -

TY  - JOUR
AU  - Li, SY
AU  - Song, BH
TI  - Normalized workflow net (NWF-net): Its definition and properties
T2  - FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE
AB  - Workflow net, based on Petri nets' characteristics, is powerful and used widely. However, the workflow net itself cannot guarantee the soundness for its defined models; and what is more, the coming analysis on the workflow net models are difficult. In this paper, based on the structure of the classical workflow net, it introduces a new kind of net for modeling business processes, named normalized workflow net (NWF-net). NWF-net, which can be represented by a context-free language, can effectively avoid those structure problems occurred in the classical workflow net by removing some possible unnatural patterns. NWF-net satisfies all the required properties needed for workflow process modeling while keeping a strongly modeling capability, and the computational performance analysis, such as the expected completion time, becomes simple. (c) 2005 Elsevier B.V. All rights reserved.
SN  - 0167-739X
SN  - 1872-7115
DA  - JUL
PY  - 2005
VL  - 21
IS  - 7
SP  - 1004
EP  - 1014
DO  - 10.1016/j.future.2005.02.003
AN  - WOS:000230288700003
ER  -

TY  - CPAPER
AU  - Fan, YS
AU  - Liu, B
ED  - Xu, LD
TI  - Architecture and key technology of WFMS in service-oriented environment
T2  - FRONTIERS IN ENTERPRISE INTEGRATION
CP  - 2nd International Conference on Research and Practical Issues of Enterprise Information Systems
AB  - In Service-Oriented Architecture, service and workflow are close related: workflow can be constituted with service, and workflow itself can be encapsulated into service as well. The loosely coupled, autonomous and dynamic nature of service results in new challenges and characteristics of workflow management system (WFMS). Thus a new workflow paradigm, Service-Oriented Workflow (SOWF), comes up into being. With a motivating scenario, the characteristics and definition of workflow in service-oriented environments are presented. Then the meta-model and architecture of SOWF are proposed from both the build-time and run-time perspectives, based on which the key technologies for SOWF are discussed respectively, including process modeling, service matching and composition, dynamic workflow engine, workflow implementation technologies and performance evaluation.
SN  - 978-0-415-45779-8
PY  - 2008
SP  - 13
EP  - 20
AN  - WOS:000251914900003
ER  -

TY  - JOUR
AU  - Prasad, B
AU  - Wang, FJ
AU  - Deng, JT
TI  - A concurrent workflow management process for integrated product development
T2  - JOURNAL OF ENGINEERING DESIGN
AB  - This paper describes a systematic concurrent workflow management (WM) process for integrated product development. WM consists of planning and scheduling teams' activities to support cooperative and concurrent works. This paper first explains process re-engineering, flowcharting and various workflow practices in concurrent engineering (CE) to come up with a general process for WM. The WM process is based on an information infrastructure containing models of product requirements, enterprise organization and resources, including the workflow activity. Finally, technologies supporting WM such as work process modeling, performance analyzing, process re-engineering strategies to redesign the process and activity management-real-time task's monitoring-are introduced.
SN  - 0954-4828
SN  - 1466-1837
DA  - JUN
PY  - 1998
VL  - 9
IS  - 2
SP  - 121
EP  - 135
DO  - 10.1080/095448298261589
AN  - WOS:000075297800002
ER  -

TY  - JOUR
AU  - Zhou, H
AU  - Lin, C
AU  - Meng, K
AU  - Chen, YR
TI  - Stochastic Workflow Nets Based Workflow Pattern Modeling
T2  - CHINESE JOURNAL OF ELECTRONICS
AB  - Petri nets are widely used in describing workflow models of management systems, and also functioning in process management and resource organizing. The number of workflow patterns are increasing along with the demands of business process modeling. The Petri net and its extensions can no longer meet the requirement of model description, due to the limitation of their semantics. The models of some patterns are very sophisticated and even grow exponentially in complexity. Also the performance of these new patterns are not well studied. This paper extends Petri nets to Stochastic workflow nets (SWNs) and introduces stochastic or state related variables into arc weights, transition enabling guard and execution time. With the new features, SWNs can describe models more accurately and flexibly. And performance of common patterns is calculated under the assumption of exponentially distributed time. The paper also illustrates how to simplify the model with equivalent patterns.
SN  - 1022-4653
SN  - 2075-5597
DA  - JAN
PY  - 2014
VL  - 23
IS  - 1
SP  - 6
EP  - 12
AN  - WOS:000330089800002
ER  -

TY  - CPAPER
AU  - Tang, FG
AU  - Yao, SZ
AU  - Bai, JX
ED  - Li, M
TI  - Research of dynamic simulation technique for concurrent software on OPN
T2  - OBJECT-ORIENTED TECHNOLOGY
CP  - 27th International Conference on Technology of Object-Oriented Languages and Systems/2nd National Conference on Object-Oriented Technology Application
AB  - The performance evaluation for concurrent software is one of the important methods for software development quality improving. This paper mainly discusses applying the combination of Object-Orientation and Petri Nets into system modeling of concurrent software development slid the simulation technology for concurrent software systems based on Object Petri Nets (OPN).
SN  - 7-80003-434-8
PY  - 1998
SP  - 18
EP  - 23
AN  - WOS:000080088100004
ER  -

TY  - JOUR
AU  - Voutetakis, SS
AU  - Lappas, AA
AU  - Iatridis, DK
AU  - Vasalos, IA
TI  - Computer application and software development for the automation of a fluid catalytic cracking pilot plant - Experimental results
T2  - COMPUTERS & CHEMICAL ENGINEERING
CP  - European Symposium on Computer Aided Process Engineering - 6 (ESCAPE-6)
AB  - This application deals with the automation and computer aided operation including monitoring and control of a pilot plant scale fluid catalytic cracking unit. The operation and monitoring of the pilot plant is carried out through graphical process flowsheets of the actual process which are presented on the screen of a personal computer. All operational results and all manipulating actions are logged and stored in a time sequential indexed file that can be recalled at any time. Through the use of automation and computer software a complex unit such the FCC unit can be operated unattended in long runs giving useful results used for process modeling and model evaluation, catalyst and process condition evaluation. Real experimental results will show the trends and the good comparison of the unit operation to the FCC theoretical estimations.
SN  - 0098-1354
SN  - 1873-4375
PY  - 1996
VL  - 20
SP  - S1601
EP  - S1606
DO  - 10.1016/0098-1354(96)00272-4
AN  - WOS:A1996UR31000137
ER  -

TY  - CPAPER
AU  - Kosinár, MA
AU  - Czopik, J
AU  - Stolfa, J
ED  - Welzer, T
ED  - Jaakkola, H
ED  - Thalheim, B
ED  - Kiyoki, Y
ED  - Yoshida, N
TI  - Formal Knowledge Framework for Software Processes Architecture
T2  - INFORMATION MODELLING AND KNOWLEDGE BASES XXVII
CP  - 25th International Conference on Information Modelling and Knowledge Bases (EJC)
AB  - Last decades have introduced different improvements into software process modeling yet none has proven itself as a silver bullet; software development community has proposed various solutions from rigid prescriptive processes to agile methods, in the end, however, every good software process implementation require process modeling that can be used for different purposes like process auditing, analysis, and evaluation. This paper discusses application of explicit knowledge profiles based on process meta-model within software process modeling, alignment with visual process modeling, and further analysis with simulation and reverse engineering methods.
SN  - 0922-6389
SN  - 1879-8314
SN  - 978-1-61499-611-8
SN  - 978-1-61499-610-1
PY  - 2016
VL  - 280
SP  - 42
EP  - 56
DO  - 10.3233/978-1-61499-611-8-42
AN  - WOS:000385790100004
ER  -

TY  - CPAPER
AU  - Winkler, D
AU  - Schönbauer, M
AU  - Biffl, S
ED  - Rabiser, R
ED  - Torkar, R
TI  - Towards Automated Process and Workflow Management: A Feasibility Study on Tool-Supported and Automated Engineering Process Modeling Approaches
T2  - 2014 40TH EUROMICRO CONFERENCE SERIES ON SOFTWARE ENGINEERING AND ADVANCED APPLICATIONS (SEAA 2014)
CP  - 40th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)
AB  - [Context] Engineering workflows and processes typically include a wide range of organization specific best-practices required for successful project execution. In contrast to business administration processes, engineering processes are highly specific and can include various heterogeneous engineering tools to be integrated in an overall process and project environment. [Challenge] High complexity and volatility of engineering processes require specific solutions to define, model, implement, and evaluate individual engineering process steps. In this paper we focus on the first step of process management, i.e., the process definition phase. Based on the well-established Business Process Management Notation (BPMN) various tools have been developed that aim at supporting process modeling and automation. The key question is to what extent different tools support process automation in a defined context, i.e., in heterogeneous engineering environments. [Method] Following a common tool evaluation approach, we developed an evaluation framework and conducted a feasibility study to investigate process modeling and automation capability of tools based on defined requirements and scenarios. [Results] The developed tool evaluation process was found useful for analyzing candidate process modeling tools. In addition, results of the evaluation process highlighted a set of specific tools that can support modeling and process automation. [Conclusions] The evaluation framework can enable benchmarking for process modeling tools and support (a) process engineers in selecting most valuable tools and (b) tool vendors to better understand complex process needs with respect to improving and/or extending current solutions.
SN  - 1089-6503
SN  - 978-1-4799-5795-8
PY  - 2014
SP  - 102
EP  - 110
DO  - 10.1109/SEAA.2014.60
AN  - WOS:000358153200016
ER  -

TY  - CPAPER
AU  - Yu, F
AU  - Guo, LP
AU  - Zhang, L
ED  - Cao, J
ED  - Liu, X
ED  - Ren, K
TI  - Process Modeling Leveraged by Workflow Structure and Running Logs Analysis
T2  - PROCESS-AWARE SYSTEMS
CP  - 2nd International Workshop on Process-Aware Systems (PAS)
AB  - The reality of big data opens up a new world for business process modeling. Omnipresent cases and workflow logs are getting accessible, which implies the chance to exploit important patterns hidden in them so as to cut down the modeling cost or to improve the quality of process models. To take the full advantage of big data in process-aware systems (PASs), we propose a novel business process modeling technique that leverages the modeling by cases and workflow logs analysis. It uses the average perceptron to analyze both of existing process structure of cases and co-occurrence relation of activities in workflow logs. In contrast to traditional manual efforts, it improves the performance significantly by recommending proved working patterns. Comparing to recent process mining strategies, it serves the modeling online with meaningful process segments. We evaluate our approach against a synthesis dataset (100 processes and 10,000 log items generated by the plugin PLG in ProM) and real data from public business processes (77 processes in the package Paul Fisher workflows for benchmarks PR and CA2 from the website myExperiment). The study reveals that 9.46 % improvement in precision can be gained by considering both case structure and log items in contrast to the structure only, or 5.94 % gaining in contrast to mere logs. Our evaluation validates the effectiveness of the proposed technique and efficiency when we applying it on real modeling scenarios.
SN  - 1865-0929
SN  - 978-981-10-1019-4
SN  - 978-981-10-1018-7
PY  - 2016
VL  - 602
SP  - 29
EP  - 39
DO  - 10.1007/978-981-10-1019-4_3
AN  - WOS:000376161800003
ER  -

TY  - JOUR
AU  - BANDINELLI, S
AU  - FUGGETTA, A
AU  - LAVAZZA, L
AU  - LOI, M
AU  - PICCO, GP
TI  - MODELING AND IMPROVING AN INDUSTRIAL SOFTWARE PROCESS
T2  - IEEE TRANSACTIONS ON SOFTWARE ENGINEERING
AB  - This paper discusses the problems that a software development organization must address in order to assess and improve its software processes, In particular, the authors are involved in a project aiming at assessing and improving the current practice and the quality manual of the Business Unit Telecommunications for Defense (BUTD) of a large telecommunications company, The paper reports on the usage of formal process modeling languages to detect inconsistencies, ambiguities, incompleteness, and opportunities for improvement of both the software process and its documentation.
SN  - 0098-5589
SN  - 1939-3520
DA  - MAY
PY  - 1995
VL  - 21
IS  - 5
SP  - 440
EP  - 454
DO  - 10.1109/32.387473
AN  - WOS:A1995QZ41900005
ER  -

TY  - CPAPER
AU  - Aerts, ATM
AU  - Blijenberg, J
AU  - Heemstra, FJ
AU  - Kusters, RJ
AU  - Somers, LJ
ED  - Budgen, D
ED  - Hoffnagle, G
ED  - Trienekens, J
TI  - Project performance indicator workbench (PPIW)
T2  - EIGHTH IEEE INTERNATIONAL WORKSHOP ON SOFTWARE TECHNOLOGY AND ENGINEERING PRACTICE INCORPORATING COMPUTER AIDED SOFTWARE ENGINEERING, PROCEEDINGS
CP  - 8th International Workshop on Software Technology and Engineering Practice (STEP 97)
AB  - A prototype has been developed of a generic tool set and accompanying methods that enable a customizable approach towards project tracking and benchmarking The underlying data model describes a generic project life cycle. The prototype has been applied to an iterative development life cycle in a number of field tests.
SN  - 0-8186-7840-2
PY  - 1997
SP  - 156
EP  - 164
DO  - 10.1109/STEP.1997.615474
AN  - WOS:A1997BJ34A00016
ER  -

TY  - JOUR
AU  - GALLE, J
TI  - APPLYING PROCESS MODELING
T2  - LECTURE NOTES IN COMPUTER SCIENCE
AB  - Software process modelling aims to represent the software development process is a coherent set of related steps. It intends to improve our understanding on the wide range of activities required for software development, and on the often complicated interactions between these activities.
   The major goal of process modelling is to get a better understanding on, to allow reasoning about, and to encourage agreeing upon the software development process. A software process assessment activity uses these improved insights to propose an action plan. Such an action plan is a revised process model. On top of this model, we propose then a software development environment automating parts of the model.
   This paper describes an effort to define a real life process model for software development in an ESA (European Space Agency) environment. It is part of a program to establish an ESSDE (European Space Software Development Environment) which contains tools supporting the ESA software process model.
SN  - 0302-9743
PY  - 1992
VL  - 635
SP  - 230
EP  - 236
AN  - WOS:A1992LF69600028
ER  -

TY  - CPAPER
AU  - Zapletal, M
AU  - van der Aalst, WMP
AU  - Russell, N
AU  - Liegl, P
AU  - Werthner, H
ED  - Eshuis, R
ED  - Grefen, P
ED  - Papadopoulos, GA
TI  - An Analysis of Windows Workflow's Control-Flow Expressiveness
T2  - ECOWS'09: PROCEEDINGS OF THE 7TH IEEE EUROPEAN CONFERENCE ON WEB SERVICES
CP  - 7th IEEE European Conference on Web Services
AB  - The Windows Workflow Foundation (WF) has been introduced as part of the .NET framework as a means of creating workflow-centric applications. Its intended field of application is broad, ranging from fat-client applications and web applications to enterprise application integration solutions. Unlike other approaches Windows Workflow supports two distinct approaches to workflow specification - sequential workflows and state machine workflows - which deal with fundamentally different types of business scenarios. To date there has been minimal investigation into its capabilities and limitations, especially with respect to the two different control-flow styles it offers. To remedy this, in this paper we present a rigorous analysis of Windows Workflow's ability to deal with common control-flow scenarios. As a framework for this evaluation we use the Workflow Patterns. Our analysis outlines the strength and shortcomings of Windows Workflow's control-flow expressiveness and compares it to BPEL and jBPM - two other popular approaches for the design and implementation of business processes in a service-oriented context.
SN  - 978-0-7695-3854-9
PY  - 2009
SP  - 200
EP  - +
DO  - 10.1109/ECOWS.2009.17
AN  - WOS:000275857700021
ER  -

TY  - CPAPER
AU  - Neumuth, T
AU  - Kaschek, B
AU  - Czygan, M
AU  - Goldstein, D
AU  - Strauss, G
AU  - Meixensberger, J
AU  - Burgert, O
ED  - Liu, BJ
ED  - Boonn, WW
TI  - Support of Surgical Process Modeling by using adaptive software user interfaces
T2  - MEDICAL IMAGING 2010: ADVANCED PACS-BASED IMAGING INFORMATICS AND THERAPEUTIC APPLICATIONS
CP  - Advanced PACS-based Imaging Informatics and Therapeutic Applications
AB  - Surgical Process Modeling (SPM) is a powerful method for acquiring data about the evolution of surgical procedures. Surgical Process Models are used in a variety of use cases including evaluation studies, requirements analysis and procedure optimization, surgical education, and workflow management scheme design.
   This work proposes the use of adaptive, situation-aware user interfaces for observation support software for SPM. We developed a method to support the modeling of the observer by using an ontological knowledge base. This is used to drive the graphical user interface for the observer to restrict the search space of terminology depending on the current situation.
   In the evaluation study it is shown, that the workload of the observer was decreased significantly by using adaptive user interfaces. 54 SPM observation protocols were analyzed by using the NASA Task Load Index and it was shown that the use of the adaptive user interface disburdens the observer significantly in workload criteria effort, mental demand and temporal demand, helping him to concentrate on his essential task of modeling the Surgical Process.
SN  - 0277-786X
SN  - 1996-756X
SN  - 978-0-8194-8029-3
PY  - 2010
VL  - 7628
C7  - 76280C
DO  - 10.1117/12.844094
AN  - WOS:000284752500010
ER  -

TY  - JOUR
AU  - Müller, S
AU  - Kahrs, LA
AU  - Gaa, J
AU  - Tauscher, S
AU  - Kluge, M
AU  - John, S
AU  - Rau, TS
AU  - Lenarz, T
AU  - Ortmaier, T
AU  - Majdani, O
TI  - Workflow assessment as a preclinical development tool: Surgical process models of three techniques for minimally invasive cochlear implantation
T2  - INTERNATIONAL JOURNAL OF COMPUTER ASSISTED RADIOLOGY AND SURGERY
AB  - PurposeMinimally invasive cochlear implant surgery is a challenging procedure due to high demands on accuracy. For clinical success, an according assistance system has to compete against the traditional approach in terms of risk, operating time and cost. It has not yet been determined what kind of system is the most suited. The purpose of this study is a proof of concept of surgical process modeling as a preclinical development tool and the comparison of workflow concepts for this new approach.MethodsThree preclinical systems (two stereotactic and one robotic) for minimally invasive cochlear implant surgery are compared using the method of surgical process modeling. All three systems were successfully tested with ex vivo human specimen to create minimally invasive surgical access to the cochlea. Those systems where chosen for comparison, because they represent three diverse approaches with different corresponding workflows for the same intervention. The experiments were used to create a process model for each system by recording the interventions.ResultsAll three conceptual systems developed by our group have shown their eligibility. The recorded process models provide a convenient method for direct comparison. Reduction in the surgical time has a higher impact on the process, than time that is needed for setting up a system beforehand. The stereotactic approaches have little preparation effort and are low cost in terms of hardware compared to the robotic approach, which in return is beneficial in terms of workload reduction for the surgeon.ConclusionSurgical process modeling is suitable for comparison of different assistant systems for minimally invasive cochlear implantation. The benefit of reduced trauma, compared to the traditional mastoidectomy, can now be assessed with consideration of the workflow of each technique. The process models enable an assessment in the regard of surgical time and workload.
SN  - 1861-6410
SN  - 1861-6429
DA  - AUG
PY  - 2019
VL  - 14
IS  - 8
SP  - 1389
EP  - 1401
DO  - 10.1007/s11548-019-02002-3
AN  - WOS:000475611100013
ER  -

TY  - JOUR
AU  - Cecconi, FR
AU  - Manfren, M
AU  - Tagliabue, LC
AU  - Ciribini, ALC
AU  - De Angelis, E
TI  - Probabilistic behavioral modeling in building performance simulation: A Monte Carlo approach
T2  - ENERGY AND BUILDINGS
AB  - The increased awareness on sustainability matters is contributing to the evolution of energy and environmental policies for the building sector at the EU level, oriented toward resource efficiency. There exist today several possible strategies to model building performance through the life cycle. The increase of available computational capacity and of data acquisition capability is opening new scenarios for practical applications, which can contribute to the reduction of the gap usually encountered between simulated and measured energy performance. This article aims to investigate an approach for probabilistic building performance simulation to be used across life cycle phases, employing reduced-order models for performance monitoring and energy management. The workflow proposed aims to establish a continuity among design and operation phases. Design phase simulation is generally subject to relevant temporal and economic constraints and a successful workflow should incorporate elements from current design practices but should also add new features, which have to be reasonably automated to reduce additional effort. Therefore, the workflow proposed is automated and tested for robustness using Monte Carlo technique. In the design phase, the approach can be used for identifying probabilistic performance bounds suitable for risk analysis in energy efficiency investments, employing cost-optimal or life cycle cost accounting methodologies. In the operation phase, it can be used for performance monitoring and energy management based on daily energy consumption analysis, similarly to other multivariate regression-based methods at the state of the art, addressing the problem of maintaining energy consumption and related costs constantly under control. (C) 2017 Elsevier B.V. All rights reserved.
SN  - 0378-7788
SN  - 1872-6178
DA  - AUG 1
PY  - 2017
VL  - 148
SP  - 128
EP  - 141
DO  - 10.1016/j.enbuild.2017.05.013
AN  - WOS:000404705000011
ER  -

TY  - BOOK
AU  - Armbrust, O
AU  - Berlage, T
AU  - Hanne, T
AU  - Lang, P
AU  - Münch, J
AU  - Neu, H
AU  - Nickel, S
AU  - Rus, I
AU  - Sarishvili, A
AU  - Van Stockum, S
AU  - Wirsen, A
ED  - Chang, SK
TI  - SIMULATION-BASED SOFTWARE PROCESS MODELING AND EVALUATION
T2  - HANDBOOK OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING, VOL 3: RECENT ADVANCES
AB  - Decision support for planning and improving software development projects is a crucial success factor. The special characteristics of software development aggregate these tasks in contrast to the planning of many other processes, such as production processes. Process simulation can be used to support decisions on process alternatives on the basis of existing knowledge. Thereby, new development knowledge can be gained faster and more cost effective.
   This chapter gives a short introduction to experimental software engineering, describes simulation approaches within that area, and introduces a method for systematically developing discrete-event software process simulation models. Advanced simulation modeling techniques will point out key problems and possible solutions, including the use of visualization techniques for better simulation result interpretation.
SN  - 978-9-81277-524-5
SN  - 978-9-81256-273-9
PY  - 2005
SP  - 333
EP  - 364
DO  - 10.1142/9789812775245_0012
DO  - 10.1142/9789812775245
AN  - WOS:000301198900013
ER  -

TY  - CPAPER
AU  - Molnár, B
AU  - Máriás, Z
A1  - IEEE
TI  - Design and Implementation of a Workflow Oriented ERP System
T2  - 2015 12TH INTERNATIONAL JOINT CONFERENCE ON E-BUSINESS AND TELECOMMUNICATIONS (ICETE), VOL 2
CP  - 12th International Joint Conference on E-Business and Telecommunications (ICETE)
AB  - Adaptation of enterprise resource planning systems to the frequently changing business environment and business processes require huge resources. That is why the demand was formulated for a method that enables introducing new features in software systems without any modification in program code. An adaptive ERP system cannot handle business processes and data flow as disjoint components. Therefore the proposed solution is bifocal: an adaptive ERP system with highly integrated data flow and document management. In this article design and programming challenges are shown that had to be met during the development, focusing on topics of effective data storage and queries, workflow control structures and workflow evaluation techniques, document representation and the connection of workflow and data flow.
SN  - 978-989-758-140-3
PY  - 2015
SP  - 160
EP  - 167
AN  - WOS:000449093800022
ER  -

TY  - CPAPER
AU  - Damiani, E
AU  - Mulazzani, F
AU  - Russo, B
AU  - Succi, G
ED  - Abramowicz, W
ED  - Fensel, D
TI  - SAF Strategic Alignment framework for monitoring organizations
T2  - BUSINESS INFORMATION SYSTEMS
CP  - 11th International Conference on Business Information Systems
AB  - Reaching a Strategic Alignment is a crucial aspect for any organization. The alignment can be achieved by controlling, through monitoring probes, the coherency of the Business Processes with the related Business Strategy. In this paper we present SAF, a powerful framework for those organizations that aim at a superior business performance and want to keep monitored the organization's alignment. SAF has been applied to a real case study and it has also been compared with GQM(+) Strategy [2] and Process Performance Indicators Monitoring Model [16].
SN  - 1865-1348
SN  - 978-3-540-79395-3
PY  - 2008
VL  - 7
SP  - 213
EP  - +
AN  - WOS:000255787300019
ER  -

TY  - CPAPER
AU  - Geambasu, CV
A1  - Editura ASE
TI  - BPMN VS. UML ACTIVITY DIAGRAM FOR BUSINESS PROCESS MODELING
T2  - PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ACCOUNTING AND MANAGEMENT INFORMATION SYSTEMS (AMIS 2012)
CP  - 7th International Conference on Accounting and Management Information Systems (AMIS)
AB  - During the last years, it has been noticed a growing interest of organizations in improving their business processes in order to be more competitive in a globalized economy that passes nowadays through a severe financial crisis with restrictive market conditions and limited profit margins. The first step in achieving this goal is to use an adequate business process modeling language to represent their business processes. For this purpose, an evaluation of the existing business process modeling languages would be very useful in making the right decision. Our research work comes to supplement the previous researches that have evaluated business process modeling languages. The evaluation performed in this paper is focused on the two most widely used graphical notations for business processes: Business Process Modeling and Notation (BPMN) and UML Activity Diagram (UML AD). The evaluation criteria are: capacity of being readily understandable, adequacy of the graphical elements of BPMN and UML AD to represent the real business processes of an organization and mapping to Business Process Execution Languages. The results of evaluating BPMN and UML AD against each of these three criteria are presented in the paper.
SN  - 2247-6245
PY  - 2012
SP  - 934
EP  - 945
AN  - WOS:000320770700061
ER  -

TY  - JOUR
AU  - Reisig, W
TI  - Remarks on Egon Borger: "Approaches to model business processes: a critical analysis of BPMN, workflow patterns and YAWL, SOSYM 11:305-318"
T2  - SOFTWARE AND SYSTEMS MODELING
AB  - Egon Borger (SOSYM, 11, pp. 305-318, 2012) challenges the concepts of BPMN, workflow patterns and YAWL as useful contributions to the modeling of business processes. I show that he misjudges the role of BPMN, YAWL and similar techniques in the modeling of business processes. In particular he mistakes YAWL's formal basis, i.e. Petri nets. Borger furthermore suggests evaluation criteria for business process modeling tools. I argue that his criteria overemphasize some less important aspects, while ignoring some decisive ones.
SN  - 1619-1366
DA  - FEB
PY  - 2013
VL  - 12
IS  - 1
SP  - 5
EP  - 9
DO  - 10.1007/s10270-012-0306-4
AN  - WOS:000314978800002
ER  -

TY  - JOUR
AU  - Hu, XM
AU  - Zhao, L
AU  - Li, WL
TI  - Petri Net-Based R&D Process Modeling and Optimization for Composite Materials
T2  - JOURNAL OF APPLIED MATHEMATICS
AB  - Considering the current R&D process for new composite materials involves some complex details, such as formula design, specimen/sample production, materials/sample test, assessment, materials/sample feedback from customers, and mass production, the workflow model of Petri net-based R&D process for new composite materials' is proposed. By analyzing the time property of the whole Petri net, the optimized model for new composite materials R&D workflow is further proposed. By analyzing the experiment data and application in some materials R&D enterprise, it is demonstrated that the workflow optimization model shortens the period of R&D on new materials for 15%, definitely improving the R&D efficiency. This indicates the feasibility and availability of the model.
SN  - 1110-757X
SN  - 1687-0042
PY  - 2013
C7  - 306704
DO  - 10.1155/2013/306704
AN  - WOS:000325901700001
ER  -

TY  - CPAPER
AU  - Campos, ALN
AU  - Oliveira, T
ED  - Heidrich, J
ED  - Oivo, M
ED  - Jedlitschka, A
ED  - Baldassarre, MT
TI  - Software Processes with BPMN: An Empirical Analysis
T2  - PRODUCT-FOCUSED SOFTWARE PROCESS IMPROVEMENT
CP  - 14th International Conference on Product-Focused Software Process Improvement (PROFES)
AB  - BPMN 2.0 is a widely used notation to model a business process that has associated tools and techniques to facilitate process management, execution and monitoring. As a result using BPMN in the context of Software Development Process (SDP) can improve by leverage on the BPMN's infrastructure to improve SDP quality. This work presents an analysis of BPMN notation in modeling software processes.
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-642-39259-7
SN  - 978-3-642-39258-0
PY  - 2013
VL  - 7983
SP  - 338
EP  - 341
AN  - WOS:000342989700029
ER  -

TY  - JOUR
AU  - Martin, R
AU  - Raffo, D
TI  - Application of a hybrid process simulation model to a software development project
T2  - JOURNAL OF SYSTEMS AND SOFTWARE
AB  - Simulation models of the software development process can be used to evaluate potential process changes. Careful evaluation should consider the change within the context of the project environment. While system dynamics models have been used to model the project environment, discrete event and state-based models are more useful when modeling process activities. Hybrid models of the software development process can examine questions that cannot be answered by either system dynamics models or discrete event models alone. In this paper, we present a detailed hybrid model of a software development process currently in use at a major industrial developer. We describe the model and show how the model was used to evaluate simultaneous changes to both the process and the project environment. (C) 2001 Elsevier Science Inc. All rights reserved.
SN  - 0164-1212
DA  - DEC 15
PY  - 2001
VL  - 59
IS  - 3
SP  - 237
EP  - 246
DO  - 10.1016/S0164-1212(01)00065-6
AN  - WOS:000173214800003
ER  -

TY  - CPAPER
AU  - Nagy, DA
AU  - Takacs, K
AU  - Rudas, IJ
AU  - Haidegger, T
A1  - IEEE
TI  - Employing Process Models for Surgical Training
T2  - 2020 IEEE 18TH WORLD SYMPOSIUM ON APPLIED MACHINE INTELLIGENCE AND INFORMATICS (SAMI 2020)
CP  - 18th IEEE World Symposium on Applied Machine Intelligence and Informatics (SAMI)
AB  - The exponential rise in minimally invasive procedures throughout the last three decades shifted the focus from individual manual skills to complex engineering solutions. To streamline the delivery of these novel techniques, Surgical Process Models (SPMs) have been under development. SPMs provide the basis for machine learning algorithms to frame the surgical procedure and anchor themselves into the workflow. Process recording is an essential tool to create an accurate representation of the SPM. Process recording, continued with human expert evaluation have been used to assess operator skills and compare interventional approaches. In this paper, we present a web-based surgical process recording tool which is evaluated in a surgical training scenario. Our aim is to involve the trainees in process recording, therefore actively exploring the generic process model of laparoscopic cholecystectomy. Along with training we also use the process records to identify the most accurately represented time points of process transitions, therefore providing target events for future monitoring systems.
SN  - 978-1-7281-3149-8
PY  - 2020
SP  - 101
EP  - 104
DO  - 10.1109/sami48414.2020.9108732
AN  - WOS:000589772600020
ER  -

TY  - JOUR
AU  - Schultheis, A
AU  - Jilg, D
AU  - Malburg, L
AU  - Bergweiler, S
AU  - Bergmann, R
TI  - Towards Flexible Control of Production Processes: A Requirements Analysis for Adaptive Workflow Management and Evaluation of Suitable Process Modeling Languages
T2  - PROCESSES
AB  - In the context of Industry 4.0, Artificial Intelligence (AI) methods are used to maximize the efficiency and flexibility of production processes. The adaptive management of such semantic processes can optimize energy and resource efficiency while providing high reliability, but it depends on the representation type of these models. This paper provides a literature review of current Process Modeling Languages (PMLs). Based on a suitable PML, the flexibility of production processes can be increased. Currently, a common understanding of this process flexibility in the context of adaptive workflow management is missing. Therefore, requirements derived from the business environment are presented for process flexibility. To enable the identification of suitable PLMs, requirements regarding this are also raised. Based on these, the PMLs identified in the literature review are evaluated. Thereby, based on a preselection, a detailed examination of the seven most promising languages is performed, including an example from a real smart factory. As a result, a recommendation is made for the use of BPMN, for which it is presented how it can be enriched with separate semantic information that is suitable for the use of AI planning and, thus, enables flexible control.
SN  - 2227-9717
DA  - DEC
PY  - 2024
VL  - 12
IS  - 12
C7  - 2714
DO  - 10.3390/pr12122714
AN  - WOS:001383880700001
ER  -

TY  - CPAPER
AU  - Rieger, C
ED  - Shishkov, B
TI  - Interoperability of BPMN and MAML for Model-Driven Development of Business Apps
T2  - BUSINESS MODELING AND SOFTWARE DESIGN, BMSD 2018
CP  - 8th International Symposium on Business Modeling and Software Design (BMSD)
AB  - With process models widely used as means for documentation and monitoring of business activities, the conversion into executable software often still remains a manual and time-consuming task. The MAML framework was developed to ease the creation of mobile business apps by jointly modeling process, data, and user interface perspectives in a graphical, process-oriented model for subsequent code generation. However, this domain-specific notation cannot benefit from existing process knowledge which is often encoded in BPMN models. The purpose of this paper is to analyze conceptual differences between both notations from a software development perspective and provide a solution for inter-operability through a model-to-model transformation. Therefore, workflow patterns identified in previous research are used to compare both notations. A conceptual mapping of supported concepts is presented and technically implemented using a QVT-O transformation to demonstrate an automated mapping between BPMN and MAML. Consequently, it is possible to simplify the automatic generation of mobile apps by reusing processes specified in BPMN.
SN  - 1865-1348
SN  - 978-3-319-94214-8
SN  - 978-3-319-94213-1
PY  - 2018
VL  - 319
SP  - 149
EP  - 166
DO  - 10.1007/978-3-319-94214-8_10
AN  - WOS:000465515000010
ER  -

TY  - CPAPER
AU  - Geambasu, CV
AU  - Jianu, I
A1  - Editura ASE
TI  - EVALUATION OF BPMN CAPACITY OF BEING READILY UNDERSTANDABLE BY BUSINESS PEOPLE
T2  - PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE ACCOUNTING AND MANAGEMENT INFORMATION SYSTEMS (AMIS 2013)
CP  - Proceedings of the 8th International Conference on Accounting and Management Information Systems (AMIS)
AB  - Analyzing and improving business processes help organizations streamline their activities and get better results. A graphical representation of business processes can be obtained using a notation language. Currently the most widely used notation language for business processes is Business Process Modeling Notation (BPMN). To achieve the objective of improving the business processes of an organization, the diagrams created using BPMN should be readily understood by both technical team and business people. The objective of our paper is to evaluate to what extent BPMN diagrams can be understood by business people that do not have prior knowledge on this notation language. In order to do that we have used an empirical research based on a questionnaire. The questions included in the questionnaire assess the extent to which the participants understand the BPMN notations used in BPMN diagrams to represent a series of workflow patterns commonly found in business processes.
SN  - 2247-6245
PY  - 2013
SP  - 474
EP  - 482
AN  - WOS:000340256300031
ER  -

TY  - JOUR
AU  - DEWAL, S
TI  - A METHODOLOGY FOR REQUIREMENTS ANALYSIS AND EVALUATION OF SDES
T2  - LECTURE NOTES IN COMPUTER SCIENCE
AB  - Nowadays software systems can be developed for nearly any purpose. For the development of such complex software systems appropriate software development environments (SDEs) are necessary as a ''paper and pencil'' development is intolerable. For a software producer the introduction of an SDE is part of the technology deployment process during which the staff must learn new methods, gain experience and knowledge by applying the SDE, etc. This paper focusses on a particular activity of the technology deployment process, namely the selection process (i.e. requirements analysis and evaluation of SDEs). The selection process must (1) produce selection results which are reproducible and comprehensive and (2) be repeatable and flexible in order to be applied for different software producers. We have developed a selection method which can be applied (1) for a thorough analysis of the requirements of the software producer, (2) for the evaluation of existing SDEs and (3) analyzing the evaluation results. For supporting the method we have developed the environment Requiem.
SN  - 0302-9743
SN  - 1611-3349
PY  - 1992
VL  - 593
SP  - 394
EP  - 409
AN  - WOS:A1992LF68100024
ER  -

TY  - CPAPER
AU  - Wei, L
AU  - Dai, P
AU  - Wang, IA
AU  - Liu, YL
ED  - Tang, TA
ED  - Zeng, XY
ED  - Chen, Y
ED  - Yu, HH
TI  - Architecture of SystemC Based Emulator for ReMAP
T2  - 2009 IEEE 8TH INTERNATIONAL CONFERENCE ON ASIC, VOLS 1 AND 2, PROCEEDINGS
CP  - IEEE 8th International Conference on ASIC
AB  - A SystemC based emulator architecture optimized for reconfigurable array processor is proposed. With novel architecture, the emulator support performance evaluation, rtl co-simulation as well as system software development. The emulator has been used in the development of a reconfigurable multimedia application processor (ReMAP) which has been successfully fabricated under SMIC 0. um processing.(1)
SN  - 978-1-4244-3868-6
PY  - 2009
SP  - 1255
EP  - +
DO  - 10.1109/ASICON.2009.5351215
AN  - WOS:000275924100314
ER  -

TY  - CPAPER
AU  - Carlsen, S
AU  - Krogstie, J
AU  - Solvberg, A
AU  - Lindland, OI
ED  - Nunamaker, JF
ED  - Sprague, RH
TI  - Evaluating flexible workflow systems
T2  - THIRTIETH HAWAII INTERNATIONAL CONFERENCE ON SYSTEM SCIENCES, VOL 2: INFORMATION SYSTEMS - COLLABORATION SYSTEMS AND TECHNOLOGY
CP  - 30th Hawaii International Conference on System Sciences (HICSS 30)
AB  - Active business process support utilizing workflow technology is promising for building information systems that are flexible regarding both business and corresponding IT infrastructure transformation. To achieve this, conceptual models of work processes have to be aligned with interests of several stakeholders, including business professionals and users. fit particular, user access to process models presuppose comprehensive in addition to expressive models. We propose a framework for evaluating quality in process modeling languages (PMLs) and models. It is applied to a representative sample of flexible workflow products and prototypes. We study the properties of the various products' underlying PMLs, and derive their first cut ontology, or meta-model. In carrying out the evaluation, we show, the applicability of our framework, and point out some research opportunities in areas where the products differ widely or carry similar deficiencies.
SN  - 1060-3425
SN  - 0-8186-7743-0
PY  - 1997
SP  - 230
EP  - 239
AN  - WOS:000073324400023
ER  -

TY  - JOUR
AU  - Montella, R
AU  - Brizius, A
AU  - Di Luccio, D
AU  - Porter, C
AU  - Elliot, J
AU  - Madduri, R
AU  - Kelly, D
AU  - Riccio, A
AU  - Foster, I
TI  - Using the FACE-IT portal and workflow engine for operational food quality prediction and assessment: An application to mussel farms monitoring in the Bay of Napoli, Italy
T2  - FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE
AB  - The Framework to Advance Climate, Economic, and Impact Investigations with Information Technology (FACE-IT) is a workflow engine and data science portal based on Galaxy and Globus technologies that enables computational scientists to integrate data, pre/post processing and simulation into a framework that supports offline environmental model coupling. We describe how the FACE-IT workflows engine can be used to couple many simulation/prediction models, leveraging high-performance cloud computing resources to enable fast full system modeling and produce operational predictions about the impact of pollutants spilled out from both natural and anthropic sources in mussels farming high density areas. Mussel farms product quality remains a challenging problem for operational marine science: in this scenario, the model chain presented in this work, orchestrated in a workflow fashion, produces a huge amount of predicted spatially-referenced (big) data. The software infrastructure we built using FACE-IT Galaxy Globus provides tools enabled to evaluate the impact of hazardous substances (chemical or biological) continuously or spottily spilled in the marine environment. (C) 2018 Elsevier B.V. All rights reserved.
SN  - 0167-739X
SN  - 1872-7115
DA  - SEP
PY  - 2020
VL  - 110
SP  - 453
EP  - 467
DO  - 10.1016/j.future.2018.03.002
AN  - WOS:000541153400041
ER  -

TY  - CPAPER
AU  - Gao, ZD
AU  - Tang, CT
AU  - Zhao, S
A1  - Hang, Y
A1  - Desheng, W
A1  - Sandhu, PS
TI  - Knowledge-based model to evaluate plastic forming process and forming quality of metal tubes
T2  - ICCSIT 2010 - 3RD IEEE INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND INFORMATION TECHNOLOGY, VOL 3
CP  - 3rd IEEE International Conference on Computer Science and Information Technology (ICCSIT)
AB  - An evaluation model of plastic forming process and forming quality of metal tubes is proposed by considering restrictions of manufacturing resource, geometry and forming quality. Three key technologies including evaluation index system, modeling of evaluation knowledge and evaluation workflow are studied in detail. Simulation method of bending process is introduced to study the machining character of the metal tubes. An evaluation software of tube bending process is developed based on the method. The achievements of the paper can be used to evaluate the feasibility of tube bending process fast and correctly.
SN  - 2381-3458
SN  - 978-1-4244-5537-9
PY  - 2010
SP  - 65
EP  - 69
AN  - WOS:000393185000015
ER  -

TY  - CPAPER
AU  - Domke, J
AU  - Wang, DL
ED  - Ali, H
ED  - Shi, Y
ED  - Khazanchi, D
ED  - Lees, M
ED  - VanAlbada, GD
ED  - Dongarra, J
ED  - Sloot, PMA
TI  - Runtime Tracing of the Community Earth System Model: Feasibility Study and Benefits
T2  - PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE, ICCS 2012
CP  - International Conference on Computational Science (ICCS)
AB  - The Community Earth System Model (CESM) is one of US's leading earth system modeling frameworks, which has decades of development history and was embraced by a large, active user community. In this paper, we first review the software development history of CESM and we explain the general objectives of performance analysis. Then we present an offline global community land model simulation within the CESM framework to demonstrate the procedure of runtime tracing of CESM using the Vampir toolset. Finally, we explain the benefits of runtime tracing to the general earth system modeling community. We hope those considerations can be beneficial to many other modeling research programs involving legacy high-performance computing applications.
SN  - 1877-0509
PY  - 2012
VL  - 9
SP  - 1950
EP  - 1958
DO  - 10.1016/j.procs.2012.04.213
AN  - WOS:000306288400212
ER  -

TY  - CPAPER
AU  - Silva, I
AU  - Alturas, B
AU  - Carneiro, A
A1  - IEEE
TI  - UML Modeling Tools: Assessment in Perspective of Users
T2  - 2017 12TH IBERIAN CONFERENCE ON INFORMATION SYSTEMS AND TECHNOLOGIES (CISTI)
CP  - 12th Iberian Conference on Information Systems and Technologies (CISTI)
AB  - Modeling tools serve to guide and discipline the software development process during the design process. However, such tools are not fully explored from a functional point of view, either the complexity of the subject matter or the usability thereof. This study proposes to evaluate the use of three system modeling tools, through a study using students of the Postgraduate Program in Applied Informatics to the Organizations. The objective of the study was to evaluate the usability of three tools that allow system modeling using UML notation. Are presented studies on the levels of acceptance and use of the analyzed tools. Based on the preliminary results of this study, it was observed that the Visio tool has better usability and functionality than the other tools.
SN  - 2166-0727
PY  - 2017
AN  - WOS:000426896900147
ER  -

TY  - JOUR
AU  - Aboutalebi, M
AU  - Parsa, S
TI  - QABPEM: Quality-Aware Business Process Engineering Method
T2  - INTERNATIONAL JOURNAL OF COOPERATIVE INFORMATION SYSTEMS
AB  - In this paper, a novel business process engineering method based on quality assessment is proposed. In the proposed method, a goal model is used to estimate the operational costs of business processes. Goals scenarios in the goal model of desired information systems are applied as a basis for estimating the design cost. Qualities of business requirements models and business process models are also estimated. Based on the quality metrics, the process of business process modeling is examined. Then, using XOR operator in the goal model, a simple and direct mapping of the goal model to the business process model is introduced. Common activities in the business process model are further factored and summarized using pre- and post-factoring operations. The proposed business process modeling method is language-independent. An ICT office in Mazandaran Power Distribution Company is used as a case study to exemplify QABPEM. Our evaluation results demonstrates the capability of the proposed method compared with the existing ones.
SN  - 0218-8430
SN  - 1793-6365
DA  - MAR
PY  - 2017
VL  - 26
IS  - 1
C7  - 1650011
DO  - 10.1142/S0218843016500118
AN  - WOS:000395945900003
ER  -

TY  - CPAPER
AU  - Liang, S
AU  - Shi, HY
AU  - Su, YY
AU  - Wang, WS
A1  - IEEE Computer Society
TI  - Research on Cooperative Design Process Modeling and Evaluation Method Oriented MAS
T2  - PROCEEDINGS OF THE 2008 INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE AND DESIGN, VOL 2
CP  - 1st International Symposium on Computational Intelligence and Design
AB  - The paper proposed a new thought that a multi-agent organization model was designed, which based on cooperative design system network framework and workflow. The system workflow and system architecture were presented. The Ant Colony Optimization (ACO) method which was improved by Bayes was applied to the partner selection. And the Principal Component Analysis (PCA) method was used to evaluate the design method of design tasks. The collaborative process of negotiation mechanism was discussed. According to the theory analysis, software SPSS was used to deal with the sample data. On the basis of the theory a network of collaborative design prototype system was developed which confirms the system is reliability, practicality and efficiency.
SN  - 978-0-7695-3311-7
PY  - 2008
SP  - 287
EP  - +
DO  - 10.1109/ISCID.2008.102
AN  - WOS:000263408100066
ER  -

TY  - JOUR
AU  - IIVARI, J
TI  - OBJECT-ORIENTATION AS STRUCTURAL, FUNCTIONAL AND BEHAVIORAL MODELING - A COMPARISON OF 6 METHODS FOR OBJECT-ORIENTED ANALYSIS
T2  - INFORMATION AND SOFTWARE TECHNOLOGY
AB  - The paper suggests a two-dimensional framework for understanding object-oriented modelling: depending on whether the viewpoint is that of an individual object or of an object community and on whether it is structure, function or behaviour that is addressed. After introducing the framework, it compares six methods for object-oriented analysis, showing considerable differences between them in functional and behavioural modelling. It also indicates that most of the methods are weak in modelling functionality at the level of object communities. It is argued that this is a considerable weakness in object-oriented analysis.
SN  - 0950-5849
DA  - MAR
PY  - 1995
VL  - 37
IS  - 3
SP  - 155
EP  - 163
DO  - 10.1016/0950-5849(95)98926-7
AN  - WOS:A1995QR83500003
ER  -

TY  - CPAPER
AU  - Bernroider, EWN
AU  - Bernroider, M
ED  - Chu, HW
ED  - Savoie, M
ED  - Ferrer, J
ED  - Franco, P
ED  - Estrems, M
TI  - A Comparative Study of Business Process Management Tools based on Open Source Software and a Commercial Reference
T2  - IMETI 2008: INTERNATIONAL MULTI-CONFERENCE ON ENGINEERING AND TECHNOLOGICAL INNOVATION, VOL I, PROCEEDINGS
CP  - International Multi-Conference on Engineering and Technological Innovation
AB  - This paper examines Business Process Management tools with a multiple attributive assessment framework based on the utility ranking and AHP method. It supplies a comprehensive list of eligible criteria for the assessment and provides a comparative analysis in terms of their features and development focus in relation to a commercial market leader (ARTS). Considering the age of the assessed OSS projects, their functionality for BPM is already impressive. The user innovations surfaced from the OSS tool analysis show that tools concentrate on model driven business process architectures. In the OSS world, the process and workflow models are seen as unified models. In the commercial world, workflow models are often referred to as an abstraction of the business process models concentrating on steps that can be automated. This work further supports the view at our research institution that OSS tools can already be used effectively in the class room in conjunction or even as alternative to commercial ones.
SN  - 978-1-934272-43-5
PY  - 2008
SP  - 225
EP  - +
AN  - WOS:000263828900044
ER  -

TY  - CPAPER
AU  - Ganguly, P
AU  - Ray, P
AU  - Lovell, N
ED  - Enderle, JD
TI  - Workflow-based approach towards distributed health care applications
T2  - PROCEEDINGS OF THE 22ND ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY, VOLS 1-4
CP  - 22nd Annual International Conference of the IEEE-Engineering-in-Medicine-and-Biology-Society
AB  - The provision of telemedicine services involves integration of information, humun-machine and health care technologies, As different modalities of patient care require applications running on heterogeneous computing environments, distributed health care applications are a major issue in telemedicine.
   A distributed application integrates resources located at distant places through networking. These resources include diagnostic instruments, medical expertise, hospital care and other related administrative services In health rare applications.
   A workflow based methodology fur development and implementation of distributed health care applications is presented, The workflow methodology involves process modeling, workflow design and implementation, and system evaluation,
   Using the specific example of a distributed telecardiology application, issues related to workflow management are reviewed and a design methodology to develop a distributed telecardiology system is proposed. The ECC process includes; data acquisition; encoding, transmission and storage; pattern recognition and feature extraction; and diagnostic classification. The key identified elements of the workflow implementation are; the point of need user; the assessor In tie online diagnostic center; ECC; knowledge repositories; online specialist and the further care center.
   As illustrated through the telecardiology case study, the proposed methodology can be effectively used as a guideline to develop distributed health care applications In other telecare areas.
SN  - 1094-687X
SN  - 0-7803-6466-X
PY  - 2000
VL  - 22
SP  - 555
EP  - 558
AN  - WOS:000166896300150
ER  -

TY  - JOUR
AU  - Ochoa, W
AU  - Legaristi, J
AU  - Larrinaga, F
AU  - Pérez, A
TI  - Dynamic context-aware workflow management architecture for efficient manufacturing: A ROS-based case study
T2  - FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE
AB  - The manufacturing industry of the future requires innovative approaches to optimize operational efficiency and adaptability. Integrating context-awareness into workflow management systems has emerged as a promising avenue to enhance efficiency in modern manufacturing processes. This research presents an innovative context -aware workflow management architecture designed to address industry-related challenges and overcome current limitations in the state-of-the-art. The architecture leverages Industry 4.0 standards for asset repre-sentation and workflow notation while incorporating a Context Analyzer component for real-time context interpretation. The effectiveness of the proposed solution is demonstrated in a real-world manufacturing setting, specifically in the scenario of collecting work order materials using the Robot Operating System (ROS) technology for robot navigation. The evaluation showcases improvements in task completion rate, resource utilization, and task completion time. These outcomes exemplify the potential benefits of incorporating context-awareness into manufacturing workflows, providing insights for further improvements. Contributions include advancing the understanding of context-aware workflow management, a review of the challenges that cap its adoption in the manufacturing domain, a qualitative comparison of similar approaches, practical implementation of the proposed architecture, evaluation of the context-aware component, and provision of the source code and datasets to the community for future advancement and reproducibility.
SN  - 0167-739X
SN  - 1872-7115
DA  - APR
PY  - 2024
VL  - 153
SP  - 505
EP  - 520
DO  - 10.1016/j.future.2023.12.024
C6  - DEC 2023
AN  - WOS:001150263100001
ER  -

TY  - CPAPER
AU  - Um, J
AU  - Kwon, WC
AU  - Hong, S
AU  - Kim, YT
AU  - Choi, KM
AU  - Kong, JT
AU  - Eo, SK
AU  - Kim, T
A1  - IEEE
TI  - A systematic IP and bus subsystem modeling for platform-based system design
T2  - 2006 DESIGN AUTOMATION AND TEST IN EUROPE, VOLS 1-3, PROCEEDINGS
CP  - Design, Automation and Test in Europe Conference and Exhibition (DATE 06)
AB  - The topic on platform-based system modeling has received a great deal of attention today. One of the important tasks that significantly affect the effectiveness and efficiency of the system modeling is the modeling of IP components and communication between IPs. To be effective, it is generally accepted that the system modeling should be performed in two steps; In the first step, a fast but some inaccurate system modeling is considered to facilitate the simultaneous development of software and hardware. The second step then refines the models of the software and hardware blocks (i.e., IPs) to increase the simulation accuracy for the system performance analysis. Here, one critical factor required for a successful system modeling is a systematic modeling of the IP blocks and bus subsystem connecting the IPs. In this respect, this work addresses the Problem of systematic modeling of the IPs and bus subsystem in different levels of refinements. In the experiments, we found that by applying our proposed IP and bus modeling methods to the MPEG-4 application, we are able to achieve 4x performance improvement and at the same time, reduce the software development time by 35%, compared to that by conventional modeling methods.
SN  - 1530-1591
SN  - 978-3-9810801-1-7
PY  - 2006
SP  - 558
EP  - 562
AN  - WOS:000243721600113
ER  -

TY  - CPAPER
AU  - Hästbacka, D
AU  - Kannisto, P
AU  - Kuikka, S
A1  - Zhang, J
ED  - Zhang, R
ED  - Cordeiro, J
ED  - Li, X
ED  - Zhang, Z
TI  - BUSINESS PROCESS MODELING AND SOA IN INDUSTRIAL O&M APPLICATION DEVELOPMENT
T2  - ICEIS 2011: PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON ENTERPRISE INFORMATION SYSTEMS, VOL 3
CP  - 13th International Conference on Enterprise Information Systems (ICEIS 2011)
AB  - While striving to increase profits in global competition, companies are trying to improve efficiency and reduce costs by outsourcing and focusing on their core functions. For operation of industrial plants this often results in provision of services even for high-priority activities such as maintenance. Integration of external information systems and service providers to business processes and information workflows brings new challenges to application development in order to support introduction of maintenance services as efficiently as possible. This paper discusses the approach of applying business process modeling and service-oriented concepts to development of supporting software applications. Business process modeling is proposed for describing service interactions and information flows, and to function as a foundation for the application development. To satisfy required flexibility in changing business environments, the applications represented as services are composed into executable process workflow orchestrations using standard Internet technologies. To validate the approach a scenario consisting of a condition monitoring process and an environment footprint estimator is presented.
SN  - 978-989-8425-55-3
PY  - 2011
SP  - 277
EP  - 285
AN  - WOS:000393449400035
ER  -

TY  - JOUR
AU  - Fu, Q
AU  - Zhang, SS
TI  - Product development process management system based on P_PROCE model
T2  - CONCURRENT ENGINEERING-RESEARCH AND APPLICATIONS
AB  - Product data management (PDM) systems are enabling tool of concurrent engineering (CE). Although the PDM system provides good support for product data, particularly at the early stages of design, it is seldom used beyond the design process. Furthermore, it provides few facilities for activity definition and no facilities for the enactment of production activities. In order to manage the whole lifecycle of motorcycle development, this paper presents a product development process management system that integrates the workflow management system (WFMS) with the PDM. On one hand we can use the powerful product data management functionality of PDM systems, on the other hand we can use the effective process management and control functionalities of WFMS. For the sake of guiding and constraining the workflow modeling, we provide an integrated product development workflow model named P_PROCE model. It is made up of five views that are the process view (P), the product view (P), the resource view (R), and the organization view (O), the control & evaluation view (CE). Based on this model, the architecture and implementation of the product development process management system is presented. It consists of the workflow modeling module, the workflow enactment module and the PDM system. The first module includes process modeling, system sustain and API. The second module includes the personal desktop and the workflow engine. The PDM system is regarded as a workflow-enabled application. The workflow engine invokes it by the Tool Agent.
SN  - 1063-293X
SN  - 1531-2003
DA  - SEP
PY  - 2002
VL  - 10
IS  - 3
SP  - 203
EP  - 211
DO  - 10.1106/106329302028055
AN  - WOS:000180761900002
ER  -

TY  - CPAPER
AU  - Blaukopf, S
AU  - Mendling, J
ED  - Teniente, E
ED  - Weidlich, M
TI  - An Organizational Routines Perspective on Process Requirements
T2  - BUSINESS PROCESS MANAGEMENT WORKSHOPS (BPM 2017)
CP  - 15th International Conference on Business Process Management (BPM)
AB  - Process modeling notations are meant to capture the requirements of a business process in a correct and complete way. Up until now, their assessment has been driven more by general modeling principles than by specific characteristics of business processes. In this paper, we turn to the theory of organizational routines in order to discuss mutual strengths and weaknesses of classical workflow management and recent adaptive case management as exemplified by their respective notations BPMN and CMMN.
SN  - 1865-1348
SN  - 978-3-319-74030-0
SN  - 978-3-319-74029-4
PY  - 2018
VL  - 308
SP  - 617
EP  - 622
DO  - 10.1007/978-3-319-74030-0_48
AN  - WOS:000434845200056
ER  -

TY  - JOUR
AU  - Büyüközkan, G
AU  - Kahraman, C
AU  - Ruan, D
TI  - A fuzzy multi-criteria decision approach for software development strategy selection
T2  - INTERNATIONAL JOURNAL OF GENERAL SYSTEMS
CP  - 5th International Conference on Computational Intelligent Systems for Applied Research
AB  - This study proposes a methodology to improve the quality of decision-making in the software development project under uncertain conditions. To deal with the uncertainty and vagueness from subjective perception and experience of humans in the decision process, a methodology based on the extent fuzzy analytic hierarchy process modeling to assess the adequate economic ( tangible) and quality ( intangible) balance is applied. Two key factors of economic and quality are evaluated separately by fuzzy approaches and both factors' estimates are combined to obtain the preference degree associated with each software development project strategy alternative for selecting the most appropriate one. Using the proposed approach, the ambiguities involved in the assessment data can be effectively represented and processed to assure a more convincing and effective decision-making. Finally, a real case-study is given to demonstrate the potential of the methodology.
SN  - 0308-1079
SN  - 1563-5104
DA  - APR-JUN
PY  - 2004
VL  - 33
IS  - 2-3
SP  - 259
EP  - 280
DO  - 10.1080/03081070310001633581
AN  - WOS:000220147100012
ER  -

TY  - JOUR
AU  - Murtaza, A
AU  - Rehman, A
AU  - Malik, SUR
AU  - Ahmed, G
AU  - Abbas, A
AU  - Khan, MA
TI  - A Model-Based Approach to Enhance the Communication Between the Participants of Collaborative Business Processes
T2  - IEEE ACCESS
AB  - Business process modeling is used to model business processes using Business Process Modeling Notation (BPMN), which is a widely accepted standard for process modeling. BPMN elements are visually represented by the existing model, but the expressiveness of elements in terms of communication between the participants of the business process is a problem reported in modeling literature. Business processes use collaboration models to gain increasing importance in software development, describing their behavior and interaction. Recent years have seen the presentation of various approaches to ensure communication between business process pools. Despite the widespread adoption of BPMN for business process modeling, existing collaboration models often suffer from significant limitations in accurately capturing complex collaborative business processes. The existing approaches do not ensure proper structure and syntax for collaboration elements. The flow of information among multiple pools causes ambiguity in the developed business process. A Collaborative Business Process Model (CBPM) is proposed to address this issue, based on modeling rules that ensure proper syntax and structure of the models. The proposed CBPM also guarantees that the model is a better approach for participant interaction. This approach contributes to improving the communication mechanism between the participants of collaborative business processes. Moreover, we formally analyze and verify the working of CBPM by specifying the model in Z specification language. Performance evaluation regarding the flow of messages through test case coverage criteria indicates that the model is capable of ensuring successful communication among the multiple participants of business processes.
SN  - 2169-3536
PY  - 2024
VL  - 12
SP  - 121780
EP  - 121791
DO  - 10.1109/ACCESS.2024.3450690
AN  - WOS:001310677200001
ER  -

TY  - JOUR
AU  - Johnston, A
AU  - Mahesh, M
AU  - Uneri, A
AU  - Rypinski, TA
AU  - Boone, JM
AU  - Siewerdsen, JH
TI  - Objective image quality assurance in cone-beam CT: Test methods, analysis, and workflow in longitudinal studies
T2  - MEDICAL PHYSICS
AB  - BackgroundStandards for image quality evaluation in multi-detector CT (MDCT) and cone-beam CT (CBCT) are evolving to keep pace with technological advances. A clear need is emerging for methods that facilitate rigorous quality assurance (QA) with up-to-date metrology and streamlined workflow suitable to a range of MDCT and CBCT systems.PurposeTo evaluate the feasibility and workflow associated with image quality (IQ) assessment in longitudinal studies for MDCT and CBCT with a single test phantom and semiautomated analysis of objective, quantitative IQ metrology.MethodsA test phantom (CorgiTM Phantom, The Phantom Lab, Greenwich, New York, USA) was used in monthly IQ testing over the course of 1 year for three MDCT scanners (one of which presented helical and volumetric scan modes) and four CBCT scanners. Semiautomated software analyzed image uniformity, linearity, contrast, noise, contrast-to-noise ratio (CNR), 3D noise-power spectrum (NPS), modulation transfer function (MTF) in axial and oblique directions, and cone-beam artifact magnitude. The workflow was evaluated using methods adapted from systems/industrial engineering, including value stream process modeling (VSPM), standard work layout (SWL), and standard work control charts (SWCT) to quantify and optimize test methodology in routine practice. The completeness and consistency of DICOM data from each system was also evaluated.ResultsQuantitative IQ metrology provided valuable insight in longitudinal quality assurance (QA), with metrics such as NPS and MTF providing insight on root cause for various forms of system failure-for example, detector calibration and geometric calibration. Monthly constancy testing showed variations in IQ test metrics owing to system performance as well as phantom setup and provided initial estimates of upper and lower control limits appropriate to QA action levels. Rigorous evaluation of QA workflow identified methods to reduce total cycle time to similar to 10 min for each system-viz., use of a single phantom configuration appropriate to all scanners and Head or Body scan protocols. Numerous gaps in the completeness and consistency of DICOM data were observed for CBCT systems.ConclusionAn IQ phantom and test methodology was found to be suitable to QA of MDCT and CBCT systems with streamlined workflow appropriate to busy clinical settings.
SN  - 0094-2405
SN  - 2473-4209
DA  - APR
PY  - 2024
VL  - 51
IS  - 4
SP  - 2424
EP  - 2443
DO  - 10.1002/mp.16983
C6  - FEB 2024
AN  - WOS:001161765200001
ER  -

TY  - JOUR
AU  - Dashbalbar, A
AU  - Song, SM
AU  - Lee, JW
AU  - Lee, B
TI  - Towards Enacting a SPEM-based Test Process with Maturity Levels
T2  - KSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS
AB  - Effective monitoring and testing during each step are essential for document verification in research and development (R& D) projects. In software development, proper testing is required to verify it carefully and constantly because of the invisibility features of software. However, not enough studies on test processes for R& D projects have been done. Thus, in this paper, we introduce a Test Maturity Model integration (TMMi)-based software field R& D test process that offers five integrity levels and makes the process compatible for different types of projects. The Software & Systems Process Engineering Metamodel (SPEM) is used widely in the software process-modeling context, but it lacks built-in enactment capabilities, so there is no tool or process engine that enables one to execute the process models described in SPEM. Business Process Model and Notation (BPMN)-based workflow engines can be a solution for process execution, but process models described in SPEM need to be converted to BPMN models. Thus, we propose an approach to support enactment of SPEM-based process models by converting them into business processes. We show the effectiveness of our approach through converting software R& D test processes specified in SPEM in a case study.
SN  - 1976-7277
DA  - FEB 28
PY  - 2017
VL  - 11
IS  - 2
SP  - 1217
EP  - 1233
DO  - 10.3837/tiis.2017.02.034
AN  - WOS:000397286100034
ER  -

TY  - JOUR
AU  - van der Aalst, WMP
TI  - Business process management as the "Killer App" for Petri nets
T2  - SOFTWARE AND SYSTEMS MODELING
AB  - Since their inception in 1962, Petri nets have been used in a wide variety of application domains. Although Petri nets are graphical and easy to understand, they have formal semantics and allow for analysis techniques ranging from model checking and structural analysis to process mining and performance analysis. Over time Petri nets emerged as a solid foundation for Business Process Management (BPM) research. The BPM discipline develops methods, techniques, and tools to support the design, enactment, management, and analysis of operational business processes. Mainstream business process modeling notations and workflow management systems are using token-based semantics borrowed from Petri nets. Moreover, state-of-the-art BPM analysis techniques are using Petri nets as an internal representation. Users of BPM methods and tools are often not aware of this. This paper aims to unveil the seminal role of Petri nets in BPM.
SN  - 1619-1366
SN  - 1619-1374
DA  - MAY
PY  - 2015
VL  - 14
IS  - 2
SP  - 685
EP  - 691
DO  - 10.1007/s10270-014-0424-2
AN  - WOS:000354094400010
ER  -

TY  - CPAPER
AU  - Küng, P
AU  - Hagen, C
AU  - Rodel, M
AU  - Seifert, S
A1  - IEEE Computer Society
TI  - Business process monitoring & measurement in a large bank:: Challenges and selected approaches
T2  - Sixteenth International Workshop on Database and Expert Systems Applications, Proceedings
CP  - 16th International Workshop on Database and Expert Systems Applications
AB  - In order to gain competitive advantage, many companies are engaging in the reorganization of their business processes and in implementing automated or semi-automated process execution based on workflow and process engines. In order to improve business processes continuously, closed-loop systems are required. This paper describes how a combination of process modeling, process execution, and process measurement has been applied at Credit Suisse, a large Swiss bank operating internationally. The paper shows that explicit process models are a vital requirement for process measurement. Moreover, it illustrates the strengths and limitations of a particular performance management tool that has been in place for about a year. Based on practical experience with different process engines and a process performance management software, alternative architectures are suggested to support process monitoring, process measurement, and business activity monitoring (BAM).
SN  - 0-7695-2424-9
PY  - 2005
SP  - 955
EP  - 961
DO  - 10.1109/DEXA.2005.60
AN  - WOS:000232421900161
ER  -

TY  - JOUR
AU  - Chaves, RO
AU  - von Wangenheim, CG
AU  - Furtado, JCC
AU  - Oliveira, SRB
AU  - Santos, A
AU  - Favero, EL
TI  - Experimental Evaluation of a Serious Game for Teaching Software Process Modeling
T2  - IEEE TRANSACTIONS ON EDUCATION
AB  - Software process modeling (SPM) is an important area of software engineering because it provides a basis for managing, automating, and supporting software process improvement (SPI). Teaching SPM is a challenging task, mainly because it lays great emphasis on theory and offers few practical exercises. Furthermore, as yet few teaching approaches have aimed at teaching SPM by introducing innovative features, such as games. The use of games has mainly been focused on other areas of software engineering, for example software project management. In an attempt to fill this gap, this paper describes a formal experiment carried out to assess the learning effectiveness of a serious game (DesigMPS), designed to support the teaching of SPM, and to compare game-based learning with a project-based learning method. In the DesigMPS game, the student models a software process from an SPI perspective, based on the Brazilian SPI model (MPS. BR). The results indicate that playing the game can have a positive learning effect and results in a greater degree of learning effectiveness than does the project-based learning instructional method.
SN  - 0018-9359
SN  - 1557-9638
DA  - NOV
PY  - 2015
VL  - 58
IS  - 4
SP  - 289
EP  - 296
DO  - 10.1109/TE.2015.2411573
AN  - WOS:000369513400011
ER  -

TY  - JOUR
AU  - Möller, J
AU  - Rodríguez, TH
AU  - Müller, J
AU  - Arndt, L
AU  - Kuchemüller, KB
AU  - Frahm, B
AU  - Eibl, R
AU  - Eibl, D
AU  - Pörtner, R
TI  - Model uncertainty-based evaluation of process strategies during scale-up of biopharmaceutical processes
T2  - COMPUTERS & CHEMICAL ENGINEERING
AB  - Reliable scale-up of biopharmaceutical production processes is key in Quality by Design. In this study, a model-based workflow is described to evaluate the bioprocess dynamics during process transfer and scale-up computationally. First, a mathematical model describes the bioprocess dynamics of different state variables (e.g., cell density, titer). Second, the model parameter probability distributions are determined at different scales due to measurement uncertainty. Third, the quantified parameter distributions are statistically compared to evaluate if the process dynamics have been changed. This workflow was tested for the scale-up of an antibody-producing CHO fed-batch process. Significant differences were identified between the process development (30 ml) and implementation (250 ml) scale, and the feeding strategy was validated using model-assisted Design of Experiments. Then, the validated process strategy was successfully scaled up to 2 l laboratory and 50 l pilot scale. In summary, the proposed workflow enables a knowledge-driven evaluation tool for bioprocess development. (C) 2020 Elsevier Ltd. All rights reserved.
SN  - 0098-1354
SN  - 1873-4375
DA  - MAR 4
PY  - 2020
VL  - 134
C7  - 106693
DO  - 10.1016/j.compchemeng.2019.106693
AN  - WOS:000517756500026
ER  -

TY  - JOUR
AU  - Becker, J
AU  - Rosemann, M
AU  - von Uthmann, C
TI  - Guidelines of business process modeling
T2  - BUSINESS PROCESS MANAGEMENT
AB  - Process modeling becomes more and more an important task not only for the purpose of software engineering, but also for many other purposes besides the development of software. Therefore it is necessary to evaluate the quality of process models from different viewpoints. This is even more important as the increasing number of different end users, different purposes and the availability of different modeling techniques and modeling tools leads to a higher complexity of information models. In this paper the Guidelines of Modeling (GoM)(1), a framework to structure factors for the evaluation of process models, is presented. Exemplary, Guidelines of Modeling for workflow management and simulation are presented. Moreover, six general techniques for adjusting models to the perspectives of different types of user and purposes will be explained.
SN  - 0302-9743
PY  - 2000
VL  - 1806
SP  - 30
EP  - 49
AN  - WOS:000089185100003
ER  -

TY  - JOUR
AU  - Askari, M
AU  - Westerhof, R
AU  - Eslami, S
AU  - Medlock, S
AU  - de Rooij, SE
AU  - Abu-Hanna, A
TI  - A combined disease management and process modeling approach for assessing and improving care processes: A fall management case-study
T2  - INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS
AB  - Objectives: To propose a combined disease management and process modeling approach for evaluating and improving care processes, and demonstrate its usability and usefulness in a real-world fall management case study.
   Methods: We identified essential disease management related concepts and mapped them into explicit questions meant to expose areas for improvement in the respective care processes. We applied the disease management oriented questions to a process model of a comprehensive real world fall prevention and treatment program covering primary and secondary care. We relied on interviews and observations to complete the process models, which were captured in UML activity diagrams. A preliminary evaluation of the usability of our approach by gauging the experience of the modeler and an external validator was conducted, and the usefulness of the method was evaluated by gathering feedback from stakeholders at an invitational conference of 75 attendees.
   Results: The process model of the fall management program was organized around the clinical tasks of case finding, risk profiling, decision making, coordination and interventions. Applying the disease management questions to the process models exposed weaknesses in the process including: absence of program ownership, under-detection of falls in primary care, and lack of efficient communication among stakeholders due to missing awareness about other stakeholders' workflow.
   The modelers experienced the approach as usable and the attendees of the invitational conference found the analysis results to be valid.
   Conclusions: The proposed disease management view of process modeling was usable and useful for systematically identifying areas of improvement in a fall management program. Although specifically applied to fall management, we believe our case study is characteristic of various disease management settings, suggesting the wider applicability of the approach. (C) 2013 Elsevier Ireland Ltd. All rights reserved.
SN  - 1386-5056
DA  - OCT
PY  - 2013
VL  - 82
IS  - 10
SP  - 1022
EP  - 1033
DO  - 10.1016/j.ijmedinf.2013.06.011
AN  - WOS:000324312000013
ER  -

TY  - JOUR
AU  - Huysmans, P
AU  - Ven, K
AU  - Verelst, J
TI  - Using the DEMO methodology for modeling open source software development processes
T2  - INFORMATION AND SOFTWARE TECHNOLOGY
AB  - Context: Open source software development (OSSD) process modeling has received increasing interest in recent years. These efforts aim to identify common elements in the development process between multiple open source software (OSS) projects. However, the complexity inherent to OSSD process modeling puts significant demands on the modeling language.
   Objective: In this paper, we propose that the Design and Engineering Methodology for Organizations (DEMO) may provide an interesting alternative to develop OSSD process models. DEMO exhibits two unique features within the context of OSSD process modeling. First, DEMO analyzes processes at the ontological level and provides high-level process descriptions, instead of focusing on the implementation level. Second, DEMO studies the communication patterns between human actors, instead of the sequences in which activities are performed.
   Method: We investigate the feasibility of using DEMO to construct OSSD process models by means of a case study. DEMO models were constructed to describe the NetBeans Requirements and Release process. In addition, the quality of these DEMO models was evaluated using a quality framework for conceptual modeling.
   Results: Our results showed that our DEMO models exhibited a high level of abstraction, thereby reducing the complexity of the OSSD process models. In addition, the evaluation of the models developed in this paper by using the quality framework for conceptual modeling showed that the models were of high quality.
   Conclusions: We have shown that the DEMO methodology can be successfully used to model OSSD processes and to obtain abstract and high-quality OSSD process models. However, given some potential drawbacks with respect to understandability and implementability, we primarily propose the use of DEMO within OSSD process modeling as an analysis tool that should be complemented with other techniques and models for communication and reenactment purposes. (C) 2010 Elsevier B.V. All rights reserved.
SN  - 0950-5849
SN  - 1873-6025
DA  - JUN
PY  - 2010
VL  - 52
IS  - 6
SP  - 656
EP  - 671
DO  - 10.1016/j.infsof.2010.02.002
AN  - WOS:000278150500006
ER  -

TY  - CHAP
AU  - Begier, B
ED  - Zielinski, K
ED  - Szmuc, T
TI  - The UID Approach - the Balance between Hard and Soft Methodologies
T2  - SOFTWARE ENGINEERING: EVOLUTION AND EMERGING TECHNOLOGIES
AB  - The UID approach (User Involved Development) is suggested to improve software development process. Its aim is to admit users as its subject. The UID approach shows an attempt to combine features of hard and soft methodologies. The user focus leads to users' involvement in the process. An initial questionnaire survey of all potential users is recommended. There are suggested specified forms of developers-users cooperation to provide a continual feedback between both sides. Business process modeling, software product assessment by all its users, competency-based selection to play various roles, construction of questionnaires - are examples of methods in use.
SN  - 0922-6389
SN  - 1879-8314
SN  - 978-1-58603-559-4
PY  - 2005
VL  - 130
SP  - 15
EP  - 26
AN  - WOS:000269535900002
ER  -

TY  - CPAPER
AU  - Annane, A
AU  - Kamel, M
AU  - Aussenac-Gilles, N
ED  - Rocha, AP
ED  - Steels, L
ED  - VanDenHerik, J
TI  - Comparing Business Process Ontologies for Task Monitoring
T2  - ICAART: PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON AGENTS AND ARTIFICIAL INTELLIGENCE, VOL 2
CP  - 12th International Conference on Agents and Artificial Intelligence (ICAART)
AB  - Business process (BP) modelling is an active area of research due to its multiple applications. For systems that support/monitor operators to perform their tasks (i.e., tasks of a given BP), a formal representation is essential. Various BP ontologies are available to formally represent BP. In this paper, we review and compare a set of nine BP ontologies according to their ability to represent process specification and process execution in a fine-grained way to enable task monitoring. The comparison shows that, on the one hand, ontologies developed from scratch establish a clear distinction between process specification and process execution, but do not allow to represent workflow constraints required for process execution. On the other hand, most of the ontologies, that are ontological versions of existing BP modeling languages, focus only on process specifications but do not represent process execution, or mix the representation of BP specification and execution.
SN  - 978-989-758-395-7
PY  - 2020
SP  - 634
EP  - 643
DO  - 10.5220/0008978706340643
AN  - WOS:000570769000068
ER  -

TY  - CPAPER
AU  - Klimavicius, M
AU  - Sukovskis, U
ED  - Demiralp, M
ED  - Mikhael, WB
ED  - Caballero, AA
ED  - Abatzoglou, N
ED  - Tabrizi, MN
ED  - Leandre, R
ED  - GarciaPlanas, M
ED  - Choras, RS
TI  - Applying MDA and universal data models for data warehouse modeling
T2  - ACMOS '08: PROCEEDINGS OF THE 10TH WSEAS INTERNATIONAL CONFERENCE ON AUTOMATIC CONTROL, MODELLING AND SIMULATION
CP  - 10th WSEAS International Conference on Automatic Control, Modelling and Simulation
AB  - Business process monitoring provides an invaluable means of an enterprise to adapt to changing conditions. Data warehouse stores the process data which is foundation for business process monitoring applications. Development of such applications by using traditional methods is challenging because of the complexity of integrating business processes and existing information systems. Different modeling approaches have been proposed to overcome every design pitfall of the development of data warehouse systems. On the other hand, model driven architecture is an approach to develop applications from domain-specific models to platform-sensitive models - that bridges the gap between business processes and information technology. Model driven architecture is a standard framework for software development that addresses the complete life cycle of designing, deploying, integrating, and managing applications by using models in software development. Authors propose to use model driven approach for data warehouse development. Also the concept of universal data models was introduced in order to ease data warehouse development by providing standard data objects. This paper introduces the overall concept of applying model driven architecture and universal data models to development process of data warehouse.
SN  - 978-960-6766-63-3
PY  - 2008
SP  - 332
EP  - +
AN  - WOS:000258072400058
ER  -

TY  - CPAPER
AU  - Vemuri, P
AU  - Poelmans, S
AU  - Compagnucci, I
AU  - Snoeck, M
A1  - IEEE
TI  - Using Formative Assessment and Feedback to Train Novice Modelers in Business Process Modeling
T2  - 2023 ACM/IEEE INTERNATIONAL CONFERENCE ON MODEL DRIVEN ENGINEERING LANGUAGES AND SYSTEMS COMPANION, MODELS-C
CP  - ACM/IEEE International Conference on Model Driven Engineering Languages and Systems (MODELS)
AB  - Process models play a vital role in modern businesses, specifically in the software development lifecycle to (re)design and (re)engineer business processes. There exists a large body of research on classifying errors in business process modeling (BPM) and on the importance of higher-quality process models in the early design stages. Some works have defined quality frameworks and listed guidelines for modeling to help modelers achieve higher-quality diagrams. But there is no established way in which BPM should be taught, and there is a research gap in connecting instructional design and BPM. The current study addresses this gap by presenting a teaching module on BPM at a higher education institute in blended learning contexts. To improve the skill sets of novice modelers it is crucial to teach them how to interpret and integrate quality into BPM by giving them feedback on errors they make. We achieve this by leveraging pedagogy, incorporating formative assessments and an error typology.
SN  - 979-8-3503-2498-3
PY  - 2023
SP  - 130
EP  - 137
DO  - 10.1109/MODELS-C59198.2023.00037
AN  - WOS:001137051500027
ER  -

TY  - CPAPER
AU  - Ionela, M
AU  - George, M
ED  - Remus, B
TI  - Business process modeling
T2  - 5th RoEduNet IEEE International Conference, Proceedings
CP  - 5th RoEduNet IEEE International Conference
AB  - Workflow in today's enterprise means more monitoring and orchestrating massive systems. A new technology called Business Process Management, or BPM, helps software architects and developers design, code, run, administer, and monitor complex network-based business processes.
   BPM replaces those sketchy flowchart diagrams that business analysts draw on whiteboards with a precise model that uses standard graphical and XML representations, and an architecture that allows it converse with other services, systems, and users. BPM complements enterprise application integration (EAI), a method for moving from older applications to new ones, and Enterprise Service BUS for integrating different web services, messaging, and XML technologies into a single network.
SN  - 978-973-739-277-0
PY  - 2006
SP  - 291
EP  - 295
AN  - WOS:000248117300059
ER  -

TY  - CPAPER
AU  - Tiwari, S
AU  - Rathore, SS
A1  - IEEE
TI  - Teaching Software Process Models to Software Engineering Students: An Exploratory Study
T2  - 2019 26TH ASIA-PACIFIC SOFTWARE ENGINEERING CONFERENCE (APSEC)
CP  - 26th Asia-Pacific Software Engineering Conference (APSEC)
AB  - A software process model (SPM) provides an abstract description of the order in which related activities of software development will be undertaken. Many process models available that can be adapted for software development. However, the selection of the best suitable process model with reference to the problem definition, constraints, and stakeholder requirements is a challenging task. Typically, in a Software Engineering (SE) course, students gain knowledge about SPM and realize their usage via classroom lectures and course projects. It is felt that if the basic knowledge imparted, through the fundamental SE course, is supplemented with some focused sessions about the SPM, then it will not only enable students to think in terms of the SPM but will also motivate them to harness the best practices of software development. This paper presents a preliminary study highlighting our experience on SPM-oriented teaching to impart the concept of requirement elicitation and process modeling, by performing a play (drama skit) annotating real-world scenarios. The feedbacks of students have been collected to evaluate whether this exercise helped them in understanding the processes they have to undergo during software development. Additionally, we have compared the student's feedback and performance in the project and reported the finding of the study.
SN  - 978-1-7281-4648-5
PY  - 2019
SP  - 308
EP  - 315
DO  - 10.1109/APSEC48747.2019.00049
AN  - WOS:000517102200039
ER  -

TY  - JOUR
AU  - Dangelmaier, W
AU  - Kress, S
AU  - Wenski, R
TI  - TelCoW: telework under the co-ordination of a workflow management system
T2  - INFORMATION AND SOFTWARE TECHNOLOGY
AB  - Telework is considered as an innovative work organization form for new decentralized structures. The flexibility of telework in both time and location of task execution must make it possible to take advantage of this work organization for company competitiveness. Today, telework is mainly used for isolated tasks and for those with only few cross-references to others. But telework will only achieve its full potential if attention is given to the fact that work in companies is normally co-operative work. For this co-operative work, business process modeling and workflow management is accepted as a supporting methodology. On the one hand this is important for enabling a production company to distribute work to employees who operate within this organizational model, and at the same time, compared with conventional models of organizing work, to increase the productivity of task execution. The increasing effort for co-ordination and monitoring is seen to be the main obstacle of co-operative telework. The use of existing workflow management systems for the support of co-operative telework is currently not possible; there is no tool available with specific co-ordination and planning functionality for this purpose. In this paper, we will define a specific business process model which is oriented for the modeling of decentralized structures especially for telework and the direct support by a workflow management system (WFMS). Compared to traditional WFMSs, our system is extended by a module for the planning and monitoring functionality required for monitoring of teleworkers and their time management. The distribution of work is supported by means of a co-ordinator as a constituent part of the WFMS. It executes workflows which are provided by a certain method for modeling business processes. This method already considers necessary refinements in passing over business processes to workflows. The models for business processes are supplied by means of a meta-model, which is extended by aspects of telematics. The system is built using Internet technology and uses platform-independent WWW software for the user interface. (C) 1999 Elsevier Science B.V. All rights reserved.
SN  - 0950-5849
DA  - APR 25
PY  - 1999
VL  - 41
IS  - 6
SP  - 341
EP  - 353
DO  - 10.1016/S0950-5849(98)00067-6
AN  - WOS:000079989400004
ER  -

TY  - CPAPER
AU  - Fernandez, R
AU  - Kienbaum, GS
AU  - Neto, AA
AU  - Ferreira, MGV
ED  - Borsato, M
ED  - Wognum, N
ED  - Peruzzini, M
ED  - Stjepandic, J
ED  - Verhagen, WJC
TI  - T-PROST: A Transdisciplinary Process Oriented Framework to Support the Product Design Phase in Systems Concurrent Engineering
T2  - TRANSDISCIPLINARY ENGINEERING: CROSSING BOUNDARIES
CP  - 23rd ISPE Inc. International Conference on Transdisciplinary Engineering
AB  - The objective of this work is to present and to discuss the potentialities of a transdisciplinary process modeling methodology, which has been named T-PROST PROST Framework, to support the product design phase processes (production and management) in the systems concurrent engineering lifecycle. The methodology consists in creating a conceptual reference model of the systems concurrent engineering lifecycle processes and to transform it into specialised models of the areas of (Model Based) Systems Engineering, Project Management, Business Process Management and Simulation Modeling. The main benefits generated by the approach are derived from: The systematisation of the model development, encompassing both the systems engineering processes and their management; The use of the T-ProST Framework for model implementation and analysis, based on the simultaneous use of diverse disciplines and their respective methodologies and tools; The joint assessment of the specialised models created to provide better solutions and to improve project development. The T-ProST Framework can be used for the implementation of generic environments, made by the ensemble of applications implemented with the specialised tools used in the diverse disciplines. These generic environments can be used to perform product lifecycle management by organisations conducting short lifecycle project developments, characterised by low and mid-level complexity scopes submitted to severe time and budgetary constraints, as an alternative to the use of complex, expensive and difficult to configure proprietary systems existing on the market.
SN  - 2352-7528
SN  - 978-1-61499-703-0
SN  - 978-1-61499-702-3
PY  - 2016
VL  - 4
SP  - 758
EP  - 767
DO  - 10.3233/978-1-61499-703-0-758
AN  - WOS:000385298900075
ER  -

TY  - CPAPER
AU  - van Elst, L
AU  - Aschoff, FR
AU  - Bernardi, A
AU  - Maus, H
AU  - Schwarz, S
A1  - IEEE COMPUTER SOCIETY
A1  - IEEE COMPUTER SOCIETY
TI  - Weakly-structured workflows for knowledge-intensive tasks: An experimental evaluation
T2  - TWELFTH IEEE INTERNATIONAL WORKSHOPS ON ENABLING TECHNOLOGIES: INFRASTRUCTURE FOR COLLABORATIVE ENTERPRISES, PROCEEDINGS
CP  - 12th IEEE International Workshop on Enabling Technologies - Infrastructure for Collaborative Enterprises
AB  - Knowledge work can typically not be modeled sufficiently by classical, static process models. To enable a process-oriented knowledge management (KM) approach under these circumstances, the concept of weakly-structured workflows has been developed. This approach intertwines process modeling and workflow enactment and facilitates active information support in dynamically changing environments. Furthermore, the approach allows process knowledge itself to be used as a valuable resource in KM. In order to investigate whether this approach is viable and useful for supporting knowledge-intensive activities we designed and performed a comprehensive experimental evaluation. We outline the conceptual approach and realization in an agent-based software framework for organizational memories and describe the experimental settings. The results of the experiment demonstrate the viability of our key concepts.
SN  - 0-7695-1963-6
PY  - 2003
SP  - 340
EP  - 345
AN  - WOS:000186183900061
ER  -

TY  - JOUR
AU  - Green, P
AU  - Rosemann, M
TI  - Integrated process modeling: An ontological evaluation
T2  - INFORMATION SYSTEMS
CP  - 11th International Conference on Advanced Information Systems Engineering (CAiSE 99)
AB  - Process modeling has gained prominence in the information systems modeling area due to its focus on business processes and its usefulness in such business improvement methodologies as Total Quality Management, Business Process Reengineering, and Workflow Management. However, process modeling techniques are not without their criticisms [13]. This paper proposes and uses the Bunge-Wand-Weber (BWW) representation model to analyze the five views - process, data, function, organization and output - provided in the Architecture of Integrated Information Systems (ARIS) popularized by Scheer [39, 40, 41]. The BWW representation model attempts to provide a theoretical base on which to evaluate and thus contribute to the improvement of information systems modeling techniques. The analysis conducted in this paper prompts some propositions. It confirms that the process view alone is not sufficient to model all the real-world constructs required. Some other symbols or views are needed to overcome these deficiencies. However, even when considering all five views in combination, problems may arise in representing all potentially required business rules, specifying the scope and boundaries of the system under consideration, and employing a "top-down" approach to analysis and design. Further work from this study will involve the operationalization of these propositions and their empirical testing in the field. (C) 2000 Elsevier Science Ltd. All rights reserved.
SN  - 0306-4379
DA  - APR
PY  - 2000
VL  - 25
IS  - 2
SP  - 73
EP  - 87
DO  - 10.1016/S0306-4379(00)00010-7
AN  - WOS:000087383600002
ER  -

TY  - JOUR
AU  - Hammouti, A
AU  - Oukaili, F
AU  - Bang, DPV
TI  - Error assessment of reconstructed 3D Digital Replica Models: From Computed Tomography data to pore-scale simulations
T2  - COMPUTERS & FLUIDS
AB  - The application of flow simulations on porous media, reconstructed through Computerized Tomography (CT) scans, has emerged as a prevalent methodology for the computation of rock permeability. However, constructing a proper 3D model of a rock sample is areal challenge, mainly due to the lack of a unified procedure. Indeed, to ensure precise outcomes, specific prerequisites must be fulfilled. This paper proposes a methodology to assess the convergence and accuracy of computed solutions from CT data to pore-scale simulations. Starting from 3D volume data obtained by X-ray CT, we develop a workflow to investigate the effects of the reconstructed shape on the permeability of a granular porous medium composed of glass beads. Indeed, the choices of CT scan resolution and digital rock discretization can compromise the quality and computational cost of numerical results. Especially in configurations of porous media with high solid volume fractions and very narrow porous spaces, as observed in solid/solid contact zones, which can be either under or over-resolved depending on the numerical tools used. Highly resolved Direct Numerical Simulations (DNS) are conducted to solve incompressible Navier-Stokes equations through porous media. Body-fitted meshes are employed to resolve irregular shapes accurately, ensuring precise results even with coarser meshes. The methodology is validated with challenging simulations of flows through simple cubic close packing of particles, incorporating various geometric surface modeling techniques. A convergence of the results with respect to grid resolution is obtained for low- to moderate-Reynolds-number flows. The numerical results indicate that permeability calculation strongly depends on surface processing. Finally, we apply these recommendations to construct accurate digital replica models generated from CT data of our assembly of randomly arranged glass beads in a tube. The study of the pressure drop convergence demonstrates an excellent agreement with the empirical correlation.
SN  - 0045-7930
SN  - 1879-0747
DA  - DEC 15
PY  - 2024
VL  - 285
C7  - 106450
DO  - 10.1016/j.compfluid.2024.106450
C6  - OCT 2024
AN  - WOS:001338549600001
ER  -

TY  - CPAPER
AU  - Espinosa-Curiel, IE
AU  - Rodríguez-Jacobo, J
AU  - Fernández-Zepeda, JA
ED  - Riel, A
ED  - OConnor, R
ED  - Tichkiewitch, S
ED  - Messnarz, R
TI  - Graphical Technique to Support the Teaching/Learning Process of Software Process Reference Models
T2  - SYSTEMS, SOFTWARE AND SERVICES PROCESS IMPROVEMENT
CP  - 17th European Conference on Systems, Software and Services Process Improvement
AB  - In this paper, we propose a set of diagrams to visualize software process reference models (PRM). The diagrams, called dimods, are the combination of some visual and process modeling techniques such as rich pictures, mind maps, IDEF and RAD diagrams. We show the use of this technique by designing a set of dimods for the Mexican Software Industry Process Model (MoProSoft). Additionally, we perform an evaluation of the usefulness of dimods. The result of the evaluation shows that dimods may be a support tool that facilitates the understanding, memorization, and learning of software PRMs in both, software development organizations and universities. The results also show that dimods may have advantages over the traditional description methods for these types of models.
SN  - 1865-0929
SN  - 1865-0937
SN  - 978-3-642-15665-6
PY  - 2010
VL  - 99
SP  - 13
EP  - 24
AN  - WOS:000289177000002
ER  -

TY  - CPAPER
AU  - Belay, GG
AU  - He, ZS
AU  - Zou, DS
ED  - Wu, YW
TI  - Software Process Management Assessment Towards CMM a Systematic Approach to Optimize Internal Process Improvement
T2  - SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING: THEORY AND PRACTICE, VOL 1
CP  - Pacific-Asia Conference on Knowledge Engineering and Software Engineering
AB  - There are currently a number of holistic approaches available in relation to software process improvement. The search for new ideas and innovations to improve software development productivity and system quality continues to be a key focus of industrial and academic research. The CMM is a software process maturity model, which well known and most widely used model for Software Process Improvement (SPI). The SEI is continuing to evolve the CMM concepts, primarily in its current work on CMM integration, which addresses software, systems engineering, and integrated process and product development. This model attempts to quantify a software organization's capability to consistently and predictably produce high-quality software products by involving process modeling, assessment, metrication, and technology transfer/ implementation to a greater or lesser extent. This paper provides a synthesis of prescriptions for successful software process improvement found from an extensive review of the quality management, organizational learning, and software process assessment process.
SN  - 1867-5662
SN  - 978-3-642-03717-7
PY  - 2012
VL  - 114
SP  - 499
EP  - 505
AN  - WOS:000310715400062
ER  -

TY  - CPAPER
AU  - Wu, MH
AU  - Ying, J
AU  - Yu, CY
A1  - IEEE
TI  - A methodology and its support environment for benchmark-based adaptable software process improvement
T2  - 2004 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN & CYBERNETICS, VOLS 1-7
CP  - IEEE International Conference on Systems, Man and Cybernetics
AB  - Software quality relies on the quality of software process to a high degree. In this paper, we first introduce the related work and list the design requirements of an ideal software process improvement system. Based on CMM/CMMI, ISO/IEC 15504, ISO 9000-3 and Bootstrap etc software process assessment and improvement methods and standards, this paper provides a methodology for benchmark-based adaptable software process improvement (MBASPI), and introduces the main components of its support environment (MBASPI/E). With the philosophy of "Balance and Optimum", through large granular software process reuse, using software process modeling language to construct the unified models of practical development, and through the enactment of these models under the support environment combining with domain knowledge to force software development organizations to comply some process standards, so to achieve a higher capability maturity level and realize a continuous software process improvement natively.
SN  - 1062-922X
SN  - 0-7803-8566-7
PY  - 2004
SP  - 5183
EP  - 5188
AN  - WOS:000226863300873
ER  -

TY  - CPAPER
AU  - Vemuri, P
AU  - Wautelet, Y
AU  - Poelmans, S
AU  - Verwimp, S
AU  - Heng, S
ED  - Ghose, A
ED  - Horkoff, J
ED  - Souza, VES
ED  - Parsons, J
ED  - Evermann, J
TI  - Top-Down Versus Operational-Only Business Process Modeling: An Experimental Evaluation of the Approach Leading to Higher Quality Representations
T2  - CONCEPTUAL MODELING, ER 2021
CP  - 40th International Conference on Conceptual Modeling (ER)
AB  - Business process modeling (BPMo) is of primary importance for assessing the current state of an organizations' practices to discover inefficiencies, redesign business processes, and build software solutions. High-quality representations best capture the true nature of the organization. This paper investigates the hypothesis of whether Business Process Modeling Notation (BPMN), Business Process Diagrams (BPDs) created through a Top-Down Modeling Approach (TDMA) are of higher quality than those made from an operational perspective only. An experiment was conducted where novice modelers were to model a case based on a textual description. The test group used the TDMA by first modeling strategic, tactical aspects using a Business Use-Case Model (BUCM) before the operational realization with BPMN BPDs. In contrast, the control group did not use the BUCM. Representations were then evaluated for overall semantic and syntactic quality by extracting metrics from known literature. Both groups have similar syntactic quality at a granular level. Nevertheless, BPMN BPDs created using TDMA are more complete: required tasks in process execution are significantly more present. An increase in completeness can be beneficial in understanding complex organizations and facilitate modular software development. Alternatively, the diagrams were significantly more complex with more linearly independent paths within workflows than needed.
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-89022-3
SN  - 978-3-030-89021-6
PY  - 2021
VL  - 13011
SP  - 74
EP  - 84
DO  - 10.1007/978-3-030-89022-3_7
AN  - WOS:000711885700007
ER  -

TY  - JOUR
AU  - Arshad, N
AU  - Butt, TA
AU  - Iqbal, M
TI  - A Comprehensive Framework for Intelligent, Scalable, and Performance-Optimized Software Development
T2  - IEEE ACCESS
AB  - Integrating Artificial Intelligence (AI) into the Software Development Life Cycle (SDLC) has become necessary to enhance efficiency, scalability, and performance in modern software systems. Instead of incorporating the AI functionality into their SDLC, traditional SDLC models typically add-on the AI software functionality after they have integrated AI functionality into their application or software process. Because of this, developers undergo inefficiencies in their development workflows, experience performance bottlenecks during testing, and experience challenges of incorporating AI to improve an application's performance through optimization. This paper proposes a new AI-Optimized Software Development Life Cycle (AI-SDLC), which is a holistic and comprehensive framework that encases the embedded AI capabilities and optimization strategies throughout the SDLC process during every stage of the system development, so that requirements-gathering, development, testing, and maintenance are hybrid software processes and not dictated by AI vs. traditional software development processes. AI-SDLC presents new development roles, such as AI Integration Specialist, Code Optimizer, and UX Optimization Specialist, which helps developers work across disciplines and increases collaborative interaction between traditional developers and AI engineers. AI-SDLC also utilizes an AI-driven automated hybrid software process in areas such as requirement elicitation, design/architecture validation, testing, deployment monitoring, and scalability to produce robust high-performance systems in all areas of practicing software development life cycle work. The discourse includes a rich case study based on a Smart Logistics Management System to demonstrate practical implementation of the AI-SDLC and how it facilitates improvement in system efficiency and improved user experience. Additionally, the discussion also highlights the possibilities of AI-SDLC practical implementation in other industrial domain areas such as e-Commerce, finance, aviation and enterprise solution based projects with practical considerations for implementation. In conclusion, the discussion provides findings that support AI-SDLC as a structured and intelligence-driven approach to Software Development Life Cycle implementation that addresses the weaknesses of traditional software design and development frameworks.
SN  - 2169-3536
PY  - 2025
VL  - 13
SP  - 74062
EP  - 74077
DO  - 10.1109/ACCESS.2025.3564139
AN  - WOS:001481864700015
ER  -

TY  - JOUR
AU  - Carrillo, F
AU  - Esfandiari, H
AU  - Mueller, S
AU  - von Atzigen, M
AU  - Massalimova, A
AU  - Suter, D
AU  - Laux, CJ
AU  - Spirig, JM
AU  - Farshad, M
AU  - Fuernstahl, P
TI  - Surgical Process Modeling for Open Spinal Surgeries
T2  - FRONTIERS IN SURGERY
AB  - Modern operating rooms are becoming increasingly advanced thanks to the emerging medical technologies and cutting-edge surgical techniques. Current surgeries are transitioning into complex processes that involve information and actions from multiple resources. When designing context-aware medical technologies for a given intervention, it is of utmost importance to have a deep understanding of the underlying surgical process. This is essential to develop technologies that can correctly address the clinical needs and can adapt to the existing workflow. Surgical Process Modeling (SPM) is a relatively recent discipline that focuses on achieving a profound understanding of the surgical workflow and providing a model that explains the elements of a given surgery as well as their sequence and hierarchy, both in quantitative and qualitative manner. To date, a significant body of work has been dedicated to the development of comprehensive SPMs for minimally invasive baroscopic and endoscopic surgeries, while such models are missing for open spinal surgeries. In this paper, we provide SPMs common open spinal interventions in orthopedics. Direct video observations of surgeries conducted in our institution were used to derive temporal and transitional information about the surgical activities. This information was later used to develop detailed SPMs that modeled different primary surgical steps and highlighted the frequency of transitions between the surgical activities made within each step. Given the recent emersion of advanced techniques that are tailored to open spinal surgeries (e.g., artificial intelligence methods for intraoperative guidance and navigation), we believe that the SPMs provided in this study can serve as the basis for further advancement of next-generation algorithms dedicated to open spinal interventions that require a profound understanding of the surgical workflow (e.g., automatic surgical activity recognition and surgical skill evaluation). Furthermore, the models provided in this study can potentially benefit the clinical community through standardization of the surgery, which is essential for surgical training.
SN  - 2296-875X
DA  - JAN 25
PY  - 2022
VL  - 8
C7  - 776945
DO  - 10.3389/fsurg.2021.776945
AN  - WOS:000752607700001
ER  -

TY  - JOUR
AU  - Gschwind, T
AU  - Pinggera, J
AU  - Zugal, S
AU  - Reijers, HA
AU  - Weber, B
TI  - A linear time layout algorithm for business process models
T2  - JOURNAL OF VISUAL LANGUAGES AND COMPUTING
AB  - The layout of a business process model influences how easily it can be understood. Existing layout features in process modeling tools often rely on graph representations, but do not take the specific properties of business process models into account. In this paper, we propose an algorithm that is based on a set of constraints which are specifically identified toward establishing a readable layout of a process model. Our algorithm exploits the structure of the process model and allows the computation of the final layout in linear time. We explain the algorithm, show its detailed run-time complexity, compare it to existing algorithms, and demonstrate in an empirical evaluation the acceptance of the layout generated by the algorithm. The data suggests that the proposed algorithm is well perceived by moderately experienced process modelers, both in terms of its usefulness as well as its ease of use. (C) 2013 Elsevier Ltd. All rights reserved.
SN  - 1045-926X
SN  - 1095-8533
DA  - APR
PY  - 2014
VL  - 25
IS  - 2
SP  - 117
EP  - 132
DO  - 10.1016/j.jvlc.2013.11.002
AN  - WOS:000334568900006
ER  -

TY  - CPAPER
AU  - Thomas, L
AU  - Kumar, MVM
AU  - Annappa, B
ED  - Batra, U
ED  - Sujata
ED  - Arpita
TI  - Efficient Process Mining through Critical Path Network Analysis
T2  - SOUVENIR OF THE 2014 IEEE INTERNATIONAL ADVANCE COMPUTING CONFERENCE (IACC)
CP  - 4th IEEE International Advance Computing Conference (IACC)
AB  - Process mining is emerging scientific research discipline, concentrates on discovering, monitoring and enhancing the operational processes using the operational traces of the process documented in log. Process mining enables the process centric analysis of the data and aims at bridging gap between data mining, business process modeling and analysis. This article analyses use of Critical Path Method used in project management, in the context of process mining in order to find critical paths in process model. This article aims in leveraging process mining practices with the application of CPM and study its feasibility. Critical path identifies the minimum time possible to finish the project. Extra care must be taken while executing activities on critical path. Delay in any of the activities on critical path would definitely delay the process completion time and collapse overall process plan.
SN  - 2164-8263
SN  - 978-1-4799-2571-1
PY  - 2014
SP  - 511
EP  - 516
AN  - WOS:000349866100093
ER  -

TY  - CPAPER
AU  - Georgakopoulos, D
AU  - Tsalgatidou, A
ED  - Dogac, A
ED  - Kalinichenko, L
ED  - Ozsu, MT
ED  - Sheth, A
TI  - Technology and tools for comprehensive business process lifecycle management
T2  - WORKFLOW MANAGEMENT SYSTEMS AND INTEROPERABILITY
CP  - NATO Advanced Study Institute on Workflow ManagementSystems (WFMS)
AB  - Business processes are collections of one or more linked activities which realize a business objective or policy goal, such as fulfilling a business contract, and/or satisfying a specific customer need. The lifecycle of a business process involves everything from capturing the process in a computerized representation to automating the process. This typically includes specific steps for measuring, evaluating, and improving the process. Currently, commercially available workflow management systems (WFMSs) and business process modeling fools (BPMTs) provide for complementary aspects of business process lifecycle management. Furthermore, new concepts and interoperating tools in these categories are emerging to provide comprehensive support for managing the entire business process lifecycle. In this paper we provide an overview and an evaluation of the process modeling, analysis, automation, and coordination capabilities provided by integrated BPMTs and WFMSs. We also discuss how state of the art WFMSs and BPMTs can interoperate to provide complete support for the entire business process lifecycle. Although we occasionally discuss research issues, we mainly focus on the state of the art in commercially available technology.
SN  - 0258-1248
SN  - 3-540-64411-3
PY  - 1998
VL  - 164
SP  - 356
EP  - 395
AN  - WOS:000078893000016
ER  -

TY  - JOUR
AU  - Wang, YX
AU  - Bryant, A
TI  - Process-based software engineering: Building the infrastructures - Editors' introduction
T2  - ANNALS OF SOFTWARE ENGINEERING
AB  - A recent trend in software engineering is the shift from a focus on laboratory-oriented software engineering to a more industry-oriented view of software engineering processes. This complements preceding ideas about software engineering in terms of organization and process-orientation. From the domain coverage point of view, many of the existing software engineering approaches have mainly concentrated on the technical aspects of software development. Important areas of software engineering, such as the technical and organizational infrastructures, have been left untouched. As software systems increase in scales, issues of complexity and professional practices become involved. Software development as an academic or laboratory activity, has to engage with software development as a key industrialized process.
   This expanded domain of software engineering exposes the limitations of existing methodologies that often address only individual subdomains. There is, therefore, a demand for an overarching approach that provide a basis for theoretical and practical infrastructures capable of accommodating the whole range of modern software engineering practices and requirements. One approach is provided by Process-Based Software Engineering (PBSE); part of the more general trend towards a focus on process.
   This paper provides a review of process techniques for software engineering and a high-level perspective on PBSE. Typical approaches and techniques for the establishment, assessment, improvement and bench-marking of software engineering process systems are introduced in this paper, and many are developed further in other contributions to this volume.
SN  - 1022-7091
DA  - DEC
PY  - 2002
VL  - 14
IS  - 1-4
SP  - 9
EP  - 37
DO  - 10.1023/A:1020537121530
AN  - WOS:000178531400001
ER  -

TY  - JOUR
AU  - Ongari, D
AU  - Yakutovich, AV
AU  - Talirz, L
AU  - Smit, B
TI  - Building a Consistent and Reproducible Database for Adsorption Evaluation in Covalent-Organic Frameworks
T2  - ACS CENTRAL SCIENCE
AB  - We present a workflow that traces the path from the bulk structure of a crystalline material to assessing its performance in carbon capture from coal's postcombustion flue gases. This workflow is applied to a database of 324 covalent-organic frameworks (COFs) reported in the literature, to characterize their CO2 adsorption properties using the following steps: (1) optimization of the crystal structure (atomic positions and unit cell) using density functional theory, (2) fitting atomic point charges based on the electron density, (3) characterizing the pore geometry of the structures before and after optimization, (4) computing carbon dioxide and nitrogen isotherms using grand canonical Monte Carlo simulations with an empirical interaction potential, and finally, (5) assessing the CO2 parasitic energy via process modeling. The full workflow has been encoded in the Automated Interactive Infrastructure and Database for Computational Science (AiiDA). Both the workflow and the automatically generated provenance graph of our calculations are made available on the Materials Cloud, allowing peers to inspect every input parameter and result along the workflow, download structures and files at intermediate stages, and start their research right from where this work has left off. In particular, our set of CURATED (Clean, Uniform, and Refined with Automatic Tracking from Experimental Database) COFs, having optimized geometry and high-quality DFT-derived point charges, are available for further investigations of gas adsorption properties. We plan to update the database as new COFs are being reported.
SN  - 2374-7943
SN  - 2374-7951
DA  - OCT 23
PY  - 2019
VL  - 5
IS  - 10
SP  - 1663
EP  - 1675
DO  - 10.1021/acscentsci.9b00619
AN  - WOS:000492698300009
ER  -

TY  - CPAPER
AU  - Hong, TP
AU  - Soh, AC
AU  - Jaafar, H
AU  - Ishak, AJ
A1  - IEEE
TI  - Real-Time Monitoring System for Parking Space Management Services
T2  - 2013 IEEE CONFERENCE ON SYSTEMS, PROCESS & CONTROL (ICSPC)
CP  - IEEE Conference on Systems, Process and Control (ICSPC)
AB  - Real-Time Monitoring System for Parking Space Management Services is the evolution of traditional parking system that it does not only provide live information to users in order to make it easy for them to look for vacant parking lot, it also give authority to operators to monitor and perform simulations to illustrate the real parking system. The purpose of this project is to apply the principles of queue theory into parking system modeling. The queue system model enables the predictions of arrival and service time in the system through analysis and calculations. Aside of it, Graphical User Interface (GUI) is also designed and integrated into the parking system in order to allow parking zone operators to monitor the status of the parking lots and view the statistics of arrival rate, service time and so on. This project focuses on the system modeling and software development and implementation. It is expected that this project will be able to assist parking zone operators in designing their system that will work efficiently and generate high income. It will also provide convenience to the operators in managing the parking zone remotely. Users will also experience a time-saving and stress-free parking zone.
SN  - 978-1-4799-2208-6
PY  - 2013
SP  - 149
EP  - 153
AN  - WOS:000346583800031
ER  -

TY  - CHAP
AU  - Lassen, KB
AU  - van der Aalst, WMP
ED  - Meersman, R
ED  - Tari, Z
TI  - WorkflowNet2BPEL4WS: A tool for translating unstructured workflow processes to readable BPEL
T2  - ON THE MOVE TO MEANINGFUL INTERNET SYSTEMS 2006: COOPIS, DOA, GADA, AND ODBAS, PT 1, PROCEEDINGS
CP  - On the Move Confederated International Conference on CoopIS/DOA/GADA, and ODBASE
AB  - This paper presents WorkflowNet2BPEL4WS a tool to automatically map a graphical workflow model expressed in terms of Workflow Nets (WF-nets) onto BPEL. The Business Process Execution Language for Web Services (BPEL) has emerged as the de-facto standard for implementing processes and is supported by an increasing number of systems (cf. the IBM WebSphere Choreographer and the Oracle BPEL Process Manager). While being a powerful language, BPEL is difficult to use. Its XML representation is very verbose and only readable for the trained eye. It offers many constructs and typically things can be implemented in many ways, e.g., using links and the flow construct or using sequences and switches. As a result only experienced users are able to select the right construct. Some vendors offer a graphical interface that generates BPEL code. However, the graphical representations are a direct reflection of the BPEL code and not easy to use by end-users. Therefore, we provide a mapping from WF-nets to BPEL. This mapping builds on the rich theory of Petri nets and can also be used to map other languages (e.g., UML, EPC, BPMN, etc.) onto BPEL. To evaluate WorkflowNet2BPEL4WS we used more than 100 processes modeled using Protos (the most widely used business process modeling tool in the Netherlands), automatically converted these into CPN Tools, and applied our mapping. The results of these evaluation are very encouraging and show the applicability of our approach.
SN  - 0302-9743
SN  - 1611-3349
SN  - 3-540-48287-3
PY  - 2006
VL  - 4275
SP  - 127
EP  - 144
AN  - WOS:000243131600009
ER  -

TY  - JOUR
AU  - Cesarano, A
TI  - Monitoring unit of technology to manage knowledge in the development of free information technologies at the national center of information technologies (CNTI)
T2  - TELEMATIQUE
AB  - This research is focus in the Designing of a Monitoring Unit of Technology to manage knowledge in the development of free software information technologies at the National Center of Information Technologies (CNTI), supported by Sanchez (2011), Castellanos (2005), Escorsa, Ortiz and Maspons (2002), as theoretical authors. This research is classify as a descriptive research. The design as non-experimental field, in the form of feasible project, based on an assessment of the substantive area at CNTI this is why their sample consisted of: Executive Director, Office of Process Modeling, Office of Strategic Planning, Management State Care, Technology Management Training, Project Management Division, Technology and Management-Operations. The Reporting units or sources were established by 14 subjects serving as director, manager or coordinator, with two (2) or more years of experience in managing external information, a questionnaire for data collection was used. The questionnaire has 30 closed questions on a interval measurement scale, the questionnaire was validated by the judgment of three experts in the area of interest. The reliability of the questionnaire was established through a pilot test applied to 3 subjects in the areas of study, using the test / retest method for calculating the coefficient of Alpha Cronbach resulting in a value of 0.928, which is regarded as highly reliable. It is important to note that the data collected was treated through the use of percentages and frequencies, represented in bar graphs through the used of a spreadsheet. It was concluded that the Monitoring of Technology can meet the needs of users in the processes for making decisions based on the information available outside the institution; under a new approach to model knowledge management.
SN  - 1856-4194
PY  - 2015
VL  - 14
IS  - 1
SP  - 31
EP  - 57
AN  - WOS:000215580700003
ER  -

TY  - CPAPER
AU  - Zhai, J
AU  - Yang, QS
AU  - Yang, Y
AU  - Xiao, JC
AU  - Wang, Q
AU  - Li, MS
ED  - Berkling, K
ED  - Joseph, M
ED  - Meyer, B
ED  - Nordio, M
TI  - Automated Process Quality Assurance for Distributed Software Development
T2  - SOFTWARE ENGINEERING APPROACHES FOR OFFSHORE AND OUTSOURCED DEVELOPMENT
CP  - 2nd International Conference on Software Engineering Approaches for Offshore and Outsourced Development
AB  - As required or implicated in many process improvement or assessment models, Process Quality Assurance (PQA) is introduced to objectively evaluate actual software processes against applicable processes descriptions, standards, and procedures and to identify potential noncompliance. In a Distributed Software Development (DSD) environment, PQA is also an absolute necessity to ensure that each development site behaves as expected and high quality software is collaboratively developed. However, several problems brought by the distribution nature of DSD, such as different interpretations of standard processes among development sites, inconsistent criteria for identifying noncompliance, visibility into development activities of all sites being challenging, hidden conflicts or noncompliance for political issues within a site, substantial investment in setting tip PQA teams in each site etc., can undermine the objectivity and effectiveness of PQA activities. To alleviate these problems, we introduce an approach in this paper that automates PQA activities for some routine checking tasks in a DSD environment. In the approach, a process model describing the actual software process is automatically built from each site's repository and, then, the model is checked against logic formulae derived from a common checklist to detect noncompliance. Experiment results show that the approach is helpful to ensure that PQA activities in each site can be conducted according to the same guideline and the objectivity of PQA results is guaranteed.
SN  - 1865-1348
SN  - 978-3-642-01855-8
PY  - 2009
VL  - 16
SP  - 196
EP  - 210
AN  - WOS:000267540000014
ER  -

TY  - CPAPER
AU  - Matinnejad, R
AU  - Ramsin, R
A1  - IEEE
TI  - An Analytical Review of Process-Centered Software Engineering Environments
T2  - 2012 IEEE 19TH INTERNATIONAL CONFERENCE AND WORKSHOPS ON ENGINEERING OF COMPUTER BASED SYSTEMS (ECBS)
CP  - 19h IEEE International Conference and Workshops on Engineering of Computer-Based Systems (ECBS)
AB  - Process-centered Software Engineering Environments, or PSEEs, are intended for the definition, modification, and enactment of software process models; they thus bring software development processes into effect. Even though research efforts in process-centered software engineering abound, PSEE technology has not received the attention that it deserves. In order to create a concise but effective and practically applicable evaluation framework for PSEEs, this paper first presents a survey of PSEEs and highlights the current state of the art of the technology. The PSEEs which have been reviewed herein have been regarded as software systems, and as such, have been characterized in terms of their requirements. After providing a conceptual critique of the scope and nature of conventional PSEEs, a detailed criteria-based evaluation of a select set of several recent PSEEs has been conducted. The evaluation criteria have been derived from PSEE requirements and the results of the critique, and have then been refined and evolved into the final criterion set.
SN  - 978-0-7695-4664-3
PY  - 2012
SP  - 64
EP  - 73
DO  - 10.1109/ECBS.2012.11
AN  - WOS:000308965100010
ER  -

TY  - JOUR
AU  - Stroppi, LJR
AU  - Chiotti, O
AU  - Villarreal, PD
TI  - Defining the resource perspective in the development of processes-aware information systems
T2  - INFORMATION AND SOFTWARE TECHNOLOGY
AB  - Context: The resource perspective has impact on the performance of business processes. However, current Workflow Management Systems (WfMSs) provide disparate support to its implementation and business process modeling languages provide limited capabilities for its definition. Thus, it is difficult to specify requirements regarding this perspective and to select an appropriate WfMS to support them in order to obtain a technological solution aligned with the organizational needs.
   Objective: To provide support to the definition, implementation, verification and validation of resource perspective requirements in the development of Process-Aware Information Systems (PAISs) based on WfMSs.
   Method: The following activities were carried out: (i) identification of resource perspective aspects in executable workflow specifications, (ii) analysis of the elements provided by the BPMN modeling language to represent these aspects, (iii) development of a framework based on BPMN for defining and implementing these aspects by using the extension mechanism provided by this language, (iv) development of a model-driven development method that leverages the framework to develop PAISs, and (v) validation of the proposed framework and method through the development of a tool supporting them, a case study, and the evaluation against the Workflow Resource Patterns.
   Results: A framework, a method and a tool that support the definition of the resource perspective in the development of PAISs.
   Conclusion: By using the proposed framework and method, practitioners are able to: define the resource perspective requirements in conceptual process models, select a WfMS as implementation platform, and define the implementation of these requirements maintaining the consistency between the conceptual process models and the workflow specifications. (C) 2014 Elsevier B.V. All rights reserved.
SN  - 0950-5849
SN  - 1873-6025
DA  - MAR
PY  - 2015
VL  - 59
SP  - 86
EP  - 108
DO  - 10.1016/j.infsof.2014.10.006
AN  - WOS:000349427200006
ER  -

TY  - JOUR
AU  - Riesner, M
AU  - Durand-Riard, P
AU  - Hubbard, J
AU  - Plesch, A
AU  - Shaw, JH
TI  - Building Objective 3D Fault Representations in Active Tectonic Settings
T2  - SEISMOLOGICAL RESEARCH LETTERS
AB  - Developing 3D representations of active faults is an important step to improve seismic-hazard assessment. However, the geometries of faults can be difficult to constrain at depth, and building representations is often subjective. We present a new objective workflow to build 3D fault geometries from surface and subsurface data that are generally available in active tectonic environments. We use surface traces, focal mechanism orientations, and relocated hypocenters as geological constraints in an implicit modeling approach. This method enables us to control the weights assigned to the different constraints, increasing the accuracy of the fault model. We evaluate and refine our method by applying it to a well-known natural case study: the Puente Hills thrust fault, a blind thrust beneath Los Angeles, California, that is imaged by high-quality seismic reflection data and that generated the 1987 M-w 6.0 Whittier Narrows earthquake. Then, we apply our new workflow to the Xianshuihe-Anninghe left-lateral strike-slip fault systems, China. Implementing this workflow allows for the development of improved fault surface representations that can contribute to Community Fault Models and support fault system modeling, rupture simulations, and regional hazard assessments.
SN  - 0895-0695
SN  - 1938-2057
DA  - MAY-JUN
PY  - 2017
VL  - 88
IS  - 3
SP  - 831
EP  - 839
DO  - 10.1785/0220160192
AN  - WOS:000416173300009
ER  -

TY  - CPAPER
AU  - Phung, TV
AU  - Tzouanas, V
A1  - ASEE
TI  - AC 2012-3507: DESIGN AND CONTROL OF A TWIN TANK WATER PROCESS
T2  - 2012 ASEE ANNUAL CONFERENCE
CP  - ASEE Annual Conference
AB  - The paper is concerned with the design of a twin tank water process and experimental evaluation of feedback and cascade control structures to achieve a desired water level in the second tank by adjusting the water supply to the first tank (feedback only structure) and the water level setpoint of the first tank (feedback/cascade structure). Detailed, first principles-based, dynamic models of this non-linear and interactive process have been developed and compared to experimental data. Furthermore, this experimental study entails and discusses the design of the twin tank process and associated instrumentation, real time data acquisition and control in LabView, process modeling, controller design, and evaluation of the performance of different control structures in a closed loop manner. This work was performed in partial fulfillment of the requirements of the Senior Capstone Project course in controls and instrumentation of the Engineering Technology department at the University of Houston-Downtown. Student experiences are summarized and the need for effective project management methods is emphasized.
SN  - 2153-5965
PY  - 2012
AN  - WOS:000380250104046
ER  -

TY  - JOUR
AU  - Ouyang, C
AU  - Dumas, M
AU  - Van der Aalst, WMP
AU  - Ter Hofstede, AHM
AU  - Mendling, J
TI  - From Business Process Models to Process-Oriented Software Systems
T2  - ACM TRANSACTIONS ON SOFTWARE ENGINEERING AND METHODOLOGY
AB  - Several methods for enterprise systems analysis rely on flow-oriented representations of business operations, otherwise known as business process models. The Business Process Modeling Notation (BPMN) is a standard for capturing such models. BPMN models facilitate communication between domain experts and analysts and provide input to software development projects. Meanwhile, there is an emergence of methods for enterprise software development that rely on detailed process definitions that are executed by process engines. These process definitions refine their counterpart BPMN models by introducing data manipulation, application binding, and other implementation details. The de facto standard for defining executable processes is the Business Process Execution Language (BPEL). Accordingly, a standards-based method for developing process-oriented systems is to start with BPMN models and to translate these models into BPEL definitions for subsequent refinement. However, instrumenting this method is challenging because BPMN models and BPEL definitions are structurally very different. Existing techniques for translating BPMN to BPEL only work for limited classes of BPMN models. This article proposes a translation technique that does not impose structural restrictions on the source BPMN model. At the same time, the technique emphasizes the generation of readable (block-structured) BPEL code. An empirical evaluation conducted over a large collection of process models shows that the resulting BPEL definitions are largely block-structured. Beyond its direct relevance in the context of BPMN and BPEL, the technique presented in this article addresses issues that arise when translating from graph-oriented to block-structure flow definition languages.
SN  - 1049-331X
SN  - 1557-7392
DA  - AUG
PY  - 2009
VL  - 19
IS  - 1
C7  - 2
DO  - 10.1145/1555392.1555395
AN  - WOS:000269266700002
ER  -

TY  - CPAPER
AU  - Evron, Y
AU  - Soffer, P
AU  - Zamansky, A
ED  - Teniente, E
ED  - Weidlich, M
TI  - Design-Time Analysis of Data Inaccuracy Awareness at Runtime
T2  - BUSINESS PROCESS MANAGEMENT WORKSHOPS (BPM 2017)
CP  - 15th International Conference on Business Process Management (BPM)
AB  - Analyzing potential data inaccuracy is an important aspect of business process design that has been mostly overlooked so far. To this end, process models should express the relevant information to support such analysis. In this paper we propose a formal framework for design-time analysis of potential data inaccuracy situations. In particular, we define a property of Data Inaccuracy Awareness which indicates the ability to know at runtime whether data values are accurate representations of real values. We propose an algorithm for analyzing this property at design time based on a process model. A preliminary evaluation of the applicability and scalability of the algorithm using a benchmark collection of process models is reported.
SN  - 1865-1348
SN  - 978-3-319-74030-0
SN  - 978-3-319-74029-4
PY  - 2018
VL  - 308
SP  - 600
EP  - 612
DO  - 10.1007/978-3-319-74030-0_47
AN  - WOS:000434845200054
ER  -

TY  - JOUR
AU  - Gao, H
AU  - Wu, XM
AU  - Ding, XS
TI  - A Geologically Informed and Data-Driven AI Workflow for Fully Seismic Stratigraphic Interpretation of Sedimentary Basin
T2  - IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING
AB  - The seismic stratigraphic interpretation of clinothems is essential for regional basin stratigraphy analysis, hydrocarbon reservoir evaluation, and basin evolutionary history reconstruction. The traditional interpretation process remains a tedious, labor intensive, expert dependent, and highly subjective process, often resulting in high uncertainty and leaving large portions of seismic data uninterpreted. Data-driven AI approaches are promising to automate this process, but still face significant challenges, including the absence of training datasets, poor generalizability, weak interpretability, and physical inconsistency. To address these challenges, we develop a data-driven and geologically informed AI workflow that integrates geological knowledge into both labeled dataset construction and AI model training. For the labeled dataset construction, we develop a geological and geophysical forward modeling workflow to simulate a massive-scale labeled synthetic dataset, encompassing diverse geological models. For the model training, we incorporate both labeled synthetic and unlabeled field datasets to jointly train a network utilizing a hybrid loss with labeled supervision and geologically informed constraints, enabling high precision, high resolution, strong robustness, and high-generalizability interpretation of all stratigraphic features within seismic data. This workflow significantly accelerates and simplifies the traditional interpretation process, achieving seismic pixel-level precision and providing detailed geological insights of the origin and architecture of sedimentary basins. In addition, our workflow lays the groundwork for constructing a refined and quantitative sequence stratigraphic framework and can be easily extended to various geological scenarios, such as depositional system tract interpretation, shoreline or shelf-edge trajectory tracking, stratigraphic completeness analysis, sea-level curve prediction, and sedimentary evolutionary history reconstruction.
SN  - 0196-2892
SN  - 1558-0644
PY  - 2025
VL  - 63
C7  - 5917413
DO  - 10.1109/TGRS.2025.3583211
AN  - WOS:001536766400008
ER  -

TY  - JOUR
AU  - Agh, H
AU  - Ramsin, R
TI  - A pattern-based model-driven approach for situational method engineering
T2  - INFORMATION AND SOFTWARE TECHNOLOGY
AB  - Context: Constructing bespoke software development methodologies for specific project situations has become a crucial need, giving rise to Situational Method Engineering (SME). Compared with Software Engineering, SME has a long way to go yet; SME approaches are especially deficient as to support for modeling, portability, and automation. Model-Driven Development (MDD) has been effectively used for addressing these issues in Software Engineering, and is also considered a promising approach for resolving them in SME.
   Objective: This paper aims to address the shortcomings of existing SME approaches by introducing a novel MDD approach, specifically intended for SME purposes, that uses a pattern-based approach for model transformation.
   Method: Developing a MDD approach for SME requires that a modeling framework, consisting of modeling levels, be defined for modeling software development methodologies. Transformation patterns should also be specified for converting the models from one level to the next. A process should then be defined for applying the framework and transformations patterns to real SME projects. The resulting MDD approach requires proper evaluation to demonstrate its applicability.
   Results: A framework and a semi-automated process have been proposed that adapt pattern-based model transformation techniques for application to the methodology models used in SME. The transformation patterns have been implemented in the Medini-QVT model transformation tool, along with two supplementary method bases: one for mapping the situational factors of SME projects to requirements, and the other for mapping the requirements to method fragments. The method engineer can produce the methodology models by using the method bases and executing the transformation patterns via the tool.
   Conclusion: The validity of the proposed approach has been assessed based on special evaluation criteria, and also through application to a real-world project. Evaluation results indicate that the proposed approach addresses the deficiencies of existing approaches, and satisfies the practicality requirements of SME approaches. (C) 2016 Elsevier B.V. All rights reserved.
SN  - 0950-5849
SN  - 1873-6025
DA  - OCT
PY  - 2016
VL  - 78
SP  - 95
EP  - 120
DO  - 10.1016/j.infsof.2016.05.010
AN  - WOS:000379634100006
ER  -

TY  - CPAPER
AU  - de Lima, IB
AU  - Yamada, DB
AU  - Yoshiura, VT
AU  - Lance, RC
AU  - Rodrigues, LML
AU  - Vinci, ALT
AU  - Martinho, R
AU  - de Pádua, SD
AU  - Rijo, RPCL
AU  - Furegato, ARF
AU  - Alves, D
ED  - QuintelaVarajao, JE
ED  - CruzCunha, MM
ED  - Martinho, R
ED  - Rijo, R
ED  - Domingos, D
ED  - Peres, E
TI  - Proposal for selection of mental health indicators in the management of health networks: from heuristic to process modeling
T2  - CENTERIS 2018 - INTERNATIONAL CONFERENCE ON ENTERPRISE INFORMATION SYSTEMS / PROJMAN 2018 - INTERNATIONAL CONFERENCE ON PROJECT MANAGEMENT / HCIST 2018 - INTERNATIONAL CONFERENCE ON HEALTH AND SOCIAL CARE INFORMATION SYSTEMS AND TECHNOLOGIES, CENTERI
CP  - International Conference on Health and Social Care Information Systems and Technologies (HCIST)
AB  - In 2011, Brazil instituted the Psychosocial Care Network (RAPS) for people suffering from mental disorders and needs arising from the use of alcohol and other drugs within the Unified Health System an electronic information system that supports the planning, regulation, control and evaluation of mental health actions. Several studies have been done to select a complete set of indicators for mental health in Brazil; however, these studies do not describe how these principles are transposed into indicators. In this work we will use Business Process Management Notation (BPMN) to analyze the process of patient care in a mental health unit and to search through this analysis to select and map those that apply to the specific RAPS institutions. We propose a method, using process modeling along the Thornicroft and Tansella matrix, to select the relevant indicators and their mapping along the network. The method was effective allowing the application of indicators to the concrete reality of a psychiatric hospital in Brazil. (C) 2018 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/) Selection and peer-review under responsibility of the scientific committee of the CENTERIS-International Conference on ENTERprise Information Systems / ProjMAN-International Conference on Project MANagement / HCist-International Conference on Health and Social Care Information Systems and Technologies.
SN  - 1877-0509
PY  - 2018
VL  - 138
SP  - 185
EP  - 190
DO  - 10.1016/j.procs.2018.10.026
AN  - WOS:000550164600025
ER  -

TY  - JOUR
AU  - Simic, SD
AU  - Tankovic, N
AU  - Etinger, D
TI  - Big data BPMN workflow resource optimization in the cloud
T2  - PARALLEL COMPUTING
AB  - Cloud computing is one of the critical technologies that meet the demand of various businesses for the high -capacity computational processing power needed to gain knowledge from their ever-growing business data. When utilizing cloud computing resources to deal with Big Data processing, companies face the challenge of determining the optimal use of resources within their business processes. The miscalculation of the necessary resources directly affects their budget and can cause delays in the cycle time of their key processes. This study investigates the simulation of cloud resource optimization for Big Data workflows modeled with the Business Process Modeling Notation (BPMN). To this end, a BPMN performance evaluation framework was developed. The framework's capabilities were presented using real-world data science workflow and later evaluated on workflows consisting of 13, 52, and 104 tasks. The results show that the developed framework is adequate for estimating the overall run-time distribution and optimizing the cloud resource deployment and that the BPMN can be utilized for Big Data processing workflows. Therefore, this study contributes to BPMN practitioners by providing a tool to apply BPMN for their Big Data workflows and decision-makers by giving them critical insights into their key business processes. The framework source code is available at https://github.com/ntankovic/python-bpmn-engine.
SN  - 0167-8191
SN  - 1872-7336
DA  - SEP
PY  - 2023
VL  - 117
C7  - 103025
DO  - 10.1016/j.parco.2023.103025
C6  - JUN 2023
AN  - WOS:001028838400001
ER  -

TY  - JOUR
AU  - Sugino, T
AU  - Kawahira, H
AU  - Nakamura, R
TI  - Comprehensive Surgical Task Analysis on Image-Guided Surgery
T2  - JOURNAL OF MEDICAL IMAGING AND HEALTH INFORMATICS
AB  - Background: Surgical workflow analysis has recently been studied to optimize complex surgical processes and assist in learning of surgical skills. Two important challenges are acquiring detailed task information for the analysis without affecting the surgical environment and deriving comprehensive evaluation of a surgeon's skill and the surgical process based on the task information. Thus, the purpose of this study was to present a comprehensive method for achieving both quantification of a surgeon's skill level and comparative evaluation of the surgical process in laparoscopic surgery on the basis of navigation information only. Material and methods: Laparoscopic cholecystectomy was selected as the target surgery and the task features during the dissecting tasks, which are especially important for the procedure, were parameterized and analyzed. The proposed approach uses these parameters to quantify a surgeon's skill level based on a multiple regression analysis, and to create and compare surgical process models based on a multiple alignment algorithm. Results: A total of 37 procedures of simulated laparoscopic cholecystectomy were analyzed for validation of the proposed approach. From the experimental results, we found that the multiple regression analysis reasonably quantified the surgeons' skill levels, and that the clustering for comparison of the surgical process models reflected the features of surgical performance during the procedure. Conclusions: This study indicates the availability of navigation information for quantitatively evaluating comprehensive surgical performance during laparoscopic surgery.
SN  - 2156-7018
SN  - 2156-7026
DA  - AUG
PY  - 2017
VL  - 7
IS  - 4
SP  - 780
EP  - 787
DO  - 10.1166/jmihi.2017.2103
AN  - WOS:000408355900006
ER  -

TY  - JOUR
AU  - TATE, G
TI  - SOFTWARE PROCESS MODELING AND METRICS - A CASE-STUDY
T2  - INFORMATION AND SOFTWARE TECHNOLOGY
AB  - The paper's specific concern is with software process modelling for the measurement of rework during application development using computer-aided software engineering (CASE) tools. In order to measure aspects of software development, one needs a defined process that models the aspects of interest. The goals of this study are rather different from those of most other software process modelling studies which are typically concerned with process definition and understanding, development standardization, developer guidance, process assessment, evolution and improvement. Most software process models are intrusive, and intentionally so. In contrast, our model is as non-intrusive as is practicable to observe and measure certain aspects of CASE development in order to improve our understanding of software cost and schedule overruns. Model requirements and selection are described. The interlocking mechanisms of software process enaction and measurement are examined and placed within a larger architectural framework including process engineering and management. Some results and experiences with a small case study are presented including the observation that, although quite high-powered tools seem to be necessary for comprehensive software process modelling, much more modest tools may suffice for simpler, more specific, goal-oriented studies.
SN  - 0950-5849
DA  - JUN-JUL
PY  - 1993
VL  - 35
IS  - 6-7
SP  - 323
EP  - 330
DO  - 10.1016/0950-5849(93)90002-K
AN  - WOS:A1993LV91400002
ER  -

TY  - JOUR
AU  - Pastrana-Pardo, MA
AU  - Ordóñez-Erazo, HA
AU  - Cobos-Lozada, CA
TI  - Process Model Represented in BPMN for Guiding the Implementation of Software Development Practices in Very Small Companies Harmonizing DEVOPS and SCRUM
T2  - REVISTA FACULTAD DE INGENIERIA, UNIVERSIDAD PEDAGOGICA Y TECNOLOGICA DE COLOMBIA
AB  - The business process model is a graphic representation mechanism that helps improve the understanding of a context, the steps undertaken, and the validations and business rules that are part of its universe. This article proposes an implementation model of practices for software development based on DevOps suggestions and how these might be executed within Scrum by the Scrum Development Team (SDT). Present a practice implementation model that integrates DevOps suggestions to be executed by a scrum development team (SDT). The practices for software development based on DevOps were identified. The moment in which the information provided is helpful for the team's continuous improvement within SCRUM was determined. With the practices identified, modeling the general process of implementing practices using BPMN was conducted, followed by detailed modeling. Finally, experts executed the evaluation of the detailed process model. A 12-question survey was implemented to understand the business process model created for implementing practices. This instrument was then made available to experts in the field to obtain feedback on what has been done. The results obtained are promising. The set of practices suggested by DevOps and its integration in Scrum allows for establishing a preventive quality approach for the best development of software products. Using business process models represented by BPMN allows companies to understand and adopt the proposed practices quickly.
SN  - 0121-1129
SN  - 2357-5328
DA  - OCT-DEC
PY  - 2022
VL  - 31
IS  - 62
C7  - e15207
DO  - 10.19053/01211129.v31.n62.2022.15207
AN  - WOS:000922334300001
ER  -

TY  - CPAPER
AU  - Nieto-Julián, E
AU  - Antón, D
AU  - Lara, ML
ED  - Mazzolani, FM
ED  - Landolfo, R
ED  - Faggiano, B
ED  - Brando, G
ED  - Cascini, L
ED  - Portioli, FPA
TI  - Applicability of a Parametric and Semantic Model for the HBIM Project of the Church of the Company of Jesus in Quito
T2  - PROTECTION OF HISTORICAL CONSTRUCTIONS, PROHITECH 2025, VOL 1
CP  - 5th International Conference on Protection of Historical Constructions-PROHITECH
AB  - This research addresses the parametric and semantic heritage modelling of the Church of the Company of Jesus in Quito, Ecuador, within the Historic Building Information Modelling (HBIM) context to manage information and support its preventive conservation and maintenance. The construction, architectural and artistic particularities of this emblematic example of the Latin American Baroque of the Jesuit order, built from 1605 to 1765, require an exhaustive 3D digital replica. An accurate model enables programmed actions in a comprehensive management project and further monitoring of the asset.
   Massive data acquisition technologies, e.g., Terrestrial Laser Scanning (TLS) and Structure-from-Motion photogrammetry, greatly help capture the real state of historic constructions. These protocols provide accurate details that enrich the building's intrinsic database-structure, materiality, evolution and metamorphosis-and are constantly updated to support future interventions. A large library of heritage objects that supports the 3D model of the historic building is pursued. To face this challenge, progress must still be made to effectively implement unstructured data organisation processes and semantic classification by architectural and artistic typologies.
   This work addresses the analysis and implementation of semi-automatic modelling techniques for accurate heritage representation. Based on TLS data, this Scan-to-mesh-to-BIM workflow culminates in the creation of 3D parametric objects. The methodological process developed explores several complementary paths to automate modelling processes, whether parts or complete historical systems, to reinforce the HBIM project.
SN  - 2366-2557
SN  - 2366-2565
SN  - 978-3-031-87314-0
SN  - 978-3-031-87312-6
SN  - 978-3-031-87311-9
PY  - 2025
VL  - 595
SP  - 537
EP  - 544
DO  - 10.1007/978-3-031-87312-6_66
AN  - WOS:001484825500064
ER  -

TY  - CPAPER
AU  - Cova, P
AU  - Bernardoni, M
AU  - Bertoluzza, F
A1  - IEEE
TI  - Feedback Control Simulation of Power Electronic Converters for Renewable Energies
T2  - 2009 INTERNATIONAL CONFERENCE ON CLEAN ELECTRICAL POWER (ICCEP 2009), VOLS 1 AND 2
CP  - International Conference on Clean Electrical Power (ICCEP 2009)
AB  - A possible workflow for designing high power electronic converters for using with renewable sources, such as wind energy farms, is proposed. It takes care or both the control strategies and the electrical characteristics of the electron devices (as IGBTs and PiN diodes) used for the converter. The control feedback loop is designed by using a MATLAB/SIMULINK (TM) based model. while a more detailed electron device behavior is investigated with other simulation tools, such as PSPICE, in order to obtain a correct evaluation of switching and conduction power losses and to estimate the efficiency of the converter. Application on a machine-side converter for wind turbine is shown.
SN  - 978-1-4244-2543-3
PY  - 2009
SP  - 390
EP  - 397
AN  - WOS:000275735500067
ER  -

TY  - JOUR
AU  - Almaraz-Rivera, JG
TI  - An Anomaly-based Detection System for Monitoring Kubernetes Infrastructures
T2  - IEEE LATIN AMERICA TRANSACTIONS
AB  - Network monitoring is crucial to analyze infrastructure baselines and alert whenever an abnormal behavior is observed. However, human effort is limited in time and scope since many variables must be considered in real-time. In addition, infrastructures such as Kubernetes are complex by nature since they do not consider fixed equipment from which to gather data; instead, these infrastructures consider distributed, event-driven, and ephemeral containers that make it complicated to capture and track metrics. Artificial Intelligence models have demonstrated high detection rates for anomaly detection; therefore, there is a need to design and implement a global solution to collect complex data and orchestrate the whole Machine Learning Operations workflow. This document shares the findings and learnings from defining a cloud-native Artificial Intelligence infrastructure at Aligo to develop an anomaly-based detection system for monitoring on-premise Kubernetes infrastructures. After Chaos Engineering experiments, it is shown that the resulting deployed system is strong when alerting outliers and that an end-to-end infrastructure has been developed for conducting future Artificial Intelligence projects at the company.
SN  - 1548-0992
DA  - MAR
PY  - 2023
VL  - 21
IS  - 3
SP  - 457
EP  - 465
DO  - 10.1109/TLA.2023.10068850
AN  - WOS:000971687400012
ER  -

TY  - CPAPER
AU  - Schäfer, B
AU  - van der Aa, H
AU  - Leopold, H
AU  - Stuckenschmidt, H
ED  - LaRosa, M
ED  - Sadiq, S
ED  - Teniente, E
TI  - Sketch2BPMN: Automatic Recognition of Hand-Drawn BPMN Models
T2  - ADVANCED INFORMATION SYSTEMS ENGINEERING (CAISE 2021)
CP  - 33rd International Conference on Advanced Information Systems Engineering (CAiSE)
AB  - Despite the widespread availability of process modeling tools, the first version of a process model is often drawn by hand on a piece of paper or whiteboard, especially when several people are involved in its elicitation. Though this has been found to be beneficial for the modeling task itself, it also creates the need to manually convert hand-drawn models afterward, such that they can be further used in a modeling tool. This manual transformation is associated with considerable time and effort and, furthermore, creates undesirable friction in the modeling workflow. In this paper, we alleviate this problem by presenting a technique that can automatically recognize and convert a sketch process model into a digital BPMN model. A key driver and contribution of our work is the creation of a publicly available dataset consisting of 502 manually annotated, hand-drawn BPMN models, covering 25 different BPMN elements. Based on this data set, we have established a neural network-based recognition technique that can reliably recognize and transform hand-drawn BPMN models. Our evaluation shows that our technique considerably outperforms available baselines and, therefore, provides a valuable basis to smoothen the modeling process.
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-030-79382-1
SN  - 978-3-030-79381-4
PY  - 2021
VL  - 12751
SP  - 344
EP  - 360
DO  - 10.1007/978-3-030-79382-1_21
AN  - WOS:000716947800021
ER  -

TY  - CPAPER
AU  - Betz, S
AU  - Caporale, T
ED  - Chen, J
ED  - Yang, LT
TI  - Sustainable Software System Engineering
T2  - 2014 IEEE FOURTH INTERNATIONAL CONFERENCE ON BIG DATA AND CLOUD COMPUTING (BDCLOUD)
CP  - IEEE Fourth International Conference on Big Data and Cloud Computing (BdCloud)
AB  - Sustainability management is one of the upcoming movements in the 21st century. Communal and private organizations are interested in finding and using "sustainable" solutions and practices. But, there is a lack of available solutions and practices. In the 21st century also, software systems and their underlying business processes are ubiquitous and fundamental for most of the organizations of the industrial society. But, until now, sustainability is not considered by software system engineering. Hence, to support the transition to sustainability, one must have sustainability (aspects) integrated into the software systems and the underlying business processes. Consequently, in this paper a holistic approach is presented to support the complete life cycle of sustainable software system engineering. To realize such an approach, the sustainable business processes and sustainable software systems need to be aligned through the mapping of sustainability aspects considering their respective life cycles. This is important because software makes an important contribution to sustainable business processes and, the other way round, sustainable software only can be developed and maintained using the relevant business processes. Moreover, to realize sustainability aware software system engineering sustainability aspects must be integrated into the business processes life cycle and software system engineering life cycle. To this end, We suggest a conceptual model to integrate sustainability aspects in a business process modeling language. Also, to assist the stakeholder when modeling, designing, executing, and monitoring the business processes a process model will be presented developed. Furthermore, to integrate sustainability aspects in software engineering, sustainability needs to be considered in the different phases of the software development process: requirements specification, design, testing, and maintenance. Thus, we suggest a method to integrate sustainability in software development. Finally, a model is presented showing the combined life cycles of sustainable business processes and sustainable software engineering.
SN  - 978-1-4799-6719-3
PY  - 2014
SP  - 612
EP  - 619
DO  - 10.1109/BDCloud.2014.113
AN  - WOS:000380451500091
ER  -

TY  - CPAPER
AU  - Wu, DY
AU  - Wei, J
AU  - Gao, CS
AU  - Dou, WS
ED  - Chao, KM
ED  - Lei, H
ED  - Li, Y
ED  - Chung, JY
ED  - Shah, N
TI  - A Highly Concurrent Process Virtual Machine Based on Event-driven Process Execution Model
T2  - 2012 NINTH IEEE INTERNATIONAL CONFERENCE ON E-BUSINESS ENGINEERING (ICEBE)
CP  - 9th IEEE International Conference on e-Business Engineering (ICEBE)/8th SOAIC/6th EM2I/6th SOKMBI/4th ASOC
AB  - Existing orchestration and choreography process engines only serve for dedicate process languages and majority of them show rough performance under even moderate workloads which cause them incapable for practical use. To solve the problems, the Event-driven Process Execution Model (EPEM) is presented as the common process model and the formalization of EPEM is presented to guarantee the correctness and efficiency for process transformation. A Process Virtual Machine - OncePVM is implemented based on EPEM and extended to support execution of WS-BPEL. For evaluation, the performance of OncePVM is compared with existing orchestration engines including Active BPEL, Apache ODE and Oracle BPEL Process Manager. The experimental results indicate that OncePVM shows better scalability than other tested platforms especially under high workloads.
SN  - 978-1-4673-2601-8
SN  - 978-0-7695-4809-8
PY  - 2012
SP  - 61
EP  - 69
DO  - 10.1109/ICEBE.2012.20
AN  - WOS:000317012600010
ER  -

TY  - JOUR
AU  - Zhou, Z
AU  - Zhi, Q
AU  - Morisaki, S
AU  - Yamamoto, S
TI  - An Evaluation of Quantitative Non-Functional Requirements Assurance Using ArchiMate
T2  - IEEE ACCESS
AB  - Goal-oriented NFR (Non-Functional Requirement) assurance approaches were used to qualitatively evaluate software architectures. Assurance cases using quantitative method have not been applied to evaluate NFR assurance for software architectures. This paper presents a system architecture evaluation method which is able to conduct quantitative NFR assurance evaluation for system architecture through ArchiMate. The paper also proposes an algorithm to automate the quantitative evaluation process. A questionnaire survey among software engineers and a case study on a vehicular safety monitor system were carried out to verify the necessity of the method. Additionally, we conducted an experimental design with 18 samples divided into 2 groups with the goal of comparing how the independent variables affect the dependent variables. The results of the experiment demonstrate that the proposed method achieves better NFR evaluation effect than the traditional approach. Moreover, compared with the traditional approach, the proposed method shortens the time for NFR evaluation. The proposed method is expected to be used at the early stage of software development projects for system NFR development, such as requirements analysis, system architecture design and system modeling. At present, the method has been applied by software engineers in a practical software project.
SN  - 2169-3536
PY  - 2020
VL  - 8
SP  - 72395
EP  - 72410
DO  - 10.1109/ACCESS.2020.2987964
AN  - WOS:000530827300013
ER  -

TY  - JOUR
AU  - Chin, KS
AU  - Mok, CK
AU  - Zu, X
TI  - Modeling and performance simulation of mould-design process
T2  - INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY
AB  - Mould design usually lies on the critical path of new product development. The mould-design process generally involves complex and multi-related design problems. The use of the IDEF0 descriptive model or similar tools alone in the analysis of complicated processes like the mould-design process usually encounter difficulties due to the complexity of the process. A quantitative analysis model thus seems necessary to supplement the IDEF0 tool for better modeling and simulation of complicated processes. In this paper, we propose to use the colored Petri nets tool for systematic and through analysis of the complicated mould-design process. We introduce an effective modeling and simulation language-hierarchical timed colored Petri-net-based workflow nets (HTCPNWF-nets). The rules for translating IDEF0 into HTCPNWF-nets have been developed. After mapping IDEF0 model onto HTCPNWF-nets, several performance analyses can be achieved by simulation of HTCPNWF-nets model. The proposed methodologies have been successfully applied to the mold design process.
SN  - 0268-3768
DA  - SEP
PY  - 2007
VL  - 34
IS  - 3-4
SP  - 236
EP  - 251
DO  - 10.1007/s00170-006-0584-5
AN  - CCC:000248833200004
ER  -

TY  - JOUR
AU  - Tang, D
AU  - Hecht, M
AU  - Miller, J
AU  - Handal, J
TI  - MEADEP: A dependability evaluation tool for engineers
T2  - IEEE TRANSACTIONS ON RELIABILITY
AB  - MEADEP is a user-friendly dependability-evaluation tool for measurement-based analysis of critical systems. MEADEP consists of 4 software modules:
   a data preprocessor for converting data in various formats to the MEADEP format,
   a data analyzer for graphical data-presentation and parameter estimation,
   a graphical modeling interface for building block diagrams (including the exponential block, Weibull block, and k-out-of-n block) and Markov reward chains,
   a model-solution module for availability/reliability calculations with graphical parametric analysis.
   Use of the tool on failure data. from measurements provides quantitative evaluations of dependability for the target system, while greatly reducing requirements for specialized skills in data-analysis and system-modeling from the user. MEADEP has been applied to evaluate availability for two air-traffic-control systems based on operational failure data; the results have provided valuable feedback to the project management of these critical systems. MEADEP has been used to analyze a. nuclear power-plant safety model, based on the Eagle 21 architecture and its early field-failure data. The study identified the most sensitive parameter and its most sensitive value segment to the plant mean time between hazards.
SN  - 0018-9529
SN  - 1558-1721
DA  - DEC
PY  - 1998
VL  - 47
IS  - 4
SP  - 443
EP  - 450
DO  - 10.1109/24.756088
AN  - WOS:000079621800008
ER  -

TY  - JOUR
AU  - Guo, L
AU  - Zhang, C
TI  - Dual-process modeling and control method for new product collaborative design based on petri net
T2  - JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING
AB  - With the increase of the complexity of new product design (NPD) and in order to adapt the competitive global market, enterprises are not only facing ensure product quality, they also facing how to shorten the design cycle as much as possible. In this context, how to improve NPD efficiency and quality is one of common aims in rapidly changing global market. This paper presents a dual-process modeling and control method for collaborative design of new products based on petri net for this reason. Firstly, the design process is divided into several design stages from the coarse-grained perspective, and the object petri net is established to make the whole design process in a collaborative environment. Secondly, the design relations among the design majors are analyzed from the perspective of fine granularity. This process transforms the complex relationships, establishes the specialty collaborative design meta model and simulates it. The work progress is quantitative analysis and feedback information is controlled based on object petri net. A design method based on factory mode is proposed to control process of design specialty collaboration, while a fuzzy comprehensive evaluation and evaluation compensation is proposed to confirm feedback information for adapting to the dynamic design process. Finally, a case is used to illustrate the method, which shows the effectiveness of the method.
SN  - 1868-5137
SN  - 1868-5145
DA  - MAR
PY  - 2019
VL  - 10
IS  - 3
SP  - 907
EP  - 921
DO  - 10.1007/s12652-018-0904-2
AN  - WOS:000459538400008
ER  -

TY  - JOUR
AU  - Weiss, A
AU  - Andrikopoulos, V
AU  - Hahn, M
AU  - Karastoyanova, D
TI  - Model-as-You-Go for Choreographies: Rewinding and Repeating Scientific Choreographies
T2  - IEEE TRANSACTIONS ON SERVICES COMPUTING
AB  - Scientists are increasingly using the workflow technology as a means for modeling and execution of scientific experiments. Despite being a very powerful paradigm workflows still lack support for trial-and-error modeling, as well as flexibility mechanisms that enable the ad hoc repetition of experiment logic to enable, for example, the convergence of results or to handle errors. In this respect, in our work on enabling multi-scale/field (multi-*) experiments using choreographies of scientific workflows, we contribute a method comprising all necessary steps to conduct the repetition of choreography logic across all workflow instances participating in a multi-* experiment. To realize the method, we contribute i) a formal model representing choreography models and instances, including the re-execute and iterate operations for choreographies, and based on it ii) algorithms for determining the rewinding points, i.e., the activity instances where the rewinding has to stop and iii) enable the actual rewinding to a previous execution state and repetition of the choreography. We present the implementation of our approach in a message-based, service-oriented system that allows scientists to model, control, and execute scientific choreographies as well as perform the rewinding and repeating of choreography logic. We also provide an evaluation of the performance of our approach.
SN  - 1939-1374
DA  - SEPT 1
PY  - 2020
VL  - 13
IS  - 5
SP  - 901
EP  - 914
DO  - 10.1109/TSC.2017.2732988
AN  - WOS:000577919100010
ER  -

TY  - CPAPER
AU  - Dalvi, AS
AU  - El-Mounayri, H
A1  - ASME
TI  - INTEGRATED SYSTEM ARCHITECTURE DEVELOPMENT FRAMEWORK AND COMPLEXITY ASSESSMENT
T2  - PROCEEDINGS OF ASME 2021 INTERNATIONAL MECHANICAL ENGINEERING CONGRESS AND EXPOSITION (IMECE2021), VOL 9
CP  - ASME International Mechanical Engineering Congress and Exposition (IMECE)
AB  - Systems engineering is the popular top-down systematic approach to understand and develop complex systems. There is a gap between the systems engineering activities and engineering analysis in major system development processes. This paper presents an integrated MBSE development framework with definite and indefinite modeling capabilities to bridge this gap. The framework uses SysML, a system modeling language, to describe its elements from the system architecture's perspective. A detailed workflow is presented that guides the engineer throughout the modeling process. The workflow establishes traceability throughout the framework. This research uses Functional Mock-up Interface (FMI) standards to integrate system engineering activities and engineering analysis. A district cooling system case study is presented to demonstrate the framework's capabilities in enabling the system into existence. The system architecture model was developed using SysML language in the Cameo Enterprise Architecture environment. The engineering analysis model used object-oriented Modelica language in the Dymola environment. The analysis results show that the district chiller model developed using Modelica produces chilled water below 6.6 degrees Celsius, satisfying the district chiller's system requirement. The exponential trend in the system architecture's complexity pattern is measured and analyzed using complexity assessment techniques. The results show that the structural complexity of the system increases steadily from 2.7080 to 8.1241. However, the behavioral complexity increases drastically from 1.7915 to 59.2686 in the problem domain.
SN  - 978-0-7918-8565-9
PY  - 2021
C7  - V009T09A044
AN  - WOS:000883348000044
ER  -

TY  - CPAPER
AU  - Mehdouani, K
AU  - Missaoui, N
AU  - Ghannouchi, SA
ED  - CruzCunha, MM
ED  - Martinho, R
ED  - Rijo, R
ED  - Peres, E
ED  - Domingos, D
TI  - An approach for Business Process Improvement Based on Simulation Technique
T2  - CENTERIS2019--INTERNATIONAL CONFERENCE ON ENTERPRISE INFORMATION SYSTEMS/PROJMAN2019--INTERNATIONAL CONFERENCE ON PROJECT MANAGEMENT/HCIST2019--INTERNATIONAL CONFERENCE ON HEALTH AND SOCIAL CARE INFORMATION SYSTEMS AND TECHNOLOGIES
CP  - International Conference on ENTERprise Information Systems (CENTERIS) / International Conference on Project MANagement (ProjMAN) / International Conference on Health and Social Care Information Systems and Technologies (HCist)
AB  - Business Process Management (BPM) is an approach used to manage business processes (BP) by facilitating the modeling, the execution as well as the improvement of a BP. It is considered as the most appropriate approach to continuously improving a process. In literature, the improvement phase is essentially based on the simulation technique where the majority of the defined works focus on improving BP models by proposing an approach for business process modeling and simulation. However, there is a lack of research studies that provide an approach for BP improvement based on simulation at both design-time and run-time. We propose in this paper an enhanced improvement approach based on simulation. The main contribution aims at integrating simulation model into BPM model, in order to promote the continuous improvement of a BP. The proposed model has been tested on a healthcare process model where we used simulation to define a set of improvement on the studied process model. (C) 2019 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of the CENTERIS -International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies.
SN  - 1877-0509
PY  - 2019
VL  - 164
SP  - 225
EP  - 232
DO  - 10.1016/j.procs.2019.12.176
AN  - WOS:000570337000030
ER  -

TY  - CPAPER
AU  - Penteado, AP
AU  - Maciel, RF
AU  - Erbs, J
AU  - Ortolani, CLF
AU  - Roza, BA
AU  - Pisa, IT
ED  - Sarkar, IN
ED  - Georgiou, A
ED  - Marques, PMD
TI  - Non-Integrated Information and Communication Technologies in the Kidney Transplantation Process in Brazil
T2  - MEDINFO 2015: EHEALTH-ENABLED HEALTH
CP  - 15th World Congress on Health and Biomedical Informatics (MEDINFO)
AB  - The entire kidney transplantation process in Brazil is defined through laws, decrees, ordinances, and resolutions, but there is no defined theoretical map describing this process. From this representation it's possible to perform analysis, such as the identification of bottlenecks and information and communication technologies (ICTs) that support this process. The aim of this study was to analyze and represent the kidney transplantation workflow using business process modeling notation (BPMN) and then to identify the ICTs involved in the process. This study was conducted in eight steps, including document analysis and professional evaluation. The results include the BPMN model of the kidney transplantation process in Brazil and the identification of ICTs. We discovered that there are great delays in the process due to there being many different ICTs involved, which can cause information to be poorly integrated.
SN  - 0926-9630
SN  - 1879-8365
SN  - 978-1-61499-564-7
SN  - 978-1-61499-563-0
PY  - 2015
VL  - 216
SP  - 1058
EP  - 1058
DO  - 10.3233/978-1-61499-564-7-1058
AN  - WOS:000455836700359
ER  -

TY  - JOUR
AU  - Xu, HQ
AU  - Gan, WZ
AU  - Cao, L
AU  - Yang, C
AU  - Wu, JH
AU  - Zhou, M
AU  - Qu, HZ
AU  - Zhang, SL
AU  - Yin, HX
AU  - Wu, ZH
TI  - A Machine Learning Approach for Optimization of Channel Geometry and Source/Drain Doping Profile of Stacked Nanosheet Transistors
T2  - IEEE TRANSACTIONS ON ELECTRON DEVICES
AB  - Complex nonlinear dependence of ultra-scaled transistor performance on its channel geometry and source/drain (S/D) doping profile bring obstacles in the advanced technology path-finding and optimization. A machine learning-based multi-objective optimization (MOO) workflow is proposed to optimize the sub-3-nm node gate-all-around (GAA) three-layer-stacked nanosheet transistors (NSFETs) accounting for the key performance knob of channel geometry and S/D doping profile. The artificial neural network (ANN) is trained to learn the compact current-voltage (I-V) relationship of NSFETs from 3-D technology computer-aided design (TCAD) simulation results. Based on the artificial neural network (ANN) model, MOO between threshold swing, on-off ratio, and on-state current of NSFETs is performed with adaptive weighted sum theory. The proposed workflow efficiently suggests an optimized design window of channel geometry and doping profile of NSFETs. The proposed devices satisfy the 2025 International Roadmap for Devices and Systems (IRDSs) target in terms of electrical characteristics for digital circuits.
SN  - 0018-9383
SN  - 1557-9646
DA  - JUL
PY  - 2022
VL  - 69
IS  - 7
SP  - 3568
EP  - 3574
DO  - 10.1109/TED.2022.3175708
C6  - MAY 2022
AN  - WOS:000800781900001
ER  -

TY  - CPAPER
AU  - Andree, K
AU  - Ihde, S
AU  - Pufahl, L
ED  - Nurcan, S
ED  - Reinhartz-Berger, I
ED  - Soffer, P
ED  - Zdravkovic, J
TI  - Exception Handling in the Context of Fragment-Based Case Management
T2  - ENTERPRISE, BUSINESS-PROCESS AND INFORMATION SYSTEMS MODELING, BPMDS 2020, EMMSAD 2020
CP  - 22nd International Working Conference on Business Process Modeling, Development and Support (BPMDS) and the 26th International Working Conference on Exploring Modeling Methods for Systems Analysis and Development (EMMSAD)
CP  - 21st International Working Conference on Business Process Modeling, Development and Support (BPMDS) and the 25th International Working Conference on Exploring Modeling Methods for Systems Analysis and Development (EMMSAD)
AB  - Case Management supports knowledge workers in defining, executing, and monitoring the handling of their cases, e.g. in healthcare or logistics. Fragment-based case management (fCM) allows to define a case model with the help of several process fragments, which can be flexible combined at run-time based on case characteristics and the case worker's intuition. Cases are often influenced by unknown exception, e.g., the sudden change of patient condition's or a storm delaying transports. So far, fCM only reacts to known circumstances. In this paper, we want to extend fCM by an exception handling approach. Thereby, existing exception patterns for workflow systems are used and extended by the fragment-level for handling unknown events. In order to enable direct integration and avoid a duplication of semantics, precise rules are specified in order to clarify how to extend which pattern in detail. The applicability of the developed exception handling technique is exemplified on a last mile delivery for parcels.
SN  - 1865-1348
SN  - 1865-1356
SN  - 978-3-030-49417-9
SN  - 978-3-030-49418-6
PY  - 2020
VL  - 387
SP  - 20
EP  - 35
DO  - 10.1007/978-3-030-49418-6_2
AN  - WOS:001289645300002
ER  -

TY  - CPAPER
AU  - Polgár, B
AU  - Agoston, I
AU  - Juhász, G
AU  - Majzik, I
ED  - Penjam, J
TI  - The INTEGRA Tool Integration Framework
T2  - 12TH SYMPOSIUM ON PROGRAMMING LANGUAGES AND SOFTWARE TOOLS, SPLST' 11
CP  - 12th Symposium on Programming Languages and Software Tools (SPLST)
AB  - System development processes especially in case of model-based development are typically supported by dozens of different tools that assist the designer in the modeling, verification, source code generation, and testing tasks. Tool-chains can be formed by the integration of tools that are related to the subsequent steps of the development process. INTEGRA is our service-oriented, metamodel-driven, process-centric tool integration framework that supports the definition and execution of these tool-chains. In this paper, we present how process modeling and process management is targeted in this framework. The underlying approach is based on platform independent, UML activity compliant process models, that are transformed automatically to platfom specific process models executable by widely used workflow engines. These engines are also integrated with the execution and monitoring part of the framework. In addition, traceability of the design data and process execution is also supported as it is extremely important for the certification of critical systems. The implementation is provided as an open, extensible framework. The approach is demonstrated using a model based test case generation process.
SN  - 978-9949-23-178-2
PY  - 2011
SP  - 66
EP  - 77
AN  - WOS:000328999800007
ER  -

TY  - CPAPER
AU  - Reuter, C
ED  - VanBommel, P
ED  - Hoppenbrouwers, S
ED  - Overbeek, S
ED  - Proper, E
ED  - Barjis, J
TI  - Composition of Semantic Process Fragments to Domain-Related Process Families
T2  - PRACTICE OF ENTERPRISE MODELING
CP  - 3rd IFIP WG 8.1 Working Conference on the Practice of Enterprise Modeling
AB  - Efficient and effective process management is considered as key success factor for competitiveness of enterprises in an increasingly complex and closely connected environment. Today, there exists a plentitude of IT-tools that support modeling, execution, monitoring, and even flexible change of processes. Though, most process management solutions offer possibilities for reusing workflow components, development of new process models is still a cost and time consuming task. Either common process knowledge is scattered among a growing amount of process models or it is divided into unspecific components, the interrelations of which are difficult to manage. This problem becomes even worse considering potential variations of workflows. In the SPOT project, we adapted the feature modeling approach in order to represent enterprise-specific process knowledge in the form of process families. Process families consist of semantically enriched process fragments and enable the composition of business processes that conform to domain-related rules and regulations.
SN  - 1865-1348
SN  - 978-3-642-16781-2
PY  - 2010
VL  - 68
SP  - 61
EP  - 75
AN  - WOS:000286483700005
ER  -

TY  - CPAPER
AU  - Tan, WN
AU  - Tang, AQ
AU  - Wang, T
AU  - Tan, W
AU  - Li, L
AU  - Zhang, ZJ
A1  - IEEE
TI  - An Overall Lifecycle Modeling Method for E-Learning Services and the Cooperative Mechanism Based on PCDA
T2  - 2013 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS (SMC 2013)
CP  - IEEE International Conference on Systems, Man, and Cybernetics (SMC)
AB  - This paper proposes a model, namely eL-PCDA, for overall lifecycle process management on e-Learning service by considering learner's behaviours during e-learning services, scheduling policies and monitoring mechanism of learning activities. Business process modeling for e-Learning services can be taken according to the study ordering of the knowledge points by using workflow modeling technology and process enactment mechanism. Overall lifecycle process management of Knowledge is addressed by combining knowledge product modeling, knowledge resource modeling, and credit polices for member selection in research team by considering trust value of learners, advisers and providers in e-Learning services. The proposed method can be used for supporting the sustainable development of e-Learning services from planning and design, organizing e-Learning process, maintenance e-Learning process, to process improvement, as well as to support learners and advisers to effectively complete innovative team study and complex computation
SN  - 1062-922X
SN  - 978-1-4799-0652-9
PY  - 2013
SP  - 669
EP  - 674
DO  - 10.1109/SMC.2013.119
AN  - WOS:000332201900112
ER  -

TY  - JOUR
AU  - Pellizzoni, L
AU  - Silva, SDE
AU  - Falavigna, A
TI  - Multilanguage health record database focused on the active follow-up of patients and adaptable for patient-reported outcomes and clinical research design
T2  - INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS
AB  - Introduction: A dataset with patient information allows a comparison between different clinical treatments in many fields of medicine as well as the effective use of medical resources. Patient-reported outcome measures (PROMs) collect data directly from patients for use in clinical practice by helping decision making and tailoring treatments according to the patients' needs. The authors present a novel database to overcome data collection related barriers, calculating automatically the questionnaire results, displayed in graphics on the patients' dashboard in real time, and focused on the active follow of the patients.
   Objective: To present electronic health record database for monitoring clinical or surgical interventions and assess the usability.
   Methodology: Process modeling and specification of system requirements were performed using the Iconix methodology along with the Post-Study System Usability Questionnaire (PSSUQ) to validate the usability and usefulness of the proposed system. The system and the questionnaires were performed in three languages: Brazilian Portuguese, Spanish, and English.
   Results: The database enables researchers to use the questionnaires defining the time of data collection according to the needs of each clinical study. The system facilitates the patient answers without any personal interference from smartphones, tablets or computers. The questionnaire scores were calculated automatically in real time and displayed in graphics on the patients' dashboard. The evaluation of the usability and usefulness of the developed database used 8 people divided into two equal groups (4 physicians and 4 medical students).
   Conclusion: The proposed electronic health record database allows a friendly and flexible use of PROMs according to the population, needs in practice and clinical settings. The platform promotes active and direct data collection from patients and physicians in English, Portuguese and Spanish.
SN  - 1386-5056
SN  - 1872-8243
DA  - MAR
PY  - 2020
VL  - 135
C7  - 104065
DO  - 10.1016/j.ijmedinf.2019.104065
AN  - WOS:000507469400007
ER  -

TY  - JOUR
AU  - Stylianakis, G
AU  - Moumoutzis, N
AU  - Arapi, P
AU  - Mylonakis, M
AU  - Christodoulakis, S
TI  - COLearn: Supporting Collaborative Learning on top of Existing Learning Infrastructures
T2  - BULLETIN OF THE TECHNICAL COMMITTEE ON LEARNING TECHNOLOGY
AB  - COLearn is a web-based collaborative learning environment that supports the specification of collaborative learning workflows, their deployment and enactment. It addresses major challenges in modern learning infrastructures to enable contextualization, social constructivism and knowledge-pull. It offers a shell, easily integrated on top of existing open learning infrastructures (such as LMSs and OER repositories) to enrich their capabilities by offering functionality to design rich collaborative learning activities. COLearn run-time environment is used to enact these activities and workflows. This way, COLearn leverages the power of the underlying infrastructures, provides structure to the groups of learners that participate in learning workflows, dynamically adapts the workflows during their enactment, monitors their evolution to facilitate assessment and provides feedback to the learners. COLearn employs an intuitive graphical representation exploiting the Business Process Modeling Notation standard. As an internal representation and interoperability model it uses IMS Learning Design thus offering effective sharing and remixing of learning designs.
SN  - 2306-0212
DA  - APR
PY  - 2015
VL  - 17
IS  - 1-2
SP  - 2
EP  - 5
AN  - WOS:000359854300002
ER  -

TY  - JOUR
AU  - Karniel, A
AU  - Reich, Y
TI  - From DSM-Based Planning to Design Process Simulation: A Review of Process Scheme Logic Verification Issues
T2  - IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT
AB  - Planning product development processes (PDP), and particularly new product development (NPD) processes, is complex and challenging. The plan should reflect the product-related knowledge, including the influences of performing changes in one product component on the need to rework the design of other components. Given the complexity, dynamics, and uncertainties of design processes (DPs), the plan evaluation requires simulation tools. The design structure matrix (DSM) is a known method for DP planning. However, the DSM itself does not express all the relevant information required for defining process logic. Many logic interpretations are applicable in different business cases; yet, a consistent method of transforming a DSM-based plan to a logically correct concurrent process model in the case of iterative activities is lacking. A gap was identified between the literature concerning activities sequencing based on DSM and the process modeling literature concerning process verification. This survey systematically classifies the approaches used in DSM-based process planning, and discusses their strengths and limitations with problems related to process modeling logic verification of iterative processes. Demonstration of the logic differences emphasizes the need for simulation-based decision making according to the specific process attributes.
SN  - 0018-9391
SN  - 1558-0040
DA  - NOV
PY  - 2009
VL  - 56
IS  - 4
SP  - 636
EP  - 649
DO  - 10.1109/TEM.2009.2032032
AN  - WOS:000271019600005
ER  -

TY  - CPAPER
AU  - Ley, D
A1  - IEEE
TI  - A Framework to classify Processes in the field of Human-Machine Systems Engineering
T2  - 2013 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS (SMC 2013)
CP  - IEEE International Conference on Systems, Man, and Cybernetics (SMC)
AB  - The analysis of workflows is a core element for the design and evaluation of complex human-machine systems. Based on the attributes of corresponding processes, appropriate methods for capturing data and a proper process modeling language must be chosen. But what are relevant attributes for these decisions? What are typical attributes of processes for discussions independent from application domains? A proper method to classify processes is missing in literature. Therefore, based on a two stage qualitative literature analysis a framework to classify processes was developed. Firstly, various process designations were identified. Secondly, definitions and descriptions of these designations were used in an iterative approach to extract attributes relevant for all kinds of processes. The qualitative approach, the consequent results in form of a "Process Disc" with eight process attributes and related values as well as the application are described in this paper.
SN  - 1062-922X
SN  - 978-1-4799-0652-9
PY  - 2013
SP  - 1582
EP  - 1587
DO  - 10.1109/SMC.2013.273
AN  - WOS:000332201901119
ER  -

TY  - JOUR
AU  - Accorsi, R
AU  - Lehmann, A
AU  - Lohmann, N
TI  - Information leak detection in business process models: Theory, application, and tool support
T2  - INFORMATION SYSTEMS
CP  - 10th International Conference on Business Process Management (BPM)
AB  - Despite the correct deployment of access control mechanisms, information leaks can persist and threaten the reliability of business process execution. This paper presents an automated and effective approach for the verification of information flow control for business process models. Building on the concept of place-based non-interference and declassification, the core contribution of this paper is the application of Petri net reachability to detect places in which information leaks occur. Such a feature allows for the use of state-of-the-art tool support to model-check business process models and detect leaks. We show that the approach is sound and complete, and present the Anica tool to identify leaks. An extensive evaluation comprising over 550 industrial process models is carried out and shows that information flow analysis of process models can be done in milliseconds. This motivates a tight integration of business process modeling and non-interference checking. (C) 2014 Elsevier Ltd. All rights reserved.
SN  - 0306-4379
SN  - 1873-6076
DA  - JAN
PY  - 2015
VL  - 47
SP  - 244
EP  - 257
DO  - 10.1016/j.is.2013.12.006
AN  - WOS:000344201100013
ER  -

TY  - CPAPER
AU  - Nikiforova, O
AU  - Kirikova, M
AU  - Pavlova, N
ED  - Vasilecas, O
ED  - Eder, J
ED  - Caplinskas, A
TI  - Principles of Model Driven Architecture in Knowledge Modeling for the Task of Study Program Evaluation
T2  - DATABASES AND INFORMATION SYSTEMS IV
CP  - 7th International Baltic Conference on Databases and Information Systems
AB  - Two-hemisphere model driven (2HMD) approach assumes modeling and use of procedural and conceptual knowledge on equal and related basis according to the principles of Model Driven Architecture (MDA), which separates different aspects of system modeling. This differentiates 2HMD approach from pure procedural, pure conceptual, and object oriented approaches. The approach may be applied in the context of modeling of a particular business domain as well as in the context of modeling the knowledge about the domain. Therefore, the principles of MDA via 2HMD approach may be applied not only in the context of software development but also in the context of the study course and program development. Knowledge modeling by 2HMD approach gives an opportunity to transparently analyze and compare knowledge to be provided and knowledge actually provided by courses belonging to a particular study program, and, thus, to identify and fill gaps between desirable and actual knowledge content of the study program.
SN  - 0922-6389
SN  - 1879-8314
SN  - 978-1-58603-715-4
PY  - 2007
VL  - 155
SP  - 291
EP  - 304
AN  - WOS:000273619400020
ER  -

TY  - JOUR
AU  - Popa, C
AU  - Stefanov, O
AU  - Goia, I
TI  - Multimodal Livestock Operations Analysis Using Business Process Modeling: A Case Study of Romanian Black Sea Ports
T2  - ECONOMIES
AB  - In spite of its strong increase and relevant position in the evolution of international maritime routes, the global livestock trade is still a poorly treated topic in the maritime business domain of research. Aiming to cover this gap, the authors are focused on revealing the livestock logistics technology in intermodal transports, approaching both equipment reliability and operation flow design, applying the business processes modeling method to map the most relevant stages in animals' port operation, transfer, and maritime transportation. This paper examines the intricate logistics of maritime livestock transportation through a case study on the Port of Midia, administrated by the Constanta Maritime Port Administration, one of Romania's primary export hubs for livestock operations, using BPM software, seeking to identify the most important deficiencies and alternatives in improving the technical and technological effectiveness. Key findings indicate that improving ramp availability, automating document verification, and implementing RFID-based animal tracking systems could significantly enhance operational efficiency. By integrating workflow models, real-time monitoring, and simulation-based optimization, the study offers a comprehensive framework for streamlining multimodal livestock transportation. The implications extend to policymakers, port authorities, and logistics operators, emphasizing the necessity of digital transformation, regulatory harmonization, and technological integration in livestock maritime transportation. This research contributes to the expansion of intermodal transportation studies, providing practical recommendations for enhancing livestock logistics efficiency while ensuring compliance with European animal welfare regulations. The findings pave the way for further research into AI-driven risk assessments, smart logistics solutions, and sustainable livestock transportation alternatives.
SN  - 2227-7099
DA  - MAR 7
PY  - 2025
VL  - 13
IS  - 3
C7  - 69
DO  - 10.3390/economies13030069
AN  - WOS:001453781600001
ER  -

TY  - JOUR
AU  - Ruiz-Rube, I
AU  - Dodero, JM
AU  - Palomo-Duarte, M
AU  - Ruiz, M
AU  - Gawn, D
TI  - Uses and applications of Software & Systems Process Engineering Meta-Model process models. A systematic mapping study
T2  - JOURNAL OF SOFTWARE-EVOLUTION AND PROCESS
AB  - Software process engineering is a discipline, which aims to study and improve software development and maintenance processes. The explicit definition of software processes is essential. To this end, the Object Management Group consortium proposed the Software & Systems Process Engineering Meta-Model (SPEM) that exploits the benefits of the Model Driven Architecture paradigm applied to software process models, instead of software specification models. The aim of this study is to discover evidence clusters and evidence deserts in the use and application of SPEM from a business process management point of view. To reach the proposed objective, we have undertaken a systematic mapping study of the existing scientific literature.
   The reviewed literature deals mainly with process modeling and, to a lesser extent, with process adaptability, verification, and validation, enactment and evaluation. Wide agreement exists in using the SPEM meta-model to develop different types of methods and processes. Further research efforts are needed in areas related to enactment and evaluation of software processes. There is a need to evolve to a new version of the meta-model that incorporates the improvements proposed by different authors. Copyright (c) 2013 John Wiley & Sons, Ltd.
SN  - 2047-7473
SN  - 2047-7481
DA  - SEP
PY  - 2013
VL  - 25
IS  - 9
SP  - 999
EP  - 1025
DO  - 10.1002/smr.1594
AN  - WOS:000324092000006
ER  -

TY  - CPAPER
AU  - Deva, A
AU  - Rypinski, TA
AU  - Soni, B
AU  - Pillai, P
AU  - Rhines, LD
AU  - Tatsui, CE
AU  - Alvarez-Breckenridge, C
AU  - North, RY
AU  - Bird, JE
AU  - Siewerdsen-, JH
ED  - Siewerdsen, JH
ED  - Rettmann, ME
TI  - SURGICAL PROCESS MODELING OF IMAGE-GUIDED SPINE SURGERY
T2  - IMAGE-GUIDED PROCEDURES, ROBOTIC INTERVENTIONS, AND MODELING, MEDICAL IMAGING 2024
CP  - Conference on Medical Imaging - Image-Guided Procedures, Robotic Interventions, and Modeling
AB  - Purpose. Spine surgery involves complex workflows and disparate levels of system integration that challenge the introduction of emerging technologies. This work develops a computational simulation framework based on statistical surgical process models (SPM) to quantitatively evaluate variations in the workflow and implementation of image guidance systems in terms of key outcome measures in spine surgery.
   Method. A statistical SPM was developed for spine surgery to describe the effects of various intraoperative technologies (viz., fluoroscopy, CT, image-to-world registration, and planning methods) and a range of procedural variables (e.g., surgeon skill, patient body mass index (BMI), target vertebrae, and fusion length) on key outcome measures, including cycle time, radiation dose and the quality of surgical product (geometric accuracy in pedicle screw placement). The model was parameterized by statistical distributions informed by clinical observation, expert feedback, literature review, and clinical data.
   Results. The results quantify the advantages of intraoperative CT and/or long-length scout radiography for reduced cycle time in vertebral localization - (4.8-7.2) min, compared to (5.8-12.4) min by fluoroscopy. The models further demonstrate the cycle time for imaging, registration, and planning in surgical guidance: the mean procedure cycle time for 11-level fusion was 540 min by fluoroscopy compared to 441 min for CT + navigation. Analysis of radiation dose quantified the effective dose to the patient (and operating room) between fluoroscopy and CT. The geometric accuracy of pedicle screw placement showed median error of 2.7 mm for fluoroscopy compared to 1.8 mm for CT+navigation and a corresponding reduction in frequency of pedicle breach for the latter.
   Conclusions. A statistical SPM provides a powerful framework for procedure simulation, evaluation of emerging technologies, and optimization of procedural workflow. Such modeling provides a quantitative basis to evidence the value of emerging technologies and identify optimal means of integration / implementation in clinical workflow.
SN  - 1605-7422
SN  - 978-1-5106-7161-4
SN  - 978-1-5106-7160-7
PY  - 2024
VL  - 12928
C7  - 1292813
DO  - 10.1117/12.3008825
AN  - WOS:001215454100034
ER  -

TY  - CPAPER
AU  - Posadas, H
AU  - Peñil, P
AU  - Nicolás, A
AU  - Villar, E
ED  - Morawiec, A
ED  - Hinderscheit, J
TI  - System Synthesis from UML/MARTE Models
T2  - PROCEEDINGS OF THE 2013 ELECTRONIC SYSTEM LEVEL SYNTHESIS CONFERENCE (ESLSYN)
CP  - 3rd Electronic System Level Synthesis Conference (ESLsyn)
AB  - Model-Driven Engineering (MDE) based on UML is a mature methodology for software development. However, its application to HW/SW embedded system specification and design requires specific features not covered by the language. For this reason, the MARTE profile for Real-Time and Embedded systems was defined. It has proven to be powerful enough to support holistic system modeling under different views. This single-source model is able to capture the required information, enabling the automatic generation of executable and configurable models for fast performance analysis without requiring additional engineering effort. As a result of this performance analysis suitable system architecture can be decided. At this point, the SW stack to be executed by each processing node in the selected heterogeneous platform has to be generated. In the general case this is a tedious and error-prone process with little assistance from available tools. Current practices oblige the SW engineer to develop the code for each node of the heterogeneous multi-core platform by hand. The code has to be written specifically for the selected architecture and architectural mapping, thus reducing reusability. In order to overcome this limitation, the FP7 PHARAON project aims to develop tools able to automatically generate the code to be executed in each node from the initial system model. This affects not only the application code, the static and run-time libraries (e. g. OpenMP/OpenCL), the middleware and communication functions, but also the OS and the driver calls in each node.
SN  - 978-1-4673-6414-0
SN  - 978-2-9539987-8-8
PY  - 2013
AN  - WOS:000326979300011
ER  -

TY  - CPAPER
AU  - Wu, BL
AU  - Li, L
AU  - Yang, Y
ED  - Chen, J
TI  - E-business value process modelling
T2  - SHAPING BUSINESS STRATEGY IN A NETWORKED WORLD, VOLS 1 AND 2, PROCEEDINGS
CP  - 4th International Conference on Electronic Business (ICEB 2004)
AB  - E-business development is a considerable complicated task because the underlying logics of e-business and its new processes that are originated from implementing enhanced information technologies to streamline business performance introduce many complex issues. One of the difficulties is to capture the dynamic aspects of e-business that can be used for monitoring the business performance, in a way that could be helpful for the business adaptation to meet competitive advantages. Among many dynamics of e-business, the value system is the most interested one that has recently been addressed. However a value system is at the strategic level with no formal approaches for its representation, which introduces a gap between system modelling and implementation in the e-business development. In this paper, we will investigate a so called value process that can be not only used for value system modeling, but executed for simulation of the resulted model. For the purpose of value process modelling, we will adopt the process algebra approach, which will be integrated with others such as workflow in our modelling environment.
SN  - 7-5062-7342-X
PY  - 2004
SP  - 408
EP  - 413
AN  - WOS:000226778000076
ER  -

TY  - JOUR
AU  - Scheuerlein, H
AU  - Rauchfuss, F
AU  - Dittmar, Y
AU  - Molle, R
AU  - Lehmann, T
AU  - Pienkos, N
AU  - Settmacher, U
TI  - New methods for clinical pathways-Business Process Modeling Notation (BPMN) and Tangible Business Process Modeling (t.BPM)
T2  - LANGENBECKS ARCHIVES OF SURGERY
AB  - Clinical pathways (CP) are nowadays used in numerous institutions, but their real impact is still a matter of debate. The optimal design of a clinical pathway remains unclear and is mainly determined by the expectations of the individual institution. The purpose of the here described pilot project was the development of two CP (colon and rectum carcinoma) according to Business Process Modeling Notation (BPMN) and Tangible Business Process Modeling (t.BPM).
   BPMN is an established standard for business process modelling in industry and economy. It is, in the broadest sense, a computer programme which enables the description and a relatively easy graphical imaging of complex processes. t.BPM is a modular construction system of the BPMN symbols which enables the creation of an outline or raw model, e.g. by placing the symbols on a spread-out paper sheet. The thus created outline can then be transferred to the computer and further modified as required. CP for the treatment of colon and rectal cancer have been developed with support of an external IT coach.
   The pathway was developed in an interdisciplinary and interprofessional manner (55 man-days over 15 working days). During this time, necessary interviews with medical, nursing and administrative staffs were conducted as well. Both pathways were developed parallel. Subsequent analysis was focussed on feasibility, expenditure, clarity and suitability for daily clinical practice. The familiarization with BPMN was relatively quick and intuitive. The use of t.BPM enabled the pragmatic, effective and results-directed creation of outlines for the CP. The development of both CP was finished from the diagnostic evaluation to the adjuvant/neoadjuvant therapy and rehabilitation phase. The integration of checklists, guidelines and important medical or other documents is easily accomplished. A direct integration into the hospital computer system is currently not possible for technical reasons.
   BPMN and t.BPM are sufficiently suitable for the planned modelling and imaging of CP. The application in medicine is new, and transfer from the industrial process management is in principle possible. BPMN-CP may be used for teaching and training, patient information and quality management. The graphical image is clearly structured and appealing. Even though the efficiency in the creation of BPMN-CP increases markedly after the training phase, high amounts of manpower and time are required. The most sensible and consequent application of a BPMN-CP would be the direct integration into the hospital computer system. The integration of a modelling language, such as BPMN, into the hospital computer systems could be a very sensible approach for the development of new hospital information systems in the future.
SN  - 1435-2443
SN  - 1435-2451
DA  - JUN
PY  - 2012
VL  - 397
IS  - 5
SP  - 755
EP  - 761
DO  - 10.1007/s00423-012-0914-z
AN  - BIOABS:BACD201200293969
ER  -

TY  - CPAPER
AU  - Krogstie, J
ED  - Sandkuhl, K
ED  - Seigerroth, U
ED  - Stirna, J
TI  - Quality of Business Process Models
T2  - PRACTICE OF ENTERPRISE MODELING, POEM 2012
CP  - 5th IFIP WG 8.1 Working Conference on Practice of Enterprise Modeling (PoEM)
AB  - Processes modeling is done for a number of reasons in relation to enterprise modeling, business process modeling and information systems development, and is a widely used technique. In particular after the introduction of BPR and workflow in the nineties, much work has looked on quality of business process models. In this paper we present a specialization of a general framework for assessing quality of models to support the evaluation the quality of business process models. The specialization takes earlier work on quality of models, process quality and quality of business process models into account. Comparing the approaches we find on the one hand that the properties of business process model quality is subsumed by the generic framework on a high level, and that there are aspects in this framework that are not covered by the existing work on business process model quality. On the other hand, the comparison has resulted in an extension of the generic framework for these kinds of models, and in this way improved the practical applicability of the framework when applied to discussing the quality of business process models.
SN  - 1865-1348
SN  - 978-3-642-34549-4
SN  - 978-3-642-34548-7
PY  - 2012
VL  - 134
SP  - 76
EP  - 90
AN  - WOS:000345316800006
ER  -

TY  - JOUR
AU  - König, S
AU  - Vogel-Heuser, B
AU  - Hradecky, A
AU  - Horn, S
AU  - Hahn, M
TI  - Executing automotive test workflows with BPMN and web service orchestration in software-defined car manufacturing
T2  - PRODUCTION ENGINEERING-RESEARCH AND DEVELOPMENT
AB  - Automotive original equipment manufacturers (OEMs) are facing the challenge of keeping their cars innovative for the customer by integrating new advanced software functions, e.g., to provide higher levels of automated driving. Nonetheless, internal business processes should stay efficient to support the transformation despite the increasing complexity caused by new information and car technologies. For engineering and production processes where interdisciplinary teams of engineers, software developers, and workers interact along the car life cycle, the concept of executable process models offers an opportunity for work streamlining and process automation by web service orchestration. This paper will shed light on using executable process models under the cross-industry Business Process Model and Notation (BPMN) standard, along with the example of automotive test and diagnostic processes in smart manufacturing. We introduce a lean approach that includes the enterprise IT architecture for process modeling and the product car for process execution. Finally, we present our evaluation of the approach through a focus group with OEM experts.
SN  - 0944-6524
SN  - 1863-7353
DA  - APR
PY  - 2025
VL  - 19
IS  - 2
SP  - 195
EP  - 209
DO  - 10.1007/s11740-024-01302-1
C6  - JUL 2024
AN  - WOS:001272735900001
ER  -

TY  - CPAPER
AU  - Huang, TH
AU  - Schneider, E
AU  - Pegoraro, M
AU  - van der Aalst, WMP
ED  - VanDerAa, H
ED  - Bork, D
ED  - Schmidt, R
ED  - Sturm, A
TI  - Fast & Sound: Accelerating Synthesis-Rules-Based Process Discovery
T2  - ENTERPRISE, BUSINESS-PROCESS AND INFORMATION SYSTEMS MODELING, BPMDS 2024, EMMSAD 2024
CP  - 25th International Working Conference on Business Process Modeling, Development, and Support (BPMDS) / 29th International Working Conference on Exploring Modeling Methods for Systems Analysis and Development (EMMSAD)
AB  - Process discovery aims to construct process models describing the observed behaviors of information systems. It is an essential step in process mining projects as most process mining techniques assume a process model as input. While various process discovery algorithms exist, few provide desirable properties: soundness and free-choiceness. By exploiting the free-choice net theory, the recently developed Synthesis Miner not only guarantees the two desirable properties but also enables a more flexible representation (non-block structures) of the discovered process models. The flexibility allows the Synthesis Miner to discover process models with potentially higher quality. Nevertheless, applying the Synthesis Miner remains a challenge due to its lack of scalability. In this paper, we identify the bottleneck and address it by introducing various extensions that utilize the log heuristics and extract the minimal sub-net of the process model. The evaluation using real-life event logs shows that the proposed extensions improve the scalability of the Synthesis Miner by reducing the computation time by 82.85% on average.
SN  - 1865-1348
SN  - 1865-1356
SN  - 978-3-031-61006-6
SN  - 978-3-031-61007-3
PY  - 2024
VL  - 511
SP  - 259
EP  - 274
DO  - 10.1007/978-3-031-61007-3_20
AN  - WOS:001288840600020
ER  -

TY  - JOUR
AU  - Poloczek, R
AU  - Oleksiak, B
TI  - Evaluation of production line expansion efficiency using computer simulation
T2  - PRODUCTION ENGINEERING ARCHIVES
AB  - The article discusses the application of computer simulation in the optimization of production processes, particularly in the context of analyzing scenarios related to the addition of new production lines. The conducted research and simulations have shown that computer simulation is a key tool for precise modeling and analysis of various options, allowing for better understanding and optimization of production activities. The article presents the theoretical foundations of simulation along with practical examples of its application, focusing on assessing the impact of different production line configurations on the overall system's efficiency. The analysis of benefits includes shortening the production cycle time, increasing flexibility, and improving operational efficiency. The challenges associated with implementing computer simulation, such as the need for specialized knowledge and the necessity for continuous updates of simulation models, are also discussed. Based on the research and analyses conducted, the article demonstrates that computer simulation is an effective tool supporting strategic and operational decision-making in production management, particularly in the context of expanding production infrastructure.
SN  - 2353-5156
SN  - 2353-7779
DA  - DEC 1
PY  - 2024
VL  - 30
IS  - 4
SP  - 520
EP  - 527
DO  - 10.30657/pea.2024.30.48
AN  - WOS:001359819100003
ER  -

TY  - CPAPER
AU  - Slaninová, K
AU  - Vymetal, D
AU  - Martinovic, J
ED  - Benatallah, B
ED  - Bestavros, A
ED  - Catania, B
ED  - Haller, A
ED  - Manolopoulos, Y
ED  - Vakali, A
ED  - Zhang, Y
TI  - Analysis of Event Logs: Behavioral Graphs
T2  - WEB INFORMATION SYSTEMS ENGINEERING - WISE 2014 WORKSHOPS
CP  - 15th International Conference on Web Information Systems Engineering (WISE)
AB  - Analysis of event logs is very important discipline used for the evaluation of performance and control-flow issues within the systems. This type of analysis is typically used in process mining sphere, where information systems, for example workflow management systems, enterprise resource planning systems, customer relationship management, supply chain management systems, and business to business systems record transactions and executed activities in a systematic way. Social network analysis takes part of process mining techniques, focused on activity performers, on users. The authors present a new approach to analysis of user behavior in the systems. The approach allows to find behavioral patterns and to find groups of users with similar behavior. An observer can obtain relations between the users on the basis of their similar behavior. The visualization of relations between the users is then presented by so called behavioral graphs. The approach was tested for event log analysis of a virtual company model developed as a multi-agent system by modeling environment MAREA.
SN  - 0302-9743
SN  - 1611-3349
SN  - 978-3-319-20370-6
SN  - 978-3-319-20369-0
PY  - 2015
VL  - 9051
SP  - 42
EP  - 56
DO  - 10.1007/978-3-319-20370-6_4
AN  - WOS:000363774800004
ER  -

TY  - JOUR
AU  - Wagner, L
AU  - Schneider, DN
AU  - Mayer, L
AU  - Jell, A
AU  - Müller, C
AU  - Lenz, A
AU  - Knoll, A
AU  - Wilhelm, D
TI  - Towards multimodal graph neural networks for surgical instrument anticipation
T2  - INTERNATIONAL JOURNAL OF COMPUTER ASSISTED RADIOLOGY AND SURGERY
AB  - PurposeDecision support systems and context-aware assistance in the operating room have emerged as the key clinical applications supporting surgeons in their daily work and are generally based on single modalities. The model- and knowledge-based integration of multimodal data as a basis for decision support systems that can dynamically adapt to the surgical workflow has not yet been established. Therefore, we propose a knowledge-enhanced method for fusing multimodal data for anticipation tasks.MethodsWe developed a holistic, multimodal graph-based approach combining imaging and non-imaging information in a knowledge graph representing the intraoperative scene of a surgery. Node and edge features of the knowledge graph are extracted from suitable data sources in the operating room using machine learning. A spatiotemporal graph neural network architecture subsequently allows for interpretation of relational and temporal patterns within the knowledge graph. We apply our approach to the downstream task of instrument anticipation while presenting a suitable modeling and evaluation strategy for this task.ResultsOur approach achieves an F1 score of 66.86% in terms of instrument anticipation, allowing for a seamless surgical workflow and adding a valuable impact for surgical decision support systems. A resting recall of 63.33% indicates the non-prematurity of the anticipations.ConclusionThis work shows how multimodal data can be combined with the topological properties of an operating room in a graph-based approach. Our multimodal graph architecture serves as a basis for context-sensitive decision support systems in laparoscopic surgery considering a comprehensive intraoperative operating scene.
SN  - 1861-6410
SN  - 1861-6429
DA  - OCT
PY  - 2024
VL  - 19
IS  - 10
SP  - 1929
EP  - 1937
DO  - 10.1007/s11548-024-03226-8
C6  - JUL 2024
AN  - WOS:001269064600001
ER  -

TY  - JOUR
AU  - Mahadevan, VS
AU  - Grindeanu, I
AU  - Jacob, R
AU  - Sarich, J
TI  - Improving climate model coupling through a complete mesh representation: a case study with E3SM (v1) and MOAB (v5.x)
T2  - GEOSCIENTIFIC MODEL DEVELOPMENT
AB  - One of the fundamental factors contributing to the spatiotemporal inaccuracy in climate modeling is the mapping of solution field data between different discretizations and numerical grids used in the coupled component models. The typical climate computational workflow involves evaluation and serialization of the remapping weights during the preprocessing step, which is then consumed by the coupled driver infrastructure during simulation to compute field projections. Tools like Earth System Modeling Framework (ESMF) (Hill et al., 2004) and TempestRemap (Ullrich et al., 2013) offer capability to generate conservative remapping weights, while the Model Coupling Toolkit (MCT) (Larson et al., 2001) that is utilized in many production climate models exposes functionality to make use of the operators to solve the coupled problem. However, such multistep processes present several hurdles in terms of the scientific workflow and impede research productivity. In order to overcome these limitations, we present a fully integrated infrastructure based on the Mesh Oriented datABase (MOAB) (Tautges et al., 2004; Mahadevan et al., 2015) library, which allows for a complete description of the numerical grids and solution data used in each submodel. Through a scalable advancing-front intersection algorithm, the supermesh of the source and target grids are computed, which is then used to assemble the high-order, conservative, and monotonicity-preserving remapping weights between discretization specifications. The Fortran-compatible interfaces in MOAB are utilized to directly link the submodels in the Energy Exascale Earth System Model (E3SM) to enable online remapping strategies in order to simplify the coupled workflow process. We demonstrate the superior computational efficiency of the remapping algorithms in comparison with other state-of-the-science tools and present strong scaling results on large-scale machines for computing remapping weights between the spectral element atmosphere and finite volume discretizations on the polygonal ocean grids.
SN  - 1991-959X
SN  - 1991-9603
DA  - MAY 26
PY  - 2020
VL  - 13
IS  - 5
SP  - 2355
EP  - 2377
DO  - 10.5194/gmd-13-2355-2020
AN  - WOS:000537580900001
ER  -

TY  - JOUR
AU  - Herz, G
AU  - Rix, C
AU  - Jacobasch, E
AU  - Müller, N
AU  - Reichelt, E
AU  - Jahn, M
AU  - Michaelis, A
TI  - Economic assessment of Power-to-Liquid processes ? Influence of electrolysis technology and operating conditions
T2  - APPLIED ENERGY
AB  - Power-to-Liquid (PtL) processes are considered as a key technology for a fossil-free raw material and energy system. With multiple technical analyses being available and technical feasibility being proven by first pilot plants, pathways towards commercial market entry are of increasing interest. In this work multiple economic aspects of Power-to-Liquid plants are being investigated. First and foremost, the seamless integration of an economic analysis in the process modeling workflow will be demonstrated. This allows for an extensive investigation of the influence of operating conditions of the considered solid oxide electrolyzer (SOEL) on process economics and a subsequent optimization not only from an engineering standpoint but considering economics as well. Furthermore, the modular nature of the model allows for a comparison of SOEL to the more mature technology of low-temperature electrolysis with a focus on possible heat integration and by-product utilization. The potential of SOEL technology for high energetic efficiency and subsequently low production cost is highlighted. The conducted forecast to 2050 shows that SOEL-based Power-to-Liquid processes offer lower production cost of NPC = 0.203 ?2020/kWhch compared to production cost of NPC = 0.262 ?2020/kWhch for the PEMELbased process. Furthermore, based on the results of the economic assessment possible governmental support mechanisms are studied, showing that projected values for governmental incentives are expected to decrease CO2 mitigation cost from ?CO2 = 791 ?2020/tCO2 to ?CO2 = 419 ?2020/tCO2 for the 2050 scenario. Thus, existing measures and currently discussed measures are not sufficient to ensure economic viability. Consequently, more extensive schemes such as mandatory quotas for sustainable PtL products need to be implemented in order to facilitate the market entry.
SN  - 0306-2619
SN  - 1872-9118
DA  - JUN 15
PY  - 2021
VL  - 292
C7  - 116655
DO  - 10.1016/j.apenergy.2021.116655
C6  - APR 2021
AN  - INSPEC:20862636
ER  -

TY  - JOUR
AU  - Polini, A
AU  - Polzonetti, A
AU  - Re, B
TI  - FORMAL METHODS TO IMPROVE PUBLIC ADMINISTRATION BUSINESS PROCESSES
T2  - RAIRO-THEORETICAL INFORMATICS AND APPLICATIONS
AB  - Starting from late 90's the public administration has started to employ a quite relevant amount of its budget in developing ICT solutions to better deliver services to citizens. In spite of this effort many statistics show that the mere availability of ICT based services does not guarantee per se their usage. Citizens have continued to largely access services through "traditional" means. In our study we suggest that the highlighted situation is partly due to the fact that relevant domain dependent requirements, mainly related to the delivery process of e-government digital services, are often ignored in the development of e-government solutions. We provide here a domain related quality framework and encoded it in a set of formal statements, so that we can apply automatic verification techniques to assess and improve ICT solutions adopted by public administrations. The paper discusses both the defined quality framework and the tool chain we developed to enable automatic assessment of ICT solutions. The tool chain is based on a denotational mapping of business process modeling notation elements into process algebraic descriptions and to the encoding of quality requirements in linear temporal logic formulas. The resulting approach has been applied to real case studies with encouraging results.
SN  - 0988-3754
SN  - 1290-385X
DA  - APR
PY  - 2012
VL  - 46
IS  - 2
SP  - 203
EP  - 229
DO  - 10.1051/ita/2012002
AN  - WOS:000303447600002
ER  -

TY  - JOUR
AU  - Ranjbar, S
AU  - Losos, D
AU  - Hoffman, S
AU  - Stoy, PC
TI  - High-Frequency Mapping of Downward Shortwave Radiation From GOES-R Using Gradient Boosting
T2  - IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
AB  - This study investigates high-frequency mapping of downward shortwave radiation (DSR) at the Earth's surface using the advanced baseline imager (ABI) instrument mounted on Geostationary Operational Environmental Satellite-R Series (GOES-R). The existing GOES-R DSR product (DSRABI) offers hourly temporal resolution and spatial resolution of 0.25 degrees. To enhance these resolutions, we explore machine learning (ML) for DSR estimation at the native temporal resolution of GOES-R Level-2 cloud and moisture imagery product (5 min) and its native spatial resolution of 2 km at nadir. We compared four common ML regression models through the leave-one-out cross-validation algorithm for robust model assessment against ground measurements from AmeriFlux and SURFRAD networks. Results show that gradient boosting regression (GBR) achieves the best performance (R-2 = 0.916, RMSE = 88.05 W<middle dot>m(-2)) with more efficient computation compared to long short-term memory, which exhibited similar performance. DSR estimates from the GBR model through the ABI live imaging of vegetated ecosystems workflow (DSRALIVE) outperform DSRABI across various temporal resolutions and sky conditions. DSRALIVE agreement with ground measurements at SURFRAD networks exhibits high accuracy at high temporal resolutions (5-min intervals) with R-2 exceeding 0.85 and RMSE = 122 W<middle dot>m(-2). We conclude that GBR offers a promising approach for high-frequency DSR mapping from GOES-R, enabling improved applications for near-real-time monitoring of terrestrial carbon and water fluxes.
SN  - 1939-1404
SN  - 2151-1535
PY  - 2024
VL  - 17
SP  - 11958
EP  - 11968
DO  - 10.1109/JSTARS.2024.3420148
AN  - WOS:001272210000005
ER  -

TY  - JOUR
AU  - Pawlak, TP
AU  - Litwiniuk, B
TI  - Ellipsoidal one-class constraint acquisition for quadratically constrained programming
T2  - EUROPEAN JOURNAL OF OPERATIONAL RESEARCH
AB  - We propose Ellipsoidal One-Class Constraint Acquisition (EOCCA), a fast and scalable algorithm for the acquisition of constraints for Mixed-Integer Quadratically Constrained Programming (MIQCP) models from data. EOCCA acquires a well-formed MIQCP model using solely the examples of the feasible solutions to this model. It combines x-means partitioning, standardization, and principal components analysis to preprocess the training set and then wraps the preprocessed data into several hyper-ellipsoids expressed using MIQCP constraints. These MIQCP constraints are projected back to the space of the original training set, and their further use does not require data preprocessing. Experimental evaluation shows that EOCCA scores better than a state-of-the-art algorithm in terms of fidelity of the acquired constraints to ground-truth constraints and achieves this in few orders of magnitude shorter time. We demonstrate the practical use case of EOCCA in a fully automated workflow of modeling and optimization of a rice farm using real-world data. (C) 2020 Elsevier B.V. All rights reserved.
SN  - 0377-2217
SN  - 1872-6860
DA  - AUG 16
PY  - 2021
VL  - 293
IS  - 1
SP  - 36
EP  - 49
DO  - 10.1016/j.ejor.2020.12.018
C6  - MAR 2021
AN  - WOS:000628803700003
ER  -

TY  - JOUR
AU  - Ferrante, S
AU  - Bonacina, S
AU  - Pozzi, G
AU  - Pinciroli, F
AU  - Marceglia, S
TI  - A Design Methodology for Medical Processes
T2  - APPLIED CLINICAL INFORMATICS
AB  - Background: Healthcare processes, especially those belonging to the clinical domain, are acknowledged as complex and characterized by the dynamic nature of the diagnosis, the variability of the decisions made by experts driven by their experiences, the local constraints, the patient's needs, the uncertainty of the patient's response, and the indeterminacy of patient's compliance to treatment. Also, the multiple actors involved in patient's care need clear and transparent communication to ensure care coordination.
   Objectives: In this paper, we propose a methodology to model healthcare processes in order to break out complexity and provide transparency.
   Methods: The model is grounded on a set of requirements that make the healthcare domain unique with respect to other knowledge domains. The modeling methodology is based on three main phases: the study of the environmental context, the conceptual modeling, and the logical modeling.
   Results: The proposed methodology was validated by applying it to the case study of the rehabilitation process of stroke patients in the specific setting of a specialized rehabilitation center. The resulting model was used to define the specifications of a software artifact for the digital administration and collection of assessment tests that was also implemented.
   Conclusions: Despite being only an example, our case study showed the ability of process modeling to answer the actual needs in healthcare practices. Independently from the medical domain in which the modeling effort is done, the proposed methodology is useful to create high-quality models, and to detect and take into account relevant and tricky situations that can occur during process execution.
SN  - 1869-0327
PY  - 2016
VL  - 7
IS  - 1
SP  - 191
EP  - 210
DO  - 10.4338/ACI-2015-08-RA-0111
AN  - WOS:000412986800002
ER  -

TY  - JOUR
AU  - Ayora, C
AU  - Torres, V
AU  - Weber, B
AU  - Reichert, M
AU  - Pelechano, V
TI  - VIVACE: A framework for the systematic evaluation of variability support in process-aware information systems
T2  - INFORMATION AND SOFTWARE TECHNOLOGY
AB  - Context: The increasing adoption of process-aware information systems (PAISs) such as workflow management systems, enterprise resource planning systems, or case management systems, together with the high variability in business processes (e.g., sales processes may vary depending on the respective products and countries), has resulted in large industrial process model repositories. To cope with this business process variability, the proper management of process variants along the entire process lifecycle becomes crucial.
   Objective: The goal of this paper is to develop a fundamental understanding of business process variability. In particular, the paper will provide a framework for assessing and comparing process variability approaches and the support they provide for the different phases of the business process lifecycle (i.e., process analysis and design, configuration, enactment, diagnosis, and evolution).
   Method: We conducted a systematic literature review (SLR) in order to discover how process variability is supported by existing approaches.
   Results: The SLR resulted in 63 primary studies which were deeply analyzed. Based on this analysis, we derived the VIVACE framework. VIVACE allows assessing the expressiveness of a process modeling language regarding the explicit specification of process variability. Furthermore, the support provided by a process-aware information system to properly deal with process model variants can be assessed with VIVACE as well.
   Conclusions: VIVACE provides an empirically-grounded framework for process engineers that enables them to evaluate existing process variability approaches as well as to select that variability approach meeting their requirements best. Finally, it helps process engineers in implementing PAISs supporting process variability along the entire process lifecycle. (C) 2014 Elsevier B.V. All rights reserved.
SN  - 0950-5849
SN  - 1873-6025
DA  - JAN
PY  - 2015
VL  - 57
SP  - 248
EP  - 276
DO  - 10.1016/j.infsof.2014.05.009
AN  - WOS:000345947500015
ER  -

TY  - JOUR
AU  - Arno, P
TI  - From Theory to Practice: Empirical Insights on BPM Lifecycle Adoption and Low-Code Utilization in SMEs
T2  - IEEE ACCESS
AB  - Small and medium-sized enterprises (SMEs) face growing pressure to adapt to dynamic, competitive markets. Business Process Management (BPM) offers a strategic pathway to operational efficiency and digital transformation. While the BPM lifecycle and low-code platforms are frequently presented as key enablers for SMEs, few empirical studies examine their practical implementation. This study addresses two research questions: 1) how SMEs use low-code technologies to support BPM, and 2) how they apply - or deviate from - the traditional BPM lifecycle. Using a triangulated research design combining a rapid literature review and an expert survey of BPM practitioners in SMEs, the study finds that low-code tools are widely adopted for basic BPM tasks such as process modeling and workflow automation. However, advanced features like AI and process mining are underutilized. Likewise, SMEs often skip or adapt BPM lifecycle phases due to organizational constraints. These findings highlight a gap between BPM theory and SME practice, emphasizing the need for more adaptive BPM frameworks and accessible low-code solutions tailored to SME contexts.
SN  - 2169-3536
PY  - 2025
VL  - 13
SP  - 117385
EP  - 117403
DO  - 10.1109/ACCESS.2025.3585261
AN  - WOS:001527248900036
ER  -

TY  - JOUR
AU  - Mülle, J
AU  - Tex, C
AU  - Böhm, K
TI  - A practical data-flow verification scheme for business processes
T2  - INFORMATION SYSTEMS
AB  - Data in business processes becomes more and more important. Current standard languages for process modeling like BPMN 2.0 which include the data flow reflect this. Ensuring the correctness of the data flow in processes is challenging. Model checking, i.e., verifying properties of process models, is a well-known technique to this end. An important part of model checking is the construction of the state space of the model. However, state-space explosion typically is in the way of an effective verification. We study how to overcome this problem in our context by means of reduction. More specifically, we propose a reduction on the level of the process model. To our knowledge, this is new for the data-flow analysis of processes. The core of our approach are so-called regions of the process model that are relevant for the verification of properties describing the data flow. Non-relevant regions are candidates for reduction of the process model, yielding a smaller state space. Our evaluation shows that our approach works well on industrial process models. (C) 2018 Elsevier Ltd. All rights reserved.
SN  - 0306-4379
SN  - 1873-6076
DA  - MAR
PY  - 2019
VL  - 81
SP  - 136
EP  - 151
DO  - 10.1016/j.is.2018.12.002
AN  - WOS:000459839400009
ER  -

TY  - JOUR
AU  - Mahmoudi, A
AU  - Neskovic, A
AU  - Thermann, C
AU  - Sehm, R
AU  - Hübner, C
AU  - Plattenteich, T
AU  - Meyer, R
AU  - Buchty, R
AU  - Berekovic, M
AU  - Mulhem, S
TI  - A Systematic Mapping Study on SystemC/TLM Modeling Capabilities in New Research Domains
T2  - ACM TRANSACTIONS ON DESIGN AUTOMATION OF ELECTRONIC SYSTEMS
AB  - With increasingly complex circuits and systems, the need for advanced design methodologies is growing. These methodologies shift the designers' focus from technology-specific implementations to more abstract electronic system design (ESL). SystemC was developed to address this need. Being an open standard based on C++, SystemC facilitates hardware and software modeling across multiple levels of abstraction, with a particular emphasis on ESL. It is further enhanced by including the transaction-level modeling (TLM) layer, strengthening its capability to model communication between components, and even full-system simulators. Traditionally, SystemC/TLM has been deployed to provide hardware prototypes for software development early in the design process. However, surveys and literature reviews showing other capabilities of SystemC/TLM are scarce. Hence, it is essential to explore SystemC/TLM's new capabilities in different domains such as in-circuit fault propagation, security assessment, and verification. In this article, we conduct a systematic mapping study (SMS) of SystemC/TLM modeling capabilities in certain research domains. We elaborate on the state-of-the-art ESL with an emphasis on SystemC/TLM-based system modeling.
SN  - 1084-4309
SN  - 1557-7309
DA  - JUL
PY  - 2025
VL  - 30
IS  - 4
C7  - 53
DO  - 10.1145/3735641
AN  - WOS:001553607200014
ER  -

TY  - JOUR
AU  - Pillat, RM
AU  - Oliveira, TC
AU  - Alencar, PSC
AU  - Cowan, DD
TI  - BPMNt: A BPMN extension for specifying software process tailoring
T2  - INFORMATION AND SOFTWARE TECHNOLOGY
AB  - Context: Although SPEM 2.0 has great potential for software process modeling, it does not provide concepts or formalisms for precise modeling of process behavior. Indeed, SPEM fails to address process simulation, execution, monitoring and analysis, which are important activities in process management. On the other hand, BPMN 2.0 is a widely used notation to model business processes that has associated tools and techniques to facilitate the aforementioned process management activities. Using BPMN to model software development processes can leverage BPMN's infrastructure to improve the quality of these processes. However, BPMN lacks an important feature to model software processes: a mechanism to represent process tailoring.
   Objective: This paper proposes BPMNt, a conservative extension to BPMN that aims at creating a tailoring representation mechanism similar to the one found in SPEM 2.0.
   Method: We have used the BPMN 2.0 extensibility mechanism to include the representation of specific tailoring relationships namely suppression, local contribution, and local replacement, which establish links between process elements (such as in the case of SPEM). Moreover, this paper also presents some rules to ensure the consistency of BPMN models when using tailoring relationships.
   Results: In order to evaluate our proposal we have implemented a tool to support the BPMNt approach and have applied it for representing real process adaptations in the context of an academic management system development project. Results of this study showed that the approach and its support tool can successfully be used to adapt BPMN-based software processes in real scenarios.
   Conclusion: We have proposed an approach to enable reuse and adaptation of BPMN-based software process models as well as derivation traceability between models through tailoring relationships. We believe that bringing such capabilities into BPMN will open new perspectives to software process management. (C) 2014 Elsevier B.V. All rights reserved.
SN  - 0950-5849
SN  - 1873-6025
DA  - JAN
PY  - 2015
VL  - 57
SP  - 95
EP  - 115
DO  - 10.1016/j.infsof.2014.09.004
AN  - WOS:000345947500007
ER  -

TY  - JOUR
AU  - Gross, J
AU  - Buettner, R
AU  - Baumgartl, H
TI  - Benchmarking Transfer Learning Strategies in Time-Series Imaging: Recommendations for Analyzing Raw Sensor Data
T2  - IEEE ACCESS
AB  - With the growing availability and complexity of time-series sequences, scalable and robust machine learning approaches are required that overcome the sampling challenge of quantitatively sufficient training data. Following the research trend towards the deep learning-based analysis of time-series encoded as images, this study proposes a time-series imaging workflow that overcomes the challenge of quantitatively limited sensor data across domains (i.e., medicine and engineering). After systematically identifying the three relevant dimensions that affect the performance of the deep learning-based analysis of visualized time-series data, we performed a benchmarking evaluation with a total of 24 unique convolutional neural network models. Following a two-level transfer learning investigation, we reveal that fine-tuning the mid-level features results in the best classification performance. As a result, we present an optimized representation of the VGG16 network, which outperforms previous studies in the field. Our approach is accurate, robust, and manifests internal and external validity. By only using the raw time-series data, our model does not require manual feature engineering, being of high practical relevance. As the post-hoc analysis of our results reveals that our model allows automated extraction of meaningful features based on the trend of the underlying time-series data, our study also adds to explainable artificial intelligence. Furthermore, our proposed workflow reduces the sequence length of the input data while preserving all information. Especially with the hurdle of long-term dependencies in sequential time-series data, we overcome related work's limitation of the vanishing gradients problem and contribute to the sequential learning theory in artificial intelligence.
SN  - 2169-3536
PY  - 2022
VL  - 10
SP  - 16977
EP  - 16991
DO  - 10.1109/ACCESS.2022.3148711
AN  - WOS:000756649100001
ER  -

TY  - JOUR
AU  - Watanasiri, S
TI  - Development of on-demand critically evaluated thermophysical properties data in process simulation
T2  - PURE AND APPLIED CHEMISTRY
AB  - Accurate thermophysical properties are essential to the development of high-quality process simulation models of chemical processes. Therefore, process-modeling software (simulator) must provide accurate, reliable, and easily accessible property data and models to enable efficient and robust process design. Property data and parameters for components of interest are generally available in the databases of the simulator. For components that are not in the databases, their property data must be supplied by the user. The number of components available in a typical simulator is about 1700. The number and types of components available in the simulator limit the scope and accuracy of process models that can be developed.
   In this paper, we review past practice in obtaining the necessary property data required in developing a process model and describe a new methodology that can be used to overcome the shortcomings of the current method. The new method is based on the dynamic data evaluation concept that combines the experimental data obtained from a comprehensive electronic database with structure-based property estimation system and data analysis and regression programs to generate critically evaluated property data. The concept and necessary software have been implemented in a process simulator, resulting in a new workflow that enables high-fidelity process models to be developed more easily and efficiently.
SN  - 0033-4545
PY  - 2011
VL  - 83
IS  - 6
SP  - 1255
EP  - 1281
DO  - 10.1351/PAC-CON-10-11-18
AN  - WOS:000290852100005
ER  -

TY  - CPAPER
AU  - Maqbool, B
AU  - Azam, F
AU  - Anwar, MW
AU  - Butt, WH
AU  - Zeb, J
AU  - Zafar, I
AU  - Nazir, AK
AU  - Umair, Z
ED  - Kim, KJ
ED  - Baek, N
TI  - A Comprehensive Investigation of BPMN Models Generation from Textual Requirements-Techniques, Tools and Trends
T2  - INFORMATION SCIENCE AND APPLICATIONS 2018, ICISA 2018
CP  - iCatse International Conference on Information Science and Applications (ICISA)
AB  - Business Process Modeling Languages (BPML's) are continuously getting attraction of software development communities due to the fact of specifying complex business requirements with simplicity. However, the development of business process models from textual requirements through existing BPML's is a time consuming task. In this context, Natural Language Processing (NLP) techniques are commonly applied to automatically generate business process models from textual requirements. Business Process Model and Notation (BPMN) is a well-known BPML. This article comprehensively investigates modern techniques, tools and trends for the generation of BPMN models from textual requirements by utilizing NLP techniques. Particularly, a Systematic Literature Review (SLR) is performed to select and evaluate 36 research studies published in the span of 2010-2018. As a result, 11 NLP and 8 BPMN tools are identified. Furthermore, 8 commonly generated BPMN constructs are recognized. Finally, a comparative analysis of NLP and BPMN tools is performed with the help of important evaluation parameters. It is concluded that the existing NLP techniques and tools significantly simplify the process of BPMN models generation from textual requirements. However, the existing approaches are inadequate to be applied in the industries, especially for real-time systems.
SN  - 1876-1100
SN  - 1876-1119
SN  - 978-981-13-1056-0
SN  - 978-981-13-1055-3
PY  - 2019
VL  - 514
SP  - 543
EP  - 557
DO  - 10.1007/978-981-13-1056-0_54
AN  - WOS:000454443400052
ER  -

TY  - CPAPER
AU  - Garcia, JI
AU  - Junqueira, F
AU  - Morales, RA
AU  - Miyagi, PE
A1  - IEEE
TI  - A Procedure for Modeling and Analysis of Service-Oriented and Distributed Productive Systems
T2  - 2008 IEEE INTERNATIONAL CONFERENCE ON AUTOMATION SCIENCE AND ENGINEERING, VOLS 1 AND 2
CP  - IEEE International Conference on Automation Science and Engineering
AB  - Geographic dispersion and distribution of productive activities in relatively autonomous systems are viable due to the advances in mechatronics, communication and information technologies. These productive systems are then composed of several components (sub-systems) with some degree of autonomy, and these must present a collaborative relationship to assure the overall performance of the system. Therefore, the challenge is to implement a collaborative distributed architecture that assures effective interactions among the services, in special for tele-operation and remote monitoring operations. Thus, new modeling techniques that guarantee verification and validation of the collaborative systems specifications are necessary. In this context, this paper proposes a procedure for modeling and analysis of service-oriented and distributed productive systems based on the characterization of the distributed productive system as a discrete event dynamic system. This procedure uses techniques derived from interpreted Petri net in order to perform the system modeling. Different levels of abstraction are considered to model the systems: the conceptual description is obtained through the Production Flow Schema (PFS) technique and a refinement into functional models through the Petri net technique. Then, a procedure to detail the workflow process in the distributed productive system is described with emphasis on the integration of the component models.
SN  - 2161-8070
SN  - 978-1-4244-2022-3
PY  - 2008
SP  - 941
EP  - +
DO  - 10.1109/COASE.2008.4626561
AN  - WOS:000261320400156
ER  -

TY  - JOUR
AU  - Neumuth, T
AU  - Meissner, C
TI  - Online recognition of surgical instruments by information fusion
T2  - INTERNATIONAL JOURNAL OF COMPUTER ASSISTED RADIOLOGY AND SURGERY
AB  - Automatic online recognition of surgical instruments is required to monitor instrument use for surgical process modeling. A system was developed and tested using available technologies.
   A recognition system was developed using RFID technology to identify surgical activities. Information fusion for online recognition of surgical process models was conceived as a layer model to abstract information from specific sensor technologies. Redundant, complementary, and cooperative sensor signal fusion was used in the layer model to increase the surgical instrument recognition rate. Several different information fusion strategies were evaluated for situation recognition abilities in a mock-up environment based on simulations of surgical processes.
   This information fusion system was able to reliably detect, identify, and localize surgical instruments in an interventional suite. A combination of information fusion strategies was able to achieve a correct classification rate of 97% and was as effective as observer-based acquisition methods.
   Different information fusion strategies for the recognition of surgical instruments were evaluated, showing that redundant, complementary, and cooperative information fusion is feasible for recognition of surgical work steps. A combination of sensor- and observer-based modeling strategies provides the most robust solution for surgical process models.
SN  - 1861-6410
SN  - 1861-6429
DA  - MAR
PY  - 2012
VL  - 7
IS  - 2
SP  - 297
EP  - 304
DO  - 10.1007/s11548-011-0662-5
AN  - MEDLINE:22005841
ER  -

TY  - JOUR
AU  - Prinz, F
AU  - Schoeffler, M
AU  - Lechler, A
AU  - Verl, A
TI  - A novel I4.0-enabled engineering method and its evaluation
T2  - INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY
AB  - Recent trends show that products are becoming more complex and multi-variant. Therefore, future production systems need to become more advanced in terms of reconfigurability, flexibility, and transformability. To achieve these advancements, future systems must be highly changeable and support plug-and-produce approaches. The majority of today's engineering methods focus on static workflows based on predefined assets and setups. As a consequence, changes in the production system come with high costs, especially during production process execution. Therefore, new engineering methods are required which are explicitly designed for highly changeable production systems. To contribute towards fully changeable production systems, an I4.0 framework is proposed that covers the entire engineering process. The focus is set on presenting a graphical I4.0-enabled engineering method that enables dynamic workflows with varying assets and setups. Moreover, in order to evaluate the method, a user study was conducted, in which participants were asked to solve multiple engineering tasks by utilizing the presented I4.0-enabled method as well as a conventional approach. The results indicated that the proposed I4.0-enabled engineering method significantly outperformed the conventional method in terms of required engineering times and subjective ratings.
SN  - 0268-3768
SN  - 1433-3015
DA  - JUN
PY  - 2019
VL  - 102
IS  - 5-8
SP  - 2245
EP  - 2263
DO  - 10.1007/s00170-019-03382-1
AN  - WOS:000469002200086
ER  -

TY  - JOUR
AU  - Uemura, M
AU  - Jannin, P
AU  - Yamashita, M
AU  - Tomikawa, M
AU  - Akahoshi, T
AU  - Obata, S
AU  - Souzaki, R
AU  - Ieiri, S
AU  - Hashizume, M
TI  - Procedural surgical skill assessment in laparoscopic training environments
T2  - INTERNATIONAL JOURNAL OF COMPUTER ASSISTED RADIOLOGY AND SURGERY
AB  - This study aimed to identify detailed differences in laparoscopic surgical processes between expert and novice surgeons in a training environment and demonstrate that surgical process modeling can be used for such detailed analysis.
   Eleven expert surgeons each of whom had performed laparoscopic procedures were compared with 10 young surgeons each of whom had performed laparoscopic procedures, and five medical students. Each examinee performed a specific skill assessment task. During tasks, instrument motion was monitored using a video capture system. From the video, the corresponding workflow was recorded by labeling the surgeons' activities according to a predefined terminology. Activities represented manual work steps performed during the task, described by a combination of a verb (representing the action), a tool, and the involved structure. The results were described as the number of occurrences (times), average duration (seconds), total duration (seconds), minimal duration (seconds), maximal duration (seconds), and occupancy percentage (%).
   The terminology for describing the processes of this task included 10 actions, six tools, four structures, and three events for each hand. There were 63 combinations of different possible activities; significant differences in 12 activities were observed between the expert and novice groups (young surgeons and medical students). The expert group performed the task with fewer occurrences and shorter duration than did the novice group in the left hand.
   We identified differences in surgical process between experts and novices in laparoscopic surgical simulation. Our proposed method would be useful for education and training in laparoscopic surgery.
SN  - 1861-6410
SN  - 1861-6429
DA  - APR
PY  - 2016
VL  - 11
IS  - 4
SP  - 543
EP  - 552
DO  - 10.1007/s11548-015-1274-2
AN  - MEDLINE:26253582
ER  -

TY  - JOUR
AU  - Lanz, A
AU  - Reichert, M
AU  - Weber, B
TI  - Process time patterns: A formal foundation
T2  - INFORMATION SYSTEMS
AB  - Companies increasingly adopt process-aware information systems (PAISs) to model, execute, monitor, and evolve their business processes. Though the handling of temporal constraints (e.g., deadlines or time lags between activities) is crucial for the proper support of business processes, existing PAISs vary significantly regarding the support of the temporal perspective. Both the formal specification and the operational support of temporal constraints constitute fundamental challenges in this context. In previous work, we introduced process time patterns, which facilitate the comparison and evaluation of PAISs in respect to their support of the temporal perspective. Furthermore, we provided empirical evidence for these time patterns. To avoid ambiguities and to ease the use as well as the implementation of the time patterns, this paper formally defines their semantics. To additionally foster the use of the patterns for a wide range of process modeling languages and to enable pattern integration with existing PAISs, the proposed semantics are expressed independently of a particular process meta model. Altogether, the presented pattern formalization will be fundamental for introducing the temporal perspective in PAISs. (C) 2015 Elsevier Ltd. All rights reserved.
SN  - 0306-4379
SN  - 1873-6076
DA  - APR
PY  - 2016
VL  - 57
SP  - 38
EP  - 68
DO  - 10.1016/j.is.2015.10.002
AN  - WOS:000371358900003
ER  -

TY  - CPAPER
AU  - Wolf, F
AU  - Kruse, J
AU  - Ernst, R
ED  - Courtois, B
ED  - Demidenko, SN
TI  - Compact trace generation and power measurement in software emulation
T2  - DESIGN, MODELING AND SIMULATION IN MICROELECTRONICS
CP  - Conference on Design, Modeling, and Simulation in Microelectronics
AB  - Evaluation boards are popular as prototyping platforms in embedded software development. They often are preferred over simulation to avoid modeling effort and simulation times as well as over complete hardware prototypes to avoid development cost. Evaluation boards provide accurate timing results as long as the main architecture parameters match the target hardware system. For larger processors, this is often not the case since the cache and main memory architectures might differ. Another problem is the lack of observability of the software execution. Pin-Out versions of processors with improved observability are expensive (so are in-circuit emulators) and not always available, and on-chip processor test support requires software adaptation. A particular problem arises when trying to verify the running time bounds of embedded software such as required for hard realtime systems. Here, formal analysis approaches have been proposed which require segment-wise execution of a program under investigation. Another problem is the accurate analysis of processor power consumption for different execution paths.
   The paper presents an approach to fast acquisition of compact timed execution traces with instruction cycle accurate power samples on commercial evaluation kits. Global system modeling abstracts the environment to a set of parameters that is included in the software under investigation for segment-wise, real-time execution. Trigger points write source code line numbers and energy samples to the address and data bus where they are read by a logic state analyzer. Experiments show that the application of trigger points avoids the acquisition of long, complete traces on sophisticated, dedicated prototyping platforms as in previous work while more accurate execution time and power consumption can be delivered.
SN  - 0277-786X
SN  - 1996-756X
SN  - 0-8194-3900-2
PY  - 2000
VL  - 4228
SP  - 97
EP  - 108
DO  - 10.1117/12.405400
AN  - WOS:000167995500012
ER  -

TY  - JOUR
AU  - PFLEEGER, SL
TI  - MATURITY, MODELS, AND GOALS - HOW TO BUILD A METRICS PLAN
T2  - JOURNAL OF SYSTEMS AND SOFTWARE
AB  - Many software project managers want to implement a process or product improvement program for their software development or maintenance project. To be effective, the improvement activities must be accompanied by measurements to support them. Thus, managers must select metrics for a measurement plan that will start small and address key project needs. This article explains what a metrics plan is and describes an approach that combines a goal-question-metric analysis with process maturity assessment and process modeling. This combination allows an organization to tailor metrics collection and analysis to the needs and characteristics of the project and organization that will use the metrics. The goal-question-metric technique ensures that each measurement is useful to someone in the organization. The process maturity framework is used to ensure that what needs to be measured is visible in the process, and the process model helps the project manager understand when and where metrics will be collected and how they will be used. In concert, these techniques enable the project manager to migrate from a small, initial set of key indicators to a larger, more comprehensive measurement program. The article lists example goals, questions, and metrics that have been identified for a defense organization and a reuse project manager, and describes how the techniques discussed have actually been used on real metrics programs to tailor metrics plans to individual project needs.
SN  - 0164-1212
DA  - NOV
PY  - 1995
VL  - 31
IS  - 2
SP  - 143
EP  - 155
DO  - 10.1016/0164-1212(94)00094-4
AN  - WOS:A1995TA70700006
ER  -

TY  - JOUR
AU  - Neumuth, D
AU  - Loebe, F
AU  - Herre, H
AU  - Neumuth, T
TI  - Modeling surgical processes: A four-level translational approach
T2  - ARTIFICIAL INTELLIGENCE IN MEDICINE
AB  - Motivation: The precise and formal specification of surgical interventions is a necessary requirement for many applications in surgery, including teaching and learning, quality assessment and evaluation, and computer-assisted surgery. Currently, surgical processes are modeled by following various approaches. This diversity lacks a commonly agreed-upon conceptual foundation and thus impedes the comparability, the interoperability, and the uniform interpretation of process data.
   Objective: However, it would be beneficial if scientific models, in the same context, shared a coherent conceptual and formal mathematical basis. Such a uniform foundation would simplify the acquisition and exchange of data, the transition and interpretation of study results, and the transfer and adaptation of methods and tools. Therefore, we propose a generic, formal framework for specifying surgical processes, which is presented together with its design methodology.
   Methods: The methodology follows a four-level translational approach and comprises an ontological foundation for the formal level that orients itself by linguistic theories.
   Results: A unifying framework for modeling surgical processes that is ontologically founded and formally and mathematically precise was developed. The expressive power and the unifying capacity of the presented framework are demonstrated by applying it to four contemporary approaches for surgical process modeling by using the common underlying formalization.
   Conclusions: The presented four-level approach allows for capturing the knowledge of the surgical intervention formally. Natural language terms are consistently translated to an implementation level to support research fields where users express their expert knowledge about processes in natural language, but, in contrast to this, statistical analysis or data mining need to be performed based on mathematically formalized data sets. The availability of such a translational approach is a valuable extension for research regarding the operating room of the future. (C) 2010 Elsevier BM. All rights reserved.
SN  - 0933-3657
SN  - 1873-2860
DA  - MAR
PY  - 2011
VL  - 51
IS  - 3
SP  - 147
EP  - 161
DO  - 10.1016/j.artmed.2010.12.003
AN  - WOS:000290281900001
ER  -

TY  - CPAPER
AU  - Kumar, M
AU  - Yoo, J
AU  - Hong, S
A1  - IEEE
TI  - Enhancing AUTOSAR Methodology to a COTS-based Development Process via Mapping to V-Model
T2  - 2009 IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL EMBEDDED SYSTEMS
CP  - IEEE International Symposium on Industrial Embedded Systems (SIES 2009)
AB  - AUTOSAR, an open standard for automotive software, is currently being exploited by the automotive industry. Although the standard mainly focuses on software architecture, it also provides a development methodology. Unfortunately, the methodology in its current form is insufficient for industrial exploitation because it describes only an incomplete set of activities, work products and their dependencies. Specifically, (1) the activities to support COTS-based development are missing even though AUTOSAR encourages the use of COTS components; (2) it does not describe the roles and their responsibilities; and (3) it does not specify the mapping of activities onto a complete process model. In this paper, we propose a new software development process for AUTOSAR by extending the existing methodology. In doing so, we add activities to allow COTS component selection, evaluation and integration. Then, we define specific roles and assign responsibilities to those roles. Finally, we describe the overall timeline of various activities in detail by mapping the activities to the V-model. In order to present the process, we have used SPEM 2.0 notation, which is backward compatible with the AUTOSAR methodology and has improved expressiveness. We have composed the proposed process model using Eclipse Process Framework Composer which not only performs a sanity check of the model but also provides a way to publish it.
SN  - 2150-3109
SN  - 2150-3117
SN  - 978-1-4244-4109-9
PY  - 2009
SP  - 50
EP  - 53
DO  - 10.1109/SIES.2009.5196192
AN  - WOS:000278758700008
ER  -

TY  - JOUR
AU  - Wang, HB
AU  - Hu, Y
TI  - RETRACTED: Artificial Intelligence Technology Based on Deep Learning in Building Construction Management System Modeling (Retracted Article)
T2  - ADVANCES IN MULTIMEDIA
AB  - In order to explore the application of AI technology in construction management system modeling, the author proposed the application of a deep learning-based AI technology in construction management system modeling. The 3 D reconstruction deep learning model is first introduced, and then the model idea of the construction progress reliability control system is designed based on BIM (building information model). Second, the construction process of the 4dbim model is described, and the construction method is introduced. The construction of the model provides data information for the construction schedule reliability control system. Finally, the three functional modules of progress monitoring, progress reliability early warning, and progress prediction are realized by combining the S-curve comparison method, and the work of the system is described through case simulation. The early warning result is from June 7 to June 11, the progress deviation is between (-2%, 2%), and the progress is basically controlled. On June 13, the planned workload was 81.099%, and the actual cumulative workload was 7.099%, which was 4% less than the planned workload. The project progress was out of date, so it needs to be closely tracked. On June 15, the planned workload was 85.511%, and the actual cumulative workload was 80.899%, 4.5% less than the planned workload. The forecast result is the line forecast of the actual cumulative completion percentage on June 17. After calculation, the forecast result on June 17 is 84.311%. The progress deviation on June 17 was 5.21%. If no timely delay is taken on June 15, the delay will get worse. In addition, the system can predict the completion period of the project. When the actual percentage of cumulative completion is greater than or equal to 100%, it indicates that the project has been completed. Therefore, we can calculate the completion period of the three-storey project, and the construction period is about June 29 or 30. When the simulation can be carried out, the simulation number is set as 1000 times, the completion probability of the project is only 40%, and the completion probability is not too high. Artificial intelligence technology can realize progress monitoring, progress reliability early warning, and progress prediction. This system model prepares for software development and is conducive to improving the progress reliability control level of construction enterprises. Starting from the schedule planning subsystem and the schedule control subsystem, this paper studies the application of the artificial intelligence technology based on deep learning in the modeling of a building construction management system. The results show that this technology can effectively improve the efficiency of the building construction schedule management. Compared with the existing management methods, it shows great advantages in terms of operating costs and ease of use. It also promotes the application of artificial intelligence technology in the construction phase.
SN  - 1687-5680
SN  - 1687-5699
DA  - MAY 9
PY  - 2022
VL  - 2022
C7  - 5602842
DO  - 10.1155/2022/5602842
AN  - WOS:000800259700001
ER  -

TY  - JOUR
AU  - Mao, Y
AU  - Zhu, GY
AU  - Yang, TT
AU  - Lange, R
AU  - Noterdaeme, T
AU  - Ma, CM
AU  - Yang, J
TI  - Rapid segmentation of computed tomography angiography images of the aortic valve: the efficacy and clinical value of a deep learning algorithm
T2  - FRONTIERS IN BIOENGINEERING AND BIOTECHNOLOGY
AB  - Objectives The goal of this study was to explore the reliability and clinical value of fast, accurate automatic segmentation of the aortic root based on a deep learning tool compared with computed tomography angiography.Methods A deep learning tool for automatic 3-dimensional aortic root reconstruction, the CVPILOT system (TAVIMercy Data Technology Ltd., Nanjing, China), was trained and tested using computed tomography angiography scans collected from 183 patients undergoing transcatheter aortic valve replacement from January 2021 to December 2022. The quality of the reconstructed models was assessed using validation data sets and evaluated clinically by experts.Results The segmentation of the ascending aorta and the left ventricle attained Dice similarity coefficients (DSC) of 0.9806/0.9711 and 0.9603/0.9643 for the training and validation sets, respectively. The leaflets had a DSC of 0.8049/0.7931, and the calcification had a DSC of 0.8814/0.8630. After 6 months of application, the system modeling time was reduced to 19.83 s.Conclusion For patients undergoing transcatheter aortic valve replacement, the CVPILOT system facilitates clinical workflow. The reliable evaluation quality of the platform indicates broad clinical application prospects in the future.
SN  - 2296-4185
DA  - MAY 30
PY  - 2024
VL  - 12
C7  - 1285166
DO  - 10.3389/fbioe.2024.1285166
AN  - WOS:001244790600001
ER  -

TY  - CPAPER
AU  - Santos, EJ
AU  - Maciel, RSP
AU  - Sant'Anna, C
A1  - ACM
TI  - A Catalogue of Bad Smells for Software Process
T2  - PROCEEDINGS OF THE 17TH BRAZILIAN SYMPOSIUM ON SOFTWARE QUALITY (SBQS)
CP  - 17th Brazilian Symposium on Software Quality (SBQS) - Ecological Capital of Brazil
AB  - Software processes play an important role in the software industry, as they influence the quality of the product and determine the efficiency of the company that develops these software products. To be used systematically in different projects, software processes need to be disseminated in the organization and continuously evaluated when one wants to understand their quality. The evaluation of a software process maintains and promotes its quality and evolution. However, if these evaluations happen from data directly collected from a process that has been applied to a given development project, process quality problems have already influenced the outcome of the process and possibly the software product. Software process models, commonly specified in a process modeling language (PML), specify in a standardized way the elements of a process and the appropriate interactions between them. In addition to assigning to the understanding, communication and execution of a software process in a company, process models offer an opportunity for them to be evaluated before their first execution or even to help identify problems in the process of ongoing projects. This paper presents a proposal to use the concept of bad smells in software process models with the objective of identifying possible disharmonies in the models. Initially bad smells of object-oriented code were analyzed and adapted to SPEM (Software & Systems Process Engineering Meta-Model) to generate a catalog. Subsequently a survey was carried out to validate the definitions, representations and possible impacts of the proposed bad smells, resulting in a validation that presented an overall rate of 86% agreement. It is expected that being possible to characterize bad smells for software processes, to enable their applicability in real software development process.
SN  - 978-1-4503-6565-9
PY  - 2015
SP  - 1
EP  - 10
DO  - 10.1145/3275245.3275264
AN  - WOS:000583860600001
ER  -

TY  - CPAPER
AU  - Damiani, E
AU  - Ceravolo, P
AU  - Fugazza, C
AU  - Reed, K
ED  - Filipe, J
ED  - Cordeiro, J
TI  - Representing and validating digital business processes
T2  - WEB INFORMATION SYSTEMS AND TECHNOLOGIES
CP  - 3rd International Conference on Web Information Systems and Technologies
AB  - Business Process Modeling is increasingly important for the digitalization of both IT and non-IT business processes, as well as for their deployment on service-oriented architectures. A number of methodologies, languages, and software tools have been proposed to support digital business process design; nonetheless, a lot remains to be done for assessing a business process model validity with respect to an existing organizational structure or external constraints like the ones imposed by security compliance regulations. In particular, web-based business coalitions and other inter-organizational transactions pose a number of research problems. The Model Driven Architecture (MDA) provides a framework for representing processes at different levels of abstraction. In this paper, a MDA-driven notion of business process model is introduced, composed of a static domain model including the domain entities and actors, plus a platform-independent workflow model providing a specification of process activities. The paper focuses on semantics-aware representation techniques, introducing logics-based static domain models and their relationship with Description Logics and current Semantic Web metadata formats. Then, the paper discusses some issues emerging from the literature on business processes representation and presents some research directions on the evaluation of the compatibility of business and IT processes with existing organizational environments and practices. The problem of implicit knowledge and of its capture in a manner which allows it to be included in business process design is also discussed, presenting some open research issues.
SN  - 1865-1348
SN  - 978-3-540-68257-8
PY  - 2008
VL  - 8
SP  - 19
EP  - +
AN  - WOS:000257923800002
ER  -

TY  - CPAPER
AU  - Hendriks, JJ
AU  - Girard, G
AU  - Baeriswyl, I
AU  - Scherwey, R
ED  - Gomes, JFS
ED  - Meguid, SA
TI  - A MODEL BASED SYSTEM ENGINEERING WITH FAULT INJECTION USING SYSML
T2  - 7TH INTERNATIONAL CONFERENCE INTEGRITY-RELIABILITY-FAILURE (IRF2020)
CP  - 7th International Conference on Integrity-Reliability-Failure (IRF)
AB  - Today's technical systems are getting more and more complex associated with the rapid increase of new technologies in a number of industrial domains. These systems have to be safe for and against humans and the environment. A number of safety regulations and standards have emerged just over the last decade. Consequently, there is a significant growth of the scope and the intensity of safety assessments for technical systems, which need to comply with these new safety regulation and standards.
   A recent new approach emerging in the many industrial fields is Model-Based System Engineering (MBSE). It has already proven its use for simulating or demonstrating special use cases in terms of behavior. In regards to safety, the traditional approach is to run the analysis once the system is designed. However, this technique is no longer cost nor time effective. The design could be drastically enhanced if the detection and handling of possible system failures could be considered earlier in the design phase. To address this idea and to avoid late design issues, the nominal and failure behavior have to be linked together in a model e.g. by using SysML (System Modeling Language). The proposed workflow combining MBSE and fault injection is part of research projects and will be explained in this paper.
SN  - 978-989-54756-1-2
PY  - 2020
SP  - 721
EP  - 728
AN  - WOS:000731040500136
ER  -

TY  - JOUR
AU  - Barabasch, J
AU  - Ducros, M
AU  - Hawie, N
AU  - Daher, SB
AU  - Nader, FH
AU  - Littke, R
TI  - Integrated 3D forward stratigraphic and petroleum system modeling of the Levant Basin, Eastern Mediterranean
T2  - BASIN RESEARCH
AB  - The Eastern Mediterranean Levant Basin is a proven hydrocarbon province with recent major gas discoveries. To date, no exploration wells targeted its northern part, in particular the Lebanese offshore. The present study assesses the tectono-stratigraphic evolution and related petroleum systems of the northern Levant Basin via an integrated approach that combines stratigraphic forward modeling and petroleum systems/basin modeling based on the previous published work. Stratigraphic modeling results provide a best-fit realisation of the basin-scale sedimentary filling, from the post-rift Upper Jurassic until the Pliocene. Simulation results suggest dominant eastern marginal and Arabian Plate sources for Cenozoic siliciclastic sediments and a significant contribution from the southern Nilotic source mostly from Lower Oligocene to Lower Miocene. Basin modeling results suggest the presence of a working thermogenic petroleum system with mature source rocks localised in the deeper offshore. The generated hydrocarbons migrated through the deep basin within Jurassic and Cretaceous permeable layers towards the Latakia Ridge in the north and the Levant margin and offshore topographic highs. Furthermore, the basin model indicates a possibly significant influence of salt deposition during Messinian salinity crisis on formation fluids. Ultimately, the proposed integrated workflow provides a powerful tool for the assessment of petroleum systems in underexplored areas.
SN  - 0950-091X
SN  - 1365-2117
DA  - APR
PY  - 2019
VL  - 31
IS  - 2
SP  - 228
EP  - 252
DO  - 10.1111/bre.12318
AN  - WOS:000461051800002
ER  -

TY  - JOUR
AU  - Driss, M
AU  - Aljehani, A
AU  - Boulila, W
AU  - Ghandorh, HW
AU  - Al-Sarem, M
TI  - Servicing Your Requirements: An FCA and RCA-Driven Approach for Semantic Web Services Composition
T2  - IEEE ACCESS
AB  - The evolution of Service-Oriented Computing (SOC) provides more efficient software development methods for building and engineering new value-added service-based applications. SOC is a computing paradigm that relies on Web services as fundamental elements. Research and technical advancements in Web services composition have been considered as an effective opportunity to develop new service-based applications satisfying complex requirements rapidly and efficiently. In this paper, we present a novel approach enhancing the composition of semantic Web services. The novelty of our approach, as compared to others reported in the literature, rests on: i) mapping user's/organization's requirements with Business Process Modeling Notation (BPMN) and semantic descriptions using ontologies, ii) considering functional requirements and also different types of non-functional requirements, such as quality of service (QoS), quality of experience (QoE), and quality of business (QoBiz), iii) using Formal Concept Analysis (FCA) technique to select the optimal set of Web services, iv) considering composability levels between sequential Web services using Relational Concept Analysis (RCA) technique to decrease the required adaptation efforts, and finally, v) validating the obtained service-based applications by performing an analytical technique, which is the monitoring. The approach experimented on an extended version of the OWLS-TC dataset, which includes more than 10830 Web services descriptions from various domains. The obtained results demonstrate that our approach allows to successfully and effectively compose Web services satisfying different types of user's functional and non-functional requirements.
SN  - 2169-3536
PY  - 2020
VL  - 8
SP  - 59326
EP  - 59339
DO  - 10.1109/ACCESS.2020.2982592
AN  - WOS:000549809000008
ER  -

TY  - CPAPER
AU  - Ceravolo, P
AU  - Damiani, E
AU  - Fugazza, C
AU  - Reed, K
ED  - Filipe, J
ED  - Cordeiro, J
TI  - Representing and validating digital business processes
T2  - WEBIST 2007: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON WEB INFORMATION SYSTEMS AND TECHNOLOGIES, VOL WIA: WEB INTERFACES AND APPLICATIONS
CP  - 3rd International Conference on Web Information Systems and Technologies
AB  - Business Process Modeling is increasingly important for the digitalization of both IT and non-IT business processes, as well as for their deployment on service-oriented architectures. A number of methodologies, languages, and software tools have been proposed to support digital business process design; nonetheless, a lot remains to be done for assessing a business process model validity with respect to an existing organizational structure or external constraints like the ones imposed by security compliance regulations. In particular, web-based business coalitions and other inter-organizational transactions pose a number of research problems. The Model Driven Architecture (MDA) provides a framework for representing processes at different levels of abstraction. In this paper, a MDA-driven notion of business process model is introduced, composed of a static domain model including the domain entities and actors, plus a platform-independent workflow model providing a specification of process activities. The paper focuses on semantics-aware representation techniques, introducing logics-based static domain models and their relationship with Description Logics and current Semantic Web metadata formats. Then, the paper discusses some issues emerging from the literature on business processes representation and presents some research directions on the evaluation of the compatibility of business and IT processes with existing organizational environments and practices. The problem of implicit knowledge and of its capture in a manner which allows it to be included in business process design is also discussed, presenting some open research issues.
SN  - 978-972-8865-78-8
PY  - 2007
SP  - IS9
EP  - IS16
AN  - WOS:000250084300001
ER  -

TY  - CPAPER
AU  - Ceravolo, P
AU  - Damiani, E
AU  - Fugazza, C
AU  - Reed, K
ED  - Filipe, J
ED  - Cordeiro, J
TI  - Representing and validating digital business processes
T2  - WEBIST 2007: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON WEB INFORMATION SYSTEMS AND TECHNOLOGIES, VOL SEBEG/EL: SOCIETY, E-BUSINESS AND E-GOVERNMENT, E-LEARNING
CP  - 3rd International Conference on Web Information Systems and Technologies
AB  - Business Process Modeling is increasingly important for the digitalization of both IT and non-IT business processes, as well as for their deployment on service-oriented architectures. A number of methodologies, languages, and software tools have been proposed to support digital business process design; nonetheless, a lot remains to be done for assessing a business process model validity with respect to an existing organizational structure or external constraints like the ones imposed by security compliance regulations. In particular, web-based business coalitions and other inter-organizational transactions pose a number of research problems. The Model Driven Architecture (MDA) provides a framework for representing processes at different levels of abstraction. In this paper, a MDA-driven notion of business process model is introduced, composed of a static domain model including the domain entities and actors, plus a platform-independent workflow model providing a specification of process activities. The paper focuses on semantics-aware representation techniques, introducing logics-based static domain models and their relationship with Description Logics and current Semantic Web metadata formats. Then, the paper discusses some issues emerging from the literature on business processes representation and presents some research directions on the evaluation of the compatibility of business and IT processes with existing organizational environments and practices. The problem of implicit knowledge and of its capture in a manner which allows it to be included in business process design is also discussed, presenting some open research issues.
SN  - 978-972-8865-79-5
PY  - 2007
SP  - A9
EP  - A16
AN  - WOS:000250085200001
ER  -

TY  - CPAPER
AU  - Ceravolo, P
AU  - Damiani, E
AU  - Fugazza, C
AU  - Reed, K
ED  - Filipe, J
ED  - Cordeiro, J
TI  - Representing and validating digital business processes
T2  - WEBIST 2007: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON WEB INFORMATION SYSTEMS AND TECHNOLOGIES, VOL IT: INTERNET TECHNOLOGY
CP  - 3rd International Conference on Web Information Systems and Technologies
AB  - Business Process Modeling is increasingly important for the digitalization of both IT and non-IT business processes, as well as for their deployment on service-oriented architectures. A number of methodologies, languages, and software tools have been proposed to support digital business process design; nonetheless, a lot remains to be done for assessing a business process model validity with respect to an existing organizational structure or external constraints like the ones imposed by security compliance regulations. In particular, web-based business coalitions and other inter-organizational transactions pose a number of research problems. The Model Driven Architecture (MDA) provides a framework for representing processes at different levels of abstraction. In this paper, a MDA-driven notion of business process model is introduced, composed of a static domain model including the domain entities and actors, plus a platform-independent workflow model providing a specification of process activities. The paper focuses on semantics-aware representation techniques, introducing logics-based static domain models and their relationship with Description Logics and current Semantic Web metadata formats. Then, the paper discusses some issues emerging from the literature on business processes representation and presents some research directions on the evaluation of the compatibility of business and IT processes with existing organizational environments and practices. The problem of implicit knowledge and of its capture in a manner which allows it to be included in business process design is also discussed, presenting some open research issues.
SN  - 978-972-8865-77-1
PY  - 2007
SP  - IS9
EP  - IS16
AN  - WOS:000250084800001
ER  -

TY  - JOUR
AU  - Samadzadegan, B
AU  - Abolhassani, SS
AU  - Dabirian, S
AU  - Ranjbar, S
AU  - Rasoulian, H
AU  - Sanei, A
AU  - Eicker, U
TI  - Novel Energy System Design Workflow for Zero-Carbon Energy District Development
T2  - FRONTIERS IN SUSTAINABLE CITIES
AB  - The growing urban population globally leads to higher greenhouse gas (GHG) emissions and stress on the electricity networks for meeting the increasing demand. In the early urban design stages, the optimization of the urban morphology and building physics characteristics can reduce energy demand. Local generation using renewable energy resources is also a viable option to reduce emissions and improve grid reliability. Notwithstanding, energy simulation and environmental impact assessment of urban building design strategies are usually not done until the execution planning stage. To address this research gap, a novel framework for designing energy systems for zero-carbon districts is developed. An urban building energy model is integrated with an urban energy system model in this framework. Dynamic prediction of heating and cooling demand and automatic sizing of different energy system configurations based on the calculated demands are the framework's primary capabilities. The workability of the framework has been tested on a case study for an urban area in Montreal to design and compare two different renewable energy systems comprising photovoltaic panels (PV), air-source, and ground source heat pumps. The case study results show that the urban building energymodel could successfully predict the heating and cooling demands in multiple spatiotemporal resolutions, while the urban energy system model provides system solutions for achieving a zero-carbon or positive energy district.
SN  - 2624-9634
PY  - 2021
VL  - 3
C7  - 662822
DO  - 10.3389/frsc.2021.662822
AN  - WOS:000751872400039
ER  -

TY  - CPAPER
AU  - Sidnev, AG
ED  - Chova, LG
ED  - Martinez, AL
ED  - Torres, IC
TI  - THE ESSENCE OF THE MATHEMATICAL BUSINESS PROCESS MODELS DISCIPLINE FOR COMPUTER SCIENCE STUDENTS
T2  - ICERI2016: 9TH INTERNATIONAL CONFERENCE OF EDUCATION, RESEARCH AND INNOVATION
CP  - 9th Annual International Conference of Education, Research and Innovation (iCERi)
AB  - The necessity of such a discipline for computer science students comes from the real practice of information system implementation that has to be preceded by business process (BP) investigation. Disregard of this stage leads to the chaos automation as it is said by those who are engaged in BP consulting. It should be noted that the problem of quantitative BP evaluation and optimization remains actual throughout the course of the whole lifetime of an information system.
   The graduates from computer science and technology department having fundamental skills in mathematics are at the same time feeling the lack of knowledge in BP modeling. The term "modeling" has many senses. With regard to our case modeling concerns the problem of BP model creation that is adequate enough to be the basis of formal methods of BP evaluation and optimization.
   There are two closely related tasks to be fulfilled whilst creating the program of BP modeling. Firstly it is needed to choose real life typical statements of the problem of BP temporal and cost indicators evaluation. Both temporal and cost BP indicators are obviously connected because of natural transformation of BP duration into its expanses. Secondly every typical BP evaluation problem has to be associated with mathematical model preferably leading to closed forms of solution. There are popular BP metrics such as Activity Based Costing, Balanced Scorecarding, etc. The problem is to gain an insight into exact formalism of BP metric having no more than its verbal description.
   An experience of delivering BP modeling lecture course for the senior students of computer science area shows that their body of knowledge is quite enough to feel confident in analytical BP temporal models, in particular network planning models with its Method of Critical Path and Performance Evaluation and Review Technique, workflow graphs with Graphic Evaluation and Review Technique, networks of queues with exact method for Jackson Networks and two-moment approximations for non-markovian networks. We arrive at the same conclusion about the models of above mentioned BP metrics. Some of BP metrics models, particularly of Activity Based Costing go to the models of Linear Algebra. The course is delivered in Peter the Great St. Petersburg Polytechnic University since 2012 till now. Its previous version was delivered to the post graduate students in Helsinki Technological University (now Aalto University) in 2004 and 2005 years. The report contains the detail course survey with all the topics, formal models descriptions and examples of assignments.
SN  - 2340-1095
SN  - 978-84-617-5895-1
PY  - 2016
SP  - 8236
EP  - 8242
AN  - WOS:000417330208034
ER  -

TY  - JOUR
AU  - Tan, WN
AU  - Chen, SB
AU  - Li, L
AU  - Li, LX
AU  - Tang, AQ
AU  - Wang, T
TI  - A method toward dynamic e-learning services modeling and the cooperative learning mechanism
T2  - INFORMATION TECHNOLOGY & MANAGEMENT
AB  - The advanced information technologies have made it possible for individuals to carry out cooperative learning efficiently and effectively from anywhere and at any time. To capitalize on the individual need and address the issues associated with the late entry into the e-learning area, it has great significance to study the service mechanism of CSCL on e-learning service and e-learning service computing modeling. This paper proposes an e-learning service model supporting for the life-cycle process management. The proposed model is developed by considering the learner's behaviours during e-learning services, the scheduling policies, and the monitoring mechanism of learning activities. Business process modeling for e-learning services can be taken according to the study ordering of the knowledge points by using workflow modeling technology and process enactment mechanism. The overall life-cycle process management of knowledge is addressed by combining knowledge product modeling, knowledge resource modeling, and credit polices for member selection in research team by considering trust value of learners, advisers and providers in e-learning services. The proposed method can be used for supporting the sustainable development of e-learning services from planning and design, organizing e-learning process, maintenance of the e-learning process, to process improvement, as well as to support learners and advisers to effectively complete innovative team study and complex computation study. Lastly, an extended topic map tool has been developed by adding a knowledge requirement level and an information extraction tool to validate the proposed methodology. These tools can used to guide learners to concentrate on the required knowledge topics and drive knowledge providers to redevelop outdated knowledge hierarchy.
SN  - 1385-951X
SN  - 1573-7667
DA  - JUN
PY  - 2017
VL  - 18
IS  - 2
SP  - 119
EP  - 130
DO  - 10.1007/s10799-015-0235-3
AN  - WOS:000403358000003
ER  -

TY  - JOUR
AU  - Ständer, SC
AU  - Barschkett, N
AU  - Kabliman, E
TI  - CAROUSEL: An Open-Source Framework for High-Throughput Microstructure Simulations
T2  - INTEGRATING MATERIALS AND MANUFACTURING INNOVATION
AB  - High-throughput screening (HTS) can significantly accelerate the design of new materials, allowing for automatic testing of a large number of material compositions and process parameters. Using HTS in Integrated Computational Materials Engineering (ICME), the computational evaluation of multiple combinations can be performed before empirical testing, thus reducing the use of material and resources. Conducting computational HTS involves the application of high-throughput computing (HTC) and developing suitable tools to handle such calculations. Among multiple ICME methods compatible with HTS and HTC, the calculation of phase diagrams known as the CALPHAD method has gained prominence. When combining thermodynamic modeling with kinetic simulations, predicting the entire history of precipitation behavior is possible. However, most reported CALPHAD-based HTS frameworks are restricted to thermodynamic modeling or not accessible. The present work introduces CAROUSEL-an open-sourCe frAmewoRk fOr high-throUghput microStructurE simuLations. It is designed to explore various alloy compositions, processing parameters, and CALPHAD implementations. CAROUSEL offers a graphical interface for easy interaction, scripting workflow for advanced simulations, the calculation distribution system, and simulation data management. Additionally, CAROUSEL incorporates visual tools for exploring the generated data and integrates through-process modeling, accounting for the interplay between solidification and solid-state precipitation. The application area is various metal manufacturing processes where the precipitation behavior is crucial. The results of simulations can be used in upscale material models, thus covering different microstructural phenomena. The present work demonstrates how CAROUSEL can be used for additive manufacturing (AM), particularly for investigating different chemical compositions and heat treatment parameters (e.g., temperature, duration).
SN  - 2193-9764
SN  - 2193-9772
DA  - DEC
PY  - 2023
VL  - 12
IS  - 4
SP  - 289
EP  - 300
DO  - 10.1007/s40192-023-00314-6
C6  - OCT 2023
AN  - WOS:001088026300001
ER  -

TY  - JOUR
AU  - Chebotarev, V
AU  - Gromov, A
TI  - AUTOMATION OF EDUCATION PROCESS
T2  - BIZNES INFORMATIKA-BUSINESS INFORMATICS
AB  - The article is based on the results obtained by the participants of the research seminar in the faculty of business Informatics of the Higher School of Economics. Its objective is to study possibilities of learning management as a process and to apply modern information technologies and tools. The learning process is defined as joint activity of participants in organization of discipline study and obtaining of subjectively or objectively new subject and procedural knowledge. The coursework in the discipline " Modeling and optimization of business processes "has been chosen for our research. We have analyzed specifics of learning process, defined requirements of process management and made modeling and assessment of automated process management. The analysis is based on information of real studies in the Faculty of Business Informatics. We have identified the following requirements to learning process: fast adaptation to changes, flexible management using various teaching methods, constant interaction with education participants, creative and reflexive abilities using participants' personality. EEPC (ARIS) and interaction and behavior diagram (Metasonic) were used as two different notations. The first notation refers to classic modeling methodology (ARIS) and the second one - to subject-oriented methodology of process management. It was shown that subjectoriented approach meets all requirements of education process that is almost impossible with classical approach. Principles of dividing learning process into smaller processes, process models (interaction diagrams and behavior diagrams of subjects) are developed. Possibility to change process management by learning subjects (teachers, students) without programming, including the automated generation of workflow application was presented. Results of research can be useful for any organization thinking about transition from traditional rigid education structure to a more advanced reflexive learning environment with network communications.
SN  - 1998-0663
PY  - 2014
VL  - 30
IS  - 4
SP  - 45
EP  - 52
AN  - WOS:000421055900005
ER  -

TY  - JOUR
AU  - Smith, S
AU  - Zimina, O
AU  - Manral, S
AU  - Nickel, M
TI  - Machine-learning assisted interpretation: Integrated fault prediction and extraction case study from the Groningen gas field, Netherlands
T2  - INTERPRETATION-A JOURNAL OF SUBSURFACE CHARACTERIZATION
AB  - Seismic fault detection using machine-learning techniques, in particular the convolution neural network (CNN), is becoming a widely accepted practice in the field of seismic interpretation. Machine-learning algorithms are trained to mimic the capabilities of an experienced interpreter by recognizing patterns within seismic data and classifying them. Regardless of the method of seismic fault detection, interpretation or extraction of 3D fault representations from edge evidence or fault probability volumes is routine. Extracted fault representations are important to the understanding of the subsurface geology and are a critical input to upstream workflows including structural framework definition, static reservoir and petroleum system modeling, and well planning and derisking activities. Efforts to automate the detection and extraction of geologic features from seismic data have evolved in line with advances in computer algorithms, hardware, and machine-learning techniques. We have developed an assisted fault interpretation workflow for seismic fault detection and extraction, demonstrated through a case study from the Groningen gas field of the Upper Permian, Dutch Rotliegend; a heavily faulted, subsalt gas field located onshore, northeast Netherlands. Supervised using interpreter-led labeling, we apply a 2D multi-CNN to detect faults within a 3D prestack depth migrated seismic data set. After prediction, we apply a geometric evaluation of predicted faults, using a principal component analysis to produce geometric attribute representations (strike azimuth and planarity) of the fault prediction. Strike azimuth and planarity attributes are used to validate and automatically extract consistent 3D fault geometries, providing geologic context to the interpreter and input to dependent workflows more efficiently.
SN  - 2324-8858
SN  - 2324-8866
DA  - MAY
PY  - 2022
VL  - 10
IS  - 2
SP  - SC17
EP  - SC30
DO  - 10.1190/INT-2021-0137.1
AN  - WOS:000806785100004
ER  -

TY  - JOUR
AU  - Sugino, T
AU  - Nakamura, R
AU  - Kuboki, A
AU  - Honda, O
AU  - Yamamoto, M
AU  - Ohtori, N
TI  - Comparative analysis of surgical processes for image-guided endoscopic sinus surgery
T2  - INTERNATIONAL JOURNAL OF COMPUTER ASSISTED RADIOLOGY AND SURGERY
AB  - PurposeThis study proposes a method to analyze surgical performance by modeling, aligning, and comparing surgical processes. This method is intended to serve as a means to support the enhancement of surgical skills for endoscopic sinus surgeries (ESSs). We focus on surgical navigation systems used in image-guided ESSs and aim to construct a comparative analysis method for surgical processes based on the information about the surgical instruments motion obtained from the navigation system. MethodsThe proposed method consists of the following three parts: quantification of surgical features, modeling of surgical processes, and alignment and comparison of surgical process models (SPMs). First, we defined time-series parameters using the navigation-based surgical data. Second, we created SPMs by applying the defined parameters and the relative positional information of the instruments to the patient's anatomy. Third, we constructed a method to align and compare SPMs based on dynamic time warping with barycenter averaging.ResultsThe proposed method was validated on a dataset containing surgical data obtained by an optical tracking system from 14 clinical ESS cases. We evaluated the validity of the comparative analysis by aligning and comparing SPMs between experts and residents. The validation results suggested that the proposed method could achieve proper alignment of the SPMs and clarify the differences in surgical processes between experts and residents.ConclusionWe developed a method to enable a time-series comparative analysis of surgical processes based on the surgical data from the navigation system. This method can allow surgeons to identify differences between their procedures and reference procedures such as experts' procedures.
SN  - 1861-6410
SN  - 1861-6429
DA  - JAN
PY  - 2019
VL  - 14
IS  - 1
SP  - 93
EP  - 104
DO  - 10.1007/s11548-018-1855-y
AN  - WOS:000456950500011
ER  -

TY  - JOUR
AU  - Zanni, M
AU  - Sharpe, T
AU  - Lammers, P
AU  - Arnold, L
AU  - Pickard, J
TI  - Developing a Methodology for Integration of Whole Life Costs into BIM Processes to Assist Design Decision Making
T2  - BUILDINGS
AB  - A common barrier to achieving design intent is the absence of comprehensive information about operational performance during design development. This results in uninformed decision-making which impacts on actual building performance, in particular Whole Life Costs (WLC). It is proposed that Building Information Modelling (BIM) has the potential to facilitate a more comprehensive and accurate design approach from the initial stages if the model can utilize reliable and robust cost and performance data from buildings in use. This paper describes the initial findings of a research project that has investigated the integration of WLC estimation into BIM processes. The study focusses specifically on the rapidly emerging Private Rental Sector (PRS) as the build-to-rent market has repeatable tasks and similar workflow patterns, roles and responsibilities, but impacts of WLC can significantly influence the business model. The study adopted a mixed method approach for the development and validation of a structured standardized process for timely WLC estimation through BIM. The research identified a number of barriers. These included varying definitions of WLC calculation methodologies; the availability and standards of data sources, in particular, the misalignment of coding systems for identification and classification of components at various levels of development, proprietary ownership of data, lack of knowledge and skills in team members to produce and/or utilize data sources, and limitations of software. However, the research proposes that these may be addressed by a reverse-engineered systematic process that uses the Integrated DEFinition (IDEF) 3 structured diagramming modelling technique that can be incorporated into a software model and has developed a model for a systematic approach for BIM-enabled WLC assessment based on CE principles which would include access to live data streams from completed buildings. The paper describes this model development which has the potential to enhance BIM lifecycle management through an augmented decision-making approach that is integral to the natural design development process.
SN  - 2075-5309
DA  - MAY
PY  - 2019
VL  - 9
IS  - 5
C7  - 114
DO  - 10.3390/buildings9050114
AN  - WOS:000472670000012
ER  -

TY  - JOUR
AU  - Borsos, A
AU  - Hámori, C
AU  - Szilágyi, E
AU  - Spaits, A
AU  - Farkas, F
AU  - Százdi, L
AU  - Fadgyas, KK
AU  - Volk, B
AU  - Szilágyi, B
TI  - Derisking Crystallization Process Development and Scale-Up Using a Complementary, "Quick and Dirty" Digital Design
T2  - ORGANIC PROCESS RESEARCH & DEVELOPMENT
AB  - Despite the spread of digital (model and AI-based) techniques, the industry-standard pharmaceutical crystallization design and scale-up is still based on experiments' design (DoE). Many orthogonally designed and usually relatively lightly monitored experiments are performed as a part of it. The final design/scale-up is inherently truncated by experimental and statistical modeling errors and assumptions, compromising the reliability of the calculated design space (DS). This study proposes to apply process modeling in a complementary way: utilize the experiments from the DoE to calibrate an application-driven model, quantify its accuracy, and use it & horbar;in parallel with the statistical interpretation of the DoE & horbar;to design the process. Both the DoE and model-based DS determination involve workflow-specific assumptions, simplifications, and errors, but the overlap between the independent results may be considered a derisked DS. We demonstrate this workflow on the design of a fed-batch salting-out crystallization for a commercial active pharmaceutical ingredient (API). The model was identified based on product particle size distribution data of a DoE set from a small-scale reactor (0.25 L) and a manufacturing batch (ca. 4000 L). Although reactors with intermediate volumes are also generally applied as a part of scale-up, included in the presented case study, those were not included in the model development and verification. The kinetic equations were taken from our previously developed cooling crystallization model of the same API. After calibration and accuracy evaluation, the critical process parameters were determined using interpretable machine learning via Shapley diagrams, and the DS was mapped and visualized using Monte Carlo sampling-based simulations. The DS was validated for 0.25 L experiments. The model-based DS was somewhat narrower than the DoE-based DS on a small scale. The DS determined for plant-scale crystallization can guide the manufacturing-scale process design and operation. The extrapolation capabilities of the model were stressed by external validation by defining and validating experimentally the DS for a 1 L crystallization. These results indicate that models developed in this application-centric way can enhance the robustness of the processes, and the modeling branch does not add any risk. In the worst-case scenario, if the modeling fails, one still has the results from the traditional design approach.
SN  - 1083-6160
SN  - 1520-586X
DA  - OCT 3
PY  - 2024
VL  - 28
IS  - 10
SP  - 3813
EP  - 3826
DO  - 10.1021/acs.oprd.4c00199
C6  - OCT 2024
AN  - CCC:001337700500001
ER  -

TY  - JOUR
AU  - Carlson, M
AU  - Watkins, J
AU  - Tezaur, I
TI  - Automatic performance tuning for Albany Land Ice
T2  - JOURNAL OF COMPUTATIONAL AND APPLIED MATHEMATICS
AB  - Accurate simulation of the evolution of polar ice-sheets requires a massive amount of computational power. In order to take advantage of the newest generation of supercomputing clusters, the Albany Land Ice code has been modernized for performance portability across a variety of parallel architectures, with a focus on enabling end-to -end GPU capability. Albany uses a multigrid preconditioning approach for solving linear systems via performance portable smoothers from the Trilinos package Ifpack2. Since the Albany Land Ice code is constantly evolving and both Albany and Trilinos are in constant development, it is likely that the optimal choice of solver parameters will change over time. It is therefore critical to have an automatic performance tuning framework to ensure that the best possible performance is maintained. Toward this effect, we have developed an automatic performance tuning framework to determine the best fine-and coarse-grid smoothing algorithms and parameters. We treat the underlying performance model of the linear solve as a black box and use the python-based GPTune Bayesian optimization library to determine the optimal smoother choice and parameters. Using this approach, we have found smoothers and their corresponding parameters that result in, on average, 1.2 times faster, and up to 1.5 times faster solve-times than our manually -tuned parameters. We also show that the proposed auto-tuning approach produces reliably better parameters than naive black box optimization techniques like random search for a given function evaluation budget. By implementing our tuning framework in the Python-based workflow management tool parsl, we also ensure that we efficiently use available computing resources during the tuning process and avoid unnecessary long wait times in computing cluster job queues.(c) 2023 Elsevier B.V. All rights reserved.
SN  - 0377-0427
SN  - 1879-1778
DA  - SEP
PY  - 2023
VL  - 429
C7  - 115222
DO  - 10.1016/j.cam.2023.115222
C6  - MAR 2023
AN  - WOS:000973056400001
ER  -

TY  - JOUR
AU  - Deckers, R
AU  - Lago, P
TI  - Specifying features in terms of domain models: MuDForM method definition and case study
T2  - JOURNAL OF SOFTWARE-EVOLUTION AND PROCESS
AB  - To enable the people involved in a software development process to communicate and reason close to their area of knowledge, we are investigating and engineering a method that formalizes and integrates knowledge of multiple domains into domain models and into specifications in terms of those domain models. We follow an action research approach, starting with a diagnosis phase, in which we have previously defined a set of method objectives, and performed a systematic literature review. During action planning, we defined how we are going to develop the method-called Multi-Domain Formalization Method (MuDForM). This paper reports on the methodical support for using a domain model as the terminology for feature specifications. During action taking, we defined an initial version of the method and set up case studies. During the evaluation phase, we performed a case study to validate how well the method helps in the specification of processes and to realize the case-specific objectives of the customer. The case study pertains to the formalization of the ISO26262 standard for functional safety in the automotive domain. The created models are explained to the involved experts to ensure their consistency with the original text. We found that MuDForM is suitable to systematically formalize processes described in natural language, such that the resulting process models are fully expressed in terms of domain concepts and concepts from outside the domains and processes of interest. Further, during the specifying learning phase, we have extended our method with concepts, steps, and guidelines for grammatical analysis, for formalization of constraints, and for the specification of processes.
   This paper presents methodical support for using domain models as a language to specify features, such that it is an intrinsic part of our domain-oriented modeling method that also covers the creation of domain models. It also presents the validation of the method in an industrial case. image
SN  - 2047-7473
SN  - 2047-7481
DA  - 2023 NOV 8
PY  - 2023
DO  - 10.1002/smr.2623
C6  - NOV 2023
AN  - WOS:001096895800001
ER  -

TY  - JOUR
AU  - Hoang, U
AU  - Williams, A
AU  - Smylie, J
AU  - Aspden, C
AU  - Button, E
AU  - Macartney, J
AU  - Okusi, C
AU  - Byford, R
AU  - Ferreira, F
AU  - Leston, M
AU  - Xie, CX
AU  - Joy, M
AU  - Marsden, G
AU  - Clark, T
AU  - de Lusignan, S
TI  - The Impact of Point-of-Care Testing for Influenza on Antimicrobial Stewardship (PIAMS) in UK Primary Care: Protocol for a Mixed Methods Study
T2  - JMIR RESEARCH PROTOCOLS
AB  - Background: Molecular point-of-care testing (POCT) used in primary care can inform whether a patient presenting with an acute respiratory infection has influenza. A confirmed clinical diagnosis, particularly early in the disease, could inform better antimicrobial stewardship. Social distancing and lockdowns during the COVID-19 pandemic have disturbed previous patterns of influenza infections in 2021. However, data from samples taken in the last quarter of 2022 suggest that influenza represents 36% of sentinel network positive virology, compared with 24% for respiratory syncytial virus. Problems with integration into the clinical workflow is a known barrier to incorporating technology into routine care.Objective: This study aims to report the impact of POCT for influenza on antimicrobial prescribing in primary care. We will additionally describe severe outcomes of infection (hospitalization and mortality) and how POCT is integrated into primary care workflows.Methods: The impact of POCT for influenza on antimicrobial stewardship (PIAMS) in UK primary care is an observational study being conducted between December 2022 and May 2023 and involving 10 practices that contribute data to the English sentinel network. Up to 1000 people who present to participating practices with respiratory symptoms will be swabbed and tested with a rapid molecular POCT analyzer in the practice. Antimicrobial prescribing and other study outcomes will be collected by linking information from the POCT analyzer with data from the patient's computerized medical record. We will collect data on how POCT is incorporated into practice using data flow diagrams, unified modeling language use case diagrams, and Business Process Modeling Notation.Results: We will present the crude and adjusted odds of antimicrobial prescribing (all antibiotics and antivirals) given a POCT diagnosis of influenza, stratifying by whether individuals have a respiratory or other relevant diagnosis (eg, bronchiectasis). We will also present the rates of hospital referrals and deaths related to influenza infection in PIAMS study practices compared with a set of matched practices in the sentinel network and the rest of the network. We will describe any difference in implementation models in terms of staff involved and workflow.Conclusions: This study will generate data on the impact of POCT testing for influenza in primary care as well as help to inform about the feasibility of incorporating POCT into primary care workflows. It will inform the design of future larger studies about the effectiveness and cost-effectiveness of POCT to improve antimicrobial stewardship and any impact on severe outcomes.
SN  - 1929-0748
PY  - 2023
VL  - 12
C7  - e46938
DO  - 10.2196/46938
AN  - MEDLINE:37327029
ER  -

TY  - CPAPER
AU  - Nomaguchi, Y
AU  - Matsuyasu, R
AU  - Horinouchi, T
AU  - Fujita, K
A1  - ASME
TI  - DESIGN PROCESS PLANNING BY MULTI-OBJECTIVE OPTIMIZATION OF TECHNICAL PERFORMANCE AND PRODUCT INTEGRITY
T2  - PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE, 2011, VOL 9
CP  - ASME International Design Engineering Technical Conferences / Computers and Information in Engineering Conference (IDETC/CIE)
AB  - The importance of flexible and optimal planning of engineering design process for ensuring business success is increasingly recognized in today's competitive environment. Because a product consists of a lot of subsystems each of which has technical functionalities, an engineering design process consists of two different activities, i.e., an activity to enhance the technical performance level of each subsystem, and an activity to enhance the integrity among the subsystems. When considering the limitation of resources that a manufacturer possesses, there is a trade-off between the achievement probabilities of those activities, although they correlate with each other. This paper proposes a new optimization method of design process planning that aims at a Pareto-optimal achievement of individual technical performances and total product integrity in order to support decision makings of a project manager. A planning of design process is a very complicated optimization problem including a number of various design variables and various evaluation indices. Some of those are discrete factors, while the others are continuous. In order to solve such a complicated problem with a reasonable computation cost, this research separates the optimization problem into two sub problems, i.e., (i) defining of process architecture and organization structure, that is, definition of activities, their sequences, and task groups and (ii) scheduling of resource allocation into activities. A growth curve model with a fuzzy number, which we have been developing, is used to formulate objective functions that evaluate the achievement probability of technical performance and its total integrity. The same model is adopted for objective functions of both sub problems so as to keep consistency between their optimization results. Neighborhood Cultivation GA (NCGA) is adopted in order to effectively solve the multi-objective optimization problem. This paper demonstrates its application to a student design project in order to show the ability of the method. A proposed optimization method facilitates a project manager to explore various process plans, to assess their risks, and decide a temporary plan.
SN  - 978-0-7918-5486-0
PY  - 2012
SP  - 827
EP  - 840
AN  - WOS:000324350600077
ER  -

TY  - JOUR
AU  - Chen, KF
AU  - Zhang, PL
AU  - Yan, H
AU  - Chen, GL
AU  - Sun, TZ
AU  - Lu, QH
AU  - Chen, Y
AU  - Shi, HC
TI  - A review of machine learning in additive manufacturing: design and process
T2  - INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY
AB  - Additive manufacturing (AM), owing to its unique manufacturing approach, can drive manufacturing towards higher levels of sophistication, flexibility, and intelligence within the context of Industry 4.0. However, the complexity of AM systems and their nature as data-intensive manufacturing domains present challenges in producing high-quality products. With the advancement of digital and computer technologies, data-driven machine learning (ML) has been widely applied in AM, as it provides effective methods for quality control, process optimization, and complex system modeling. This paper succinctly summarizes the various phases of utilizing ML to assist in AM processes. It elucidates the advantages of using ML over traditional methods in each phase, starting from the pre-processing phase of design for additive manufacturing (DfAM) and parameter optimization, through the processing phase of defect detection, to the post-processing phase of part quality assessment. The objective of DfAM is to optimize product design, taking into account the interactions between multiple variables. The nonlinear capability of ML can be effectively utilized in DfAM. Defect detection aims to ensure the repeatability, durability, and reliability of parts. Combining sensors with ML can guarantee part quality. Lastly, assessing part quality and inspecting various subsystems of AM are conducted from the perspective of the part's microstructure. This review explores the application of ML in addressing numerous issues related to the AM process workflow. It provides a comprehensive and systematic summary of the application of various ML models at different phases, investigates the potential of newer ML technologies to assist AM in the future, and concludes with the limitations of current ML applications in AM as well as future development directions. By intersecting these two dynamic fields, it aims to enrich the AM research domain.
SN  - 0268-3768
SN  - 1433-3015
DA  - NOV
PY  - 2024
VL  - 135
IS  - 3-4
SP  - 1051
EP  - 1087
DO  - 10.1007/s00170-024-14543-2
C6  - OCT 2024
AN  - WOS:001326408200001
ER  -

TY  - CHAP
AU  - Mittal, S
ED  - Tolk, A
ED  - Jain, LC
TI  - Agile Net-Centric Systems Using DEVS Unified Process
T2  - INTELLIGENCE-BASED SYSTEMS ENGINEERING
AB  - Industry and government are spending extensively to transition their business processes and governance to Service Oriented Architecture (SOA) implementations for efficient information reuse, integration, collaboration and cost-sharing. SOA enables orchestrating web services to execute such processes using Business Process Execution Language (BPEL). Business Process Modeling Notation (BPMN) is another method that outputs BPEL for deployment. As an example, the Department of Defenses (DoD) grand vision is the Global Information Grid that is founded on SOA infrastructure. The SOA infrastructure is to be based on a small set of capabilities known as Core Enterprise Services (CES) whose use is mandated to enable interoperability and increased information sharing within and across Mission Areas, such as the Warfighter domain, Business processes, Defense Intelligence, and so on. Net-Centric Enterprise Services (NOES) is DoDs implementation of its Data Strategy over the GIG. However, composing/orchestrating web services in a process workflow (a.k.a Mission thread in the Doll domain) is currently bounded by the BPMN/BPEL technologies. With so much resting on SOA, their reliability and analysis must be rigorously considered. The BPMN/BPEL combination neither has any grounding in system theoretical principles nor can it be used in designing net-centric systems based on SOA in its current state. In this work we present a system theoretical framework using the DEVS Unified Process (DUNIP) that allows bifurcated model-continuity based life cycle process for simultaneous development of the executable system using web-services (including the model) and the automated generation of Test-suite for Verification and Validation. The entire net-centric system, which includes artifacts like the model, the simulation and the real system, is deployed on SOA. The simulation system is made possible on a recently developed DEVS-based service framework called DEVS/SOA. We will show the design of DEVS-agents based on WSDLs and how they are composed towards the systems specification. We will demonstrate how agility is an inherent characteristic of such a system founded on DUNIP. We will also present the case of Department of Defense Architecture Framework (DoDAF) and how agility can be applied to the design and evaluation process.
SN  - 1868-4394
SN  - 1868-4408
SN  - 978-3-642-17930-3
PY  - 2011
VL  - 10
SP  - 159
EP  - 199
DO  - 10.1007/978-3-642-17931-0_7
DO  - 10.1007/978-3-642-17931-0
AN  - WOS:000293318100007
ER  -

TY  - JOUR
AU  - Soenandi, IA
AU  - Djatna, T
AU  - Suryani, A
AU  - Irzaman
TI  - Real-time optimization using gradient adaptive selection and classification from infrared sensors measurement for esterification oleic acid with glycerol
T2  - INTERNATIONAL JOURNAL OF INTELLIGENT COMPUTING AND CYBERNETICS
AB  - Purpose - The production of glycerol derivatives by the esterification process is subject to many constraints related to the yield of the production target and the lack of process efficiency. An accurate monitoring and controlling of the process can improve production yield and efficiency. The purpose of this paper is to propose a real-time optimization (RTO) using gradient adaptive selection and classification from infrared sensor measurement to cover various disturbances and uncertainties in the reactor.
   Design/methodology/approach - The integration of the esterification process optimization using self-optimization (SO) was developed with classification process was combined with necessary condition optimum (NCO) as gradient adaptive selection, supported with laboratory scaled medium wavelength infrared (mid-IR) sensors, and measured the proposed optimization system indicator in the batch process. Business Process Modeling and Notation (BPMN 2.0) was built to describe the tasks of SO workflow in collaboration with NCO as an abstraction for the conceptual phase. Next, Stateflow modeling was deployed to simulate the three states of gradient-based adaptive control combined with support vector machine (SVM) classification and Arduino microcontroller for implementation.
   Findings - This new method shows that the real-time optimization responsiveness of control increased product yield up to 13 percent, lower error measurement with percentage error 1.11 percent, reduced the process duration up to 22 minutes, with an effective range of stirrer rotation set between 300 and 400 rpm and final temperature between 200 and 210 degrees C which was more efficient, as it consumed less energy.
   Research limitations/implications - In this research the authors just have an experiment for the esterification process using glycerol, but as a development concept of RTO, it would be possible to apply for another chemical reaction or system.
   Practical implications - This research introduces new development of an RTO approach to optimal control and as such marks the starting point for more research of its properties. As the methodology is generic, it can be applied to different optimization problems for a batch system in chemical industries.
   Originality/value - The paper presented is original as it presents the first application of adaptive selection based on the gradient value of mid-IR sensor data, applied to the real-time determining control state by classification with the SVM algorithm for esterification process control to increase the efficiency.
SN  - 1756-378X
SN  - 1756-3798
PY  - 2017
VL  - 10
IS  - 2
SP  - 130
EP  - 144
DO  - 10.1108/IJICC-06-2016-0022
AN  - WOS:000404786800003
ER  -

TY  - JOUR
AU  - Chen, PF
AU  - Zhou, K
AU  - Fang, HL
TI  - High-Resolution Seamless Mapping of the Leaf Area Index via Multisource Data and the Transformer Deep Learning Model
T2  - IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING
AB  - The leaf area index (LAI) is a critical parameter for monitoring vegetation health and studying climate change. The spatial resolutions of most LAI products range from 500 to 1000 m. Only a few LAI products exhibit spatial resolutions ranging from 16 to 30 m, but notable missing data occur because of the revisit cycle of satellites and the effect of weather conditions. These methods cannot satisfy the requirements of LAI application communities. To address this issue, a workflow was proposed for high-resolution seamless mapping of the LAI on the basis of multisource data and the Transformer deep learning model. Jiangsu Province in China was chosen as the study area. In this area, numerous cloudy and rainy days occur annually. Harmonized Landsat and Sentinel-2 (HLS) and moderate resolution imaging spectroradiometer (MODIS) reflectance images were obtained. The MODIS and HLS images were first composited and spatially aligned. Then, on the basis of the MODIS images and the spatiotemporal fusion incorporating spectral autocorrection (FIRST) method, missing HLS image data were reconstructed, thus producing a reflectance product with a spatial resolution of 30 m and a temporal resolution of 12 days. On the basis of the reflectance product, a Transformer model was designed for LAI prediction and compared with models designed through backpropagation neural network (BPNN), convolutional neural network (CNN), long short-term memory (LSTM), and bidirectional LSTM (Bi-LSTM) methods. These models were compared with and without transfer learning on the basis of an independent dataset. The best model was selected and employed to produce a LAI product for the study area, which was subsequently compared with an existing MODIS product from spatial and temporal perspectives. The results showed that the procedure for HLS reconstruction is effective, with errors varying between 4.00% and 15.93% for different bands. Among the LAI prediction models, the Transformer model consistently performed the best across all scenarios. Notably, the Transformer model trained via transfer learning yielded the best results, with a test R-2 value of 0.62, a root mean square error (RMSE) of 0.79 and a mean relative error (MRE) of 14.80%. The R-2 values of the other models ranged from 0.31 to 0.59, the RMSE values ranged from 0.82 to 1.06, and the MRE values ranged from 15.41% to 22.28%. In addition, the HLS LAI established via the above best model provided greater spatiotemporal accuracy than did the MODIS LAI product. This study provides reference data for establishing seamless LAI products with high spatial and temporal resolutions, contributing to applications such as vegetation health monitoring and global change research.
SN  - 0196-2892
SN  - 1558-0644
SN  - 1558-0644
SN  - 1558-0644
PY  - 2025
VL  - 63
C7  - 4408512
DO  - 10.1109/tgrs.2025.3561326
DO  - 10.1109/TGRS.2025.3561326
AN  - CCC:001480475600007
ER  -

TY  - JOUR
AU  - Huang, HL
AU  - Qian, Y
AU  - Bisht, G
AU  - Wang, JL
AU  - Chakraborty, T
AU  - Hao, DL
AU  - Li, JF
AU  - Thurber, T
AU  - Singh, B
AU  - Yang, Z
AU  - Liu, Y
AU  - Xue, PF
AU  - Sacks, WJ
AU  - Coon, E
AU  - Hetland, R
TI  - WRF-ELM v1.0: a regional climate model to study land-atmosphere interactions over heterogeneous land use regions
T2  - GEOSCIENTIFIC MODEL DEVELOPMENT
AB  - The Energy Exascale Earth System Model (E3SM) Land Model (ELM) is a state-of-the-art land surface model that simulates the intricate interactions between the terrestrial land surface and other components of the Earth system. Originating from the Community Land Model (CLM) version 4.5, ELM has been under active development, with added new features and functionality, including plant hydraulics, radiation-topography interaction, subsurface multiphase flow, and more explicit land use and management practices. This study integrates ELM v2.1 with the Weather Research and Forecasting (WRF; WRF-ELM) model through a modified Lightweight Infrastructure for Land Atmosphere Coupling (LILAC) framework, enabling affordable high-resolution regional modeling by leveraging ELM's innovative features alongside WRF's diverse atmospheric parameterization options. This framework includes a top-level driver for variable communication between WRF and ELM and Earth System Modeling Framework (ESMF) caps for the WRF atmospheric component and ELM workflow control, encompassing initialization, execution, and finalization. Importantly, this LILAC-ESMF framework demonstrates a more modular approach compared to previous coupling efforts between WRF and land surface models. It maintains the integrity of ELM's source code structure and facilitates the transfer of future developments in ELM to WRF-ELM.To test the ability of the coupled model to capture land-atmosphere interactions over regions with a variety of land uses and land covers, we conducted high-resolution (4 km) WRF-ELM ensemble simulations over the Great Lakes region (GLR) in the summer of 2018 and systematically compared the results against observations, reanalysis data, and WRF-CTSM (WRF coupled with the Community Terrestrial Systems Model). In general, the coupled WRF-ELM model has reasonably captured the spatial distribution of surface state variables and fluxes across the GLR, particularly over the natural vegetation areas. The evaluation results provide a baseline reference for further improvements in ELM in the regional application of high-resolution weather and climate predictions. Our work serves as an example to the model development community for expanding an advanced land surface model's capability to represent fully-coupled land-atmosphere interactions at fine spatial scales. The development and release of WRF-ELM marks a significant advancement for the ELM user community, providing opportunities for fine-scale regional representation, parameter calibration in coupled mode, and examination of new schemes with atmospheric feedback.
SN  - 1991-959X
SN  - 1991-9603
DA  - MAR 7
PY  - 2025
VL  - 18
IS  - 5
SP  - 1427
EP  - 1443
DO  - 10.5194/gmd-18-1427-2025
AN  - WOS:001438581800001
ER  -

